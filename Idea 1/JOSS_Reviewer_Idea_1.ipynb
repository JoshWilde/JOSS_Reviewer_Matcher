{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "15ed6164-2f2c-4faa-aa1e-35d5503fd0b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import glob\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e65264d2-adb4-445c-a9e2-8af7090dbb1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/sunilmcesh/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "from tqdm import tqdm \n",
    "#import pdfminer\n",
    "from pdfminer.high_level import extract_text\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk \n",
    "nltk.download('stopwords')\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "400363b2-9d96-4fd2-9e1f-eb115b8321bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/sunilmcesh/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/sunilmcesh/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk import sent_tokenize\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "from nltk.probability import FreqDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "576b4f68-4f8a-49aa-9c9b-6871a74b5e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "PAPER_OF_INTEREST_FNAME  =  glob.glob('/Volumes/Seagate Backup Plus Drive/JOSS Project/joss-papers-master/*/*/*.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "bf0a5e92-3e3f-4d37-a074-b29ad7ad12dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Get_Lemma_Words(POI_PDF):\n",
    "  text = str(POI_PDF)\n",
    "  text2 = text.split()\n",
    "  words_no_punc = []\n",
    "\n",
    "  for w in text2:\n",
    "    if w.isalpha():\n",
    "      words_no_punc.append(w.lower())\n",
    "  from nltk.corpus import stopwords\n",
    "  stopwords = stopwords.words('english')  \n",
    "  clean_words = []\n",
    "  for w in words_no_punc:\n",
    "    if w not in stopwords:\n",
    "      clean_words.append(w)\n",
    "  clean_words_arr = ''\n",
    "  for i in range(len(clean_words)):\n",
    "    clean_words_arr = clean_words_arr + ' ' + str(clean_words[i])\n",
    "\n",
    "  string_for_lemmatizing = clean_words_arr\n",
    "  lemmatizer = WordNetLemmatizer()\n",
    "  words_2 = word_tokenize(string_for_lemmatizing)\n",
    "  lemmatized_words = [lemmatizer.lemmatize(word) for word in words_2]\n",
    "\n",
    "  lemmatized_words_arr = ''\n",
    "  for i in range(len(lemmatized_words)):\n",
    "    lemmatized_words_arr = lemmatized_words_arr + ' ' + str(lemmatized_words[i])\n",
    "  words = word_tokenize(lemmatized_words_arr)\n",
    "\n",
    "  return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0088cba4-82fe-4716-950f-da697b0b0663",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reviewers = pd.read_csv('../Data/JOSS Table Test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fa637666-2b60-4849-9fc4-bb4f0f6552da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Get_reviewer_sample_tf(Paper_interest, df_reviewers=df_reviewers,num_suggestions=5, num_top20=20):\n",
    "    POI_PDF = [extract_text(Paper_interest)]\n",
    "    text = str(POI_PDF)\n",
    "    words =  Get_Lemma_Words(POI_PDF)\n",
    "    #print(len(words))\n",
    "    fdist = FreqDist(words)\n",
    "    X = np.array(fdist.most_common())\n",
    "    top20_tf = X[:num_top20,0]\n",
    "    match_arr = Compare_topics(top20_tf, df_reviewers)\n",
    "    top5_reviewers = np.argsort(match_arr)[-num_suggestions:]\n",
    "\n",
    "    \n",
    "    all_usernames = []\n",
    "    all_domains = []\n",
    "    all_num_matched_words = []\n",
    "    all_matched_words = []\n",
    "    for i in range(num_suggestions):\n",
    "      K = -1*(i+1)\n",
    "      index = top5_reviewers[K]\n",
    "      #print(i)\n",
    "      t =df_reviewers.iloc[index+1]['Domains/topic areas you are comfortable reviewing'].lower()\n",
    "      \n",
    "      all_usernames.append(df_reviewers.username.iloc[index+1])\n",
    "      all_domains.append(t)\n",
    "      all_num_matched_words.append(match_arr[index])\n",
    "\n",
    "      uniarr = Split_columns(t)\n",
    "      matched_words = []\n",
    "      #print(uniarr)\n",
    "      for j in range(len(uniarr)):\n",
    "        for k in range(len(top20_tf)):\n",
    "          if uniarr[j] == top20_tf[k]:\n",
    "            matched_words.append(uniarr[j])\n",
    "      all_matched_words.append(matched_words)\n",
    "\n",
    "    #df_reviewers.username.iloc[+1]\n",
    "\n",
    "    return all_usernames, all_domains, all_num_matched_words, all_matched_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "844388bc-ccca-4c83-b0c7-270748c7ee7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Compare_topics(top20, df_reviewers):\n",
    "  length = df_reviewers.shape[0] - 1\n",
    "  match_arr = np.zeros(length)\n",
    "  for i in range(length):\n",
    "    if pd.isna(df_reviewers['Domains/topic areas you are comfortable reviewing'].str.lower().values[1+i]) == False:\n",
    "      t = df_reviewers['Domains/topic areas you are comfortable reviewing'].str.lower().values[1+i]\n",
    "      #print(i)\n",
    "      uniarr = Split_columns(t)\n",
    "      for j in range(len(uniarr)):\n",
    "        for k in range(len(top20)):\n",
    "          if uniarr[j] == top20[k]:\n",
    "            match_arr[i] = match_arr[i] + 1\n",
    "  return match_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "4ff675ff-8d67-4bcb-80a8-5901d1fa0046",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Split_columns(t):\n",
    "  txt = \" \".join(\"\".join([\" \" if ch in string.punctuation else ch for ch in t]).split())\n",
    "  sol1 = np.char.split(txt, ' ')\n",
    "  txt_arr  = array_of_lists_to_array(sol1)\n",
    "  uniarr = np.unique(txt_arr)\n",
    "  return uniarr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1abc58eb-39a5-4b6a-8866-1721039f6b57",
   "metadata": {},
   "outputs": [],
   "source": [
    " def array_of_lists_to_array(arr):\n",
    "    return np.apply_along_axis(lambda a: np.array(a[0]), -1, arr[..., None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "50f2525d-c171-4c20-8d95-f7795510a314",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summatation_bot(all_usernames, all_domains, all_num_matched_words, all_matched_words):\n",
    "  length = len(all_usernames)\n",
    "  message = 'Hello. \\nI have found ' + str(length) + ' possible reviewers for this paper.' +'\\n\\n'\n",
    "  for i in range(length):\n",
    "    ps = 'I believe ' + all_usernames[i] + ' will make a good reviewer for this paper because they have matched ' + str(int(all_num_matched_words[i])) +  ' words from their comfortable domain topics with the top 20 most frequent words in the paper. These matched words are ' + str(all_matched_words[i]) +'.\\nFrom their topics domain: ' + str(all_domains[i].replace('\\n', ', ')) +'.\\n'\n",
    "    message = message + ps + '\\n'\n",
    "  print(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "77b98f0c-0a7c-4521-8f99-fcfabcdd1d4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Volumes/Seagate Backup Plus Drive/JOSS Project/joss-papers-master/joss-papers-master/joss.00775/10.21105.joss.00775.pdf\n"
     ]
    }
   ],
   "source": [
    "Q = 340\n",
    "print(PAPER_OF_INTEREST_FNAME[Q])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "231128fb-1b74-4a88-acc8-6da76c4e84d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_usernames, all_domains, all_num_matched_words, all_matched_words = Get_reviewer_sample_tf(PAPER_OF_INTEREST_FNAME[Q])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "33dd397f-237c-46fc-92da-9faae46b3473",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello. \n",
      "I have found 5 possible reviewers for this paper.\n",
      "\n",
      "I believe jifarquharson will make a good reviewer for this paper because they have matched 3 words from their comfortable domain topics with the top 20 most frequent words in the paper. These matched words are ['open', 'software', 'source'].\n",
      "From their topics domain: geoscience tools,  open-source publishing,  applied open-source software,  remote sensing.\n",
      "\n",
      "I believe ashok-arora will make a good reviewer for this paper because they have matched 3 words from their comfortable domain topics with the top 20 most frequent words in the paper. These matched words are ['open', 'software', 'source'].\n",
      "From their topics domain: artificial intelligence, computer vision, open source software, cmake.\n",
      "\n",
      "I believe vinodkahuja will make a good reviewer for this paper because they have matched 3 words from their comfortable domain topics with the top 20 most frequent words in the paper. These matched words are ['open', 'software', 'source'].\n",
      "From their topics domain: open source software.\n",
      "\n",
      "I believe ritwikagarwal will make a good reviewer for this paper because they have matched 3 words from their comfortable domain topics with the top 20 most frequent words in the paper. These matched words are ['open', 'software', 'source'].\n",
      "From their topics domain: ict ,  emerging technologies,  advancement in programming languages,  use of ai and open source in climate change,  open source software.\n",
      "\n",
      "I believe GregoryAshton will make a good reviewer for this paper because they have matched 3 words from their comfortable domain topics with the top 20 most frequent words in the paper. These matched words are ['open', 'software', 'source'].\n",
      "From their topics domain: open source software, statistics, bayesian inference/stochastic sampling, data visualization.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "summatation_bot(all_usernames, all_domains, all_num_matched_words, all_matched_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796da3c8-7576-4a6a-b9b2-7bc879394414",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
