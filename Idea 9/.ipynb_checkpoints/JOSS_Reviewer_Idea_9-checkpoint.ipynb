{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d4f9c43d-e250-46ae-8be7-a01f7d95f3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sense2vec import Sense2Vec\n",
    "import re \n",
    "from termcolor import colored\n",
    "from JOSS_PDF_Cleaner import Clean_PDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7b746244-7988-40fe-b9c6-5861ce67df9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim import utils\n",
    "import numpy as np\n",
    "import sys\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from nltk import word_tokenize\n",
    "from nltk import download\n",
    "from nltk.corpus import stopwords\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "%matplotlib inline\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93c847ae-6de8-4b75-8bd2-46fa86b69394",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/sunilmcesh/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/sunilmcesh/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/sunilmcesh/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import glob\n",
    "import string\n",
    "import glob\n",
    "from tqdm import tqdm \n",
    "#import pdfminer\n",
    "from pdfminer.high_level import extract_text\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk \n",
    "nltk.download('stopwords')\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk import sent_tokenize\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "from nltk.probability import FreqDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8224e7bd-e72d-4d93-8c12-a027bb30c580",
   "metadata": {},
   "outputs": [],
   "source": [
    "s2v = Sense2Vec().from_disk(\"../../s2v_old\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b8ff0c8a-4d3b-47d5-9863-b7dc5f17f726",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Volumes/Seagate Backup Plus Drive/JOSS Project/joss-papers-master/joss-papers-master/joss.00011/10.21105.joss.00011.pdf\n"
     ]
    }
   ],
   "source": [
    "Q = 0\n",
    "PAPER_OF_INTEREST_FNAME  = glob.glob('/Volumes/Seagate Backup Plus Drive/JOSS Project/joss-papers-master/*/*/*.pdf')\n",
    "print(PAPER_OF_INTEREST_FNAME[Q])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e7616cbc-f61b-495d-aada-cddab2bec503",
   "metadata": {},
   "outputs": [],
   "source": [
    "Paper_interest = PAPER_OF_INTEREST_FNAME[Q]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f16e5b49-744f-479c-aa71-d28ec0a87219",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mcarl: a likelihood-free inference toolbox\n",
      "\u001b[0m\n",
      "\u001b[31mgilles louppe1, kyle cranmer1, and juan pavez2\n",
      "\u001b[0m\n",
      "\u001b[31mdoi: 10.21105/joss.00011\n",
      "\u001b[0m\n",
      "\u001b[31m1 new york university 2 federico santa maría university\n",
      "\u001b[0m\n",
      "\u001b[31msummary\n",
      "\u001b[0m\n",
      "\u001b[31msoftware\n",
      "\u001b[0m\n",
      "\u001b[31m• review\n",
      "• repository\n",
      "• archive\n",
      "\u001b[0m\n",
      "\u001b[31mlicence\n",
      "authors of joss papers retain\n",
      "copyright and release the work un-\n",
      "der a creative commons attri-\n",
      "bution 4.0 international license\n",
      "(cc-by).\n",
      "\u001b[0m\n",
      "\u001b[32mcarl is a toolbox for likelihood-free inference in python.\n",
      "\u001b[0m\n",
      "\u001b[32mthe likelihood function is the central object that summarizes the information from an\n",
      "experiment needed for inference of model parameters. it is key to many areas of science\n",
      "that report the results of classical hypothesis tests or confidence intervals using the (gen-\n",
      "eralized or profile) likelihood ratio as a test statistic. at the same time, with the advance\n",
      "of computing technology, it has become increasingly common that a simulator (or gener-\n",
      "ative model) is used to describe complex processes that tie parameters of an underlying\n",
      "theory and measurement apparatus to high-dimensional observations. however, directly\n",
      "evaluating the likelihood function in these cases is often impossible or is computationally\n",
      "impractical.\n",
      "\u001b[0m\n",
      "\u001b[32min this context, the goal of this package is to provide tools for the likelihood-free setup,\n",
      "including likelihood (or density) ratio estimation algorithms, along with helpers to carry\n",
      "out inference on top of these.\n",
      "\u001b[0m\n",
      "\u001b[32mapproximating likelihood ratios with calibrated classifiers\n",
      "\u001b[0m\n",
      "\u001b[32mmethodological details regarding likelihood-free inference with calibrated classifiers can\n",
      "be found in the companion paper (cranmer, pavez, and louppe 2016).\n",
      "\u001b[0m\n",
      "\u001b[32mfuture development aims at providing further density ratio estimation algorithms, along\n",
      "with alternative algorithms for the likelihood-free setup, such as approximate bayesian\n",
      "computation (abc).\n",
      "\u001b[0m\n",
      "\u001b[31mfuture works\n",
      "\u001b[0m\n",
      "\u001b[31mreferences\n",
      "\u001b[0m\n",
      "\u001b[31mcranmer, kyle, juan pavez, and gilles louppe. 2016. “approximating likelihood ratios\n",
      "with calibrated discriminative classifiers,” march. http://arxiv.org/abs/1506.02169v2.\n",
      "\u001b[0m\n",
      "\u001b[31mlouppe et al., (2016). carl: a likelihood-free inference toolbox. journal of open source software, 1(1), 11, doi:10.21105/joss.00011\n",
      "\u001b[0m\n",
      "\u001b[31m1\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "text = ''\n",
    "arr = []\n",
    "from pdfminer.high_level import extract_pages\n",
    "from pdfminer.layout import LTTextContainer\n",
    "for page_layout in extract_pages(Paper_interest):\n",
    "    for element in page_layout:\n",
    "        if isinstance(element, LTTextContainer):\n",
    "            score = Clean_PDF(element.get_text().lower())\n",
    "            #print(score)\n",
    "            if score == 0:\n",
    "                print(colored(element.get_text().lower(), 'green'))\n",
    "                arr.append(element.get_text())\n",
    "                text = text  + element.get_text() + ' '\n",
    "            else:\n",
    "                print(colored(element.get_text().lower(), 'red'))\n",
    "            #arr.append(element.get_text())\n",
    "                #text = text  + element.get_text() + ' '\n",
    "#arr = np.array(arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "37c1b67f-f975-42d9-8628-6e50c7bc3378",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'carl: a likelihood-free inference toolbox\\n Carl is a toolbox for likelihood-free inference in Python.\\n The likelihood function is the central object that summarizes the information from an\\nexperiment needed for inference of model parameters. It is key to many areas of science\\nthat report the results of classical hypothesis tests or confidence intervals using the (gen-\\neralized or profile) likelihood ratio as a test statistic. At the same time, with the advance\\nof computing technology, it has become increasingly common that a simulator (or gener-\\native model) is used to describe complex processes that tie parameters of an underlying\\ntheory and measurement apparatus to high-dimensional observations. However, directly\\nevaluating the likelihood function in these cases is often impossible or is computationally\\nimpractical.\\n In this context, the goal of this package is to provide tools for the likelihood-free setup,\\nincluding likelihood (or density) ratio estimation algorithms, along with helpers to carry\\nout inference on top of these.\\n Approximating likelihood ratios with calibrated classifiers\\n Methodological details regarding likelihood-free inference with calibrated classifiers can\\nbe found in the companion paper (Cranmer, Pavez, and Louppe 2016).\\n Future development aims at providing further density ratio estimation algorithms, along\\nwith alternative algorithms for the likelihood-free setup, such as Approximate Bayesian\\nComputation (ABC).\\n '"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "707287a2-7f71-4913-9fde-62d028b792a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = text.replace('-\\\\n','')\n",
    "text = text.replace('\\\\n',' ')\n",
    "text = text.replace('\\n',' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "36e2f5ec-abbd-4f5b-9f7d-b2ebadad961b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'carl: a likelihood-free inference toolbox  Carl is a toolbox for likelihood-free inference in Python.  The likelihood function is the central object that summarizes the information from an experiment needed for inference of model parameters. It is key to many areas of science that report the results of classical hypothesis tests or confidence intervals using the (gen- eralized or profile) likelihood ratio as a test statistic. At the same time, with the advance of computing technology, it has become increasingly common that a simulator (or gener- ative model) is used to describe complex processes that tie parameters of an underlying theory and measurement apparatus to high-dimensional observations. However, directly evaluating the likelihood function in these cases is often impossible or is computationally impractical.  In this context, the goal of this package is to provide tools for the likelihood-free setup, including likelihood (or density) ratio estimation algorithms, along with helpers to carry out inference on top of these.  Approximating likelihood ratios with calibrated classifiers  Methodological details regarding likelihood-free inference with calibrated classifiers can be found in the companion paper (Cranmer, Pavez, and Louppe 2016).  Future development aims at providing further density ratio estimation algorithms, along with alternative algorithms for the likelihood-free setup, such as Approximate Bayesian Computation (ABC).  '"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "811b0ebe-a62c-488c-a2ab-92874b958261",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0e3a746b-85e5-4a77-a0ca-923a7c5fa759",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = model(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "de608c9d-f8a2-47cf-ba8a-bba8ff6eb240",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_arr = []\n",
    "pos_arr = []\n",
    "tag_arr = []\n",
    "dep_arr = []\n",
    "for token in doc:\n",
    "    if token.is_alpha == True:\n",
    "        if token.is_stop == False:\n",
    "            doc_arr.append(str(token.lemma_).lower())\n",
    "            pos_arr.append(str(token.pos_))\n",
    "            tag_arr.append(str(token.tag_))\n",
    "            dep_arr.append(str(token.dep_))\n",
    "            \n",
    "doc_arr = np.array(doc_arr)\n",
    "pos_arr = np.array(pos_arr)\n",
    "tag_arr = np.array(tag_arr)\n",
    "dep_arr = np.array(dep_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2a730ac3-be74-4fa5-90ab-be2f6cc53467",
   "metadata": {},
   "outputs": [],
   "source": [
    "word_vec = np.zeros((128))\n",
    "counter = 0\n",
    "for P in range(len(doc_arr)):\n",
    "    best = s2v.get_best_sense(doc_arr[P])\n",
    "    if best != None:\n",
    "        vector = s2v[best]\n",
    "        word_vec = word_vec + vector\n",
    "        counter = counter + 1\n",
    "average_word_vec = word_vec / counter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cd976848-fdaa-404e-b431-2938d1c3fef9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 9.55035150e-02,  1.15096331e-01, -2.08605383e-01,  2.49549811e-02,\n",
       "       -1.63957822e-01, -1.82500976e-01,  3.54724237e-01,  7.59333492e-03,\n",
       "       -2.48715675e-01, -1.29130119e-01,  9.04052620e-02, -1.96562017e-02,\n",
       "       -1.06887785e-01,  1.67335286e-01,  5.94552689e-02, -1.73927741e-01,\n",
       "        1.01058351e-01,  1.18059406e-01,  1.68743259e-01, -1.16475592e-01,\n",
       "        3.31682169e-02,  9.56777588e-02,  6.33247315e-02, -2.84642323e-02,\n",
       "       -3.08546942e-02,  3.01342631e-02, -4.12084146e-02, -7.47804250e-03,\n",
       "       -7.09591153e-02,  5.45698228e-02,  1.59326689e-01, -1.17143740e-01,\n",
       "        4.34543131e-02,  7.56359400e-02, -9.59226468e-02, -2.25565155e-01,\n",
       "        5.10381894e-02,  3.24805607e-02, -1.06193912e-02, -2.48768986e-01,\n",
       "        4.65245815e-02,  1.55282108e-01, -1.21930094e-03, -7.17324917e-02,\n",
       "        4.36696426e-02,  1.15326463e-02, -3.67834128e-02, -1.76593735e-01,\n",
       "        6.75176867e-02,  2.11209233e-02,  1.35142540e-01, -1.28322116e-01,\n",
       "       -6.06392667e-02,  7.94088615e-02, -2.03138712e-01,  3.63751531e-02,\n",
       "        2.43109987e-01,  8.39793396e-02,  1.39078430e-01, -1.79788477e-04,\n",
       "        2.01115594e-02,  4.28525180e-03,  3.64241755e-02,  5.84547773e-02,\n",
       "       -1.75278435e-01, -1.09755468e-01, -6.93003075e-02, -1.22447644e-01,\n",
       "       -4.48705068e-02, -1.45680933e-01,  8.35946154e-02, -8.25395203e-02,\n",
       "        1.52729420e-01, -4.83657369e-02,  1.68923267e-01, -1.12595358e-01,\n",
       "       -7.15192844e-02, -8.18869101e-02,  1.76219856e-01,  1.87042197e-01,\n",
       "       -2.12030321e-01, -6.71912010e-02, -1.17022867e-01, -1.50979374e-01,\n",
       "       -6.66642648e-02, -9.13902530e-02, -3.87035155e-02,  1.15915811e-01,\n",
       "        1.58593541e-01, -8.44806622e-02,  2.93956778e-01, -3.40997325e-01,\n",
       "        6.90735601e-02,  1.40914126e-01,  3.74966874e-02,  6.79786831e-02,\n",
       "       -1.75262695e-02, -3.03537340e-02, -4.80065459e-02,  9.40600290e-03,\n",
       "        9.03737119e-02, -1.17992231e-01,  3.12285330e-02, -1.57927635e-01,\n",
       "        1.19087396e-01,  4.38428184e-01,  1.79741147e-01,  1.21901020e-01,\n",
       "       -9.96932026e-02,  1.63675397e-01, -1.14552108e-01,  3.81908563e-02,\n",
       "       -4.20511828e-02,  1.52483178e-01, -7.45718667e-02, -1.51542867e-01,\n",
       "        2.97868908e-01, -9.46362488e-02,  9.00358355e-02,  2.72167369e-01,\n",
       "        3.43474404e-01, -5.28707174e-02, -5.57154015e-03, -1.41729465e-01,\n",
       "       -1.25432119e-01, -1.31353319e-01,  2.64001263e-01,  1.16198254e-01])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "average_word_vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2645bf88-ca6c-499b-aac0-b290b4b8f2b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_vecs = np.load('../..//Testing PlayGround/sense2vec_vector_space.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ae1caa51-b591-440e-9293-843c9e700a18",
   "metadata": {},
   "outputs": [],
   "source": [
    "def FindWord(number):\n",
    "    counter = 0\n",
    "    for i in s2v.items():\n",
    "        if counter == number:\n",
    "            name = i[0]\n",
    "            #print(i[0])\n",
    "            break\n",
    "        counter = counter + 1\n",
    "    return name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "bd4b88c1-753f-45f1-9498-8183c00951f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "73f625c5-8e6e-4df0-bc24-8876ea765b7d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "10000\n",
      "20000\n",
      "30000\n",
      "40000\n",
      "50000\n",
      "60000\n",
      "70000\n",
      "80000\n",
      "90000\n",
      "100000\n",
      "110000\n",
      "120000\n",
      "130000\n",
      "140000\n",
      "150000\n",
      "160000\n",
      "170000\n",
      "180000\n",
      "190000\n",
      "200000\n",
      "210000\n",
      "220000\n",
      "230000\n",
      "240000\n",
      "250000\n",
      "260000\n",
      "270000\n",
      "280000\n",
      "290000\n",
      "300000\n",
      "310000\n",
      "320000\n",
      "330000\n",
      "340000\n",
      "350000\n",
      "360000\n",
      "370000\n",
      "380000\n",
      "390000\n",
      "400000\n",
      "410000\n",
      "420000\n",
      "430000\n",
      "440000\n",
      "450000\n",
      "460000\n",
      "470000\n",
      "480000\n",
      "490000\n",
      "500000\n",
      "510000\n",
      "520000\n",
      "530000\n",
      "540000\n",
      "550000\n",
      "560000\n",
      "570000\n",
      "580000\n",
      "590000\n",
      "600000\n",
      "610000\n",
      "620000\n",
      "630000\n",
      "640000\n",
      "650000\n",
      "660000\n",
      "670000\n",
      "680000\n",
      "690000\n",
      "700000\n",
      "710000\n",
      "720000\n",
      "730000\n",
      "740000\n",
      "750000\n",
      "760000\n",
      "770000\n",
      "780000\n",
      "790000\n",
      "800000\n",
      "810000\n",
      "820000\n",
      "830000\n",
      "840000\n",
      "850000\n",
      "860000\n",
      "870000\n",
      "880000\n",
      "890000\n",
      "900000\n",
      "910000\n",
      "920000\n",
      "930000\n",
      "940000\n",
      "950000\n",
      "960000\n",
      "970000\n",
      "980000\n",
      "990000\n",
      "1000000\n",
      "1010000\n",
      "1020000\n",
      "1030000\n",
      "1040000\n",
      "1050000\n",
      "1060000\n",
      "1070000\n",
      "1080000\n",
      "1090000\n",
      "1100000\n",
      "1110000\n",
      "1120000\n",
      "1130000\n",
      "1140000\n",
      "1150000\n",
      "1160000\n",
      "1170000\n",
      "1180000\n"
     ]
    }
   ],
   "source": [
    "cos_sim = np.zeros((1187453))\n",
    "for j in range(1187453):\n",
    "    if j%10000==0:\n",
    "        print(j)\n",
    "    cos_sim[j] = cosine_similarity(np.array([average_word_vec]), np.array([all_vecs[j]]) )[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ae027eb7-356b-4528-867e-5c6a30c887d2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1187453,)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cos_sim.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "d0b30ebf-8a98-4e86-903f-b0dcdb695f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "K = 25\n",
    "topten_cos_sim = np.argsort(cos_sim)[-K:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8bef4d38-4e96-4898-b8f1-3358b9ca9946",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([  65009,  634069,  403308,  742921,  237100,  822708,  506234,\n",
       "        279873,  783829,  343212,   87238,  312260,  477739,  847657,\n",
       "       1140015, 1153015, 1037369, 1073625,  926559,  480984, 1053058,\n",
       "        182533,  751396, 1009895,  797982])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "topten_cos_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7151dc89-29f1-4d66-8ce7-2381e8c5dff3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hence|ADV\n",
      "simple_way|NOUN\n",
      "approximation|NOUN\n",
      "general_idea|NOUN\n",
      "estimation|NOUN\n",
      "usefully|ADV\n",
      "Source|Rating|Link|VERB\n",
      "ie|PUNCT\n",
      "which|ADJ\n",
      "heuristic|NOUN\n",
      "methods|NOUN\n",
      "i.e.|X\n",
      "course|NOUN\n",
      "i.e|NOUN\n",
      "useful_way|NOUN\n",
      "perhaps|ADV\n",
      "http://folkarps.com/|X\n",
      "moreover|ADV\n",
      "parameters|NOUN\n",
      "upshot|NOUN\n",
      "basic_idea|NOUN\n",
      "simply|ADV\n",
      "I.e.|X\n",
      "method|NOUN\n",
      "precisely|ADV\n"
     ]
    }
   ],
   "source": [
    "for i in range(K):\n",
    "    number = topten_cos_sim[i]\n",
    "    name = FindWord(number)\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16d09851-8fa7-4e0f-b285-39503b88a05b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4c8912a-82b0-46eb-b511-67471a33f0f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269d3cbd-b05a-4b19-8075-20576159a67e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "d6b26057-e1f7-4de4-b57e-d491ee7a2ddf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hehhehehe_Hehhehehe|NOUN\n",
      "--_Michael_Scott|PERSON\n",
      "^Shurima|PUNCT\n",
      "^--lenin|PUNCT\n",
      "http://www.reddit.com/r/podemos/comments/337qnh/705_amnist%C3%ADa_fiscal_2012_espa%C3%B1a_exijo_al_gobierno/|X\n",
      "|Link|User|\n",
      "http://www.reddit.com/r/FIFA/comments/2ygsat/official_ea_announcement_on_price_ranges/|X\n",
      "Misleading**.|NOUN\n",
      "twitch.tv/Tentanman_|X\n",
      "&gt;***Post|NOUN\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    number = topten_cos_sim[i]\n",
    "    name = FindWord(number)\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "d184390f-2d1f-4eed-8897-c3b020b87acd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Carth|GPE\n",
      "Roland_Crakehall|PERSON\n",
      "pageau|NOUN\n",
      "**Power_Supply**_|_[EVGA_SuperNOVA_NEX_650W_80+_Gold_Certified_Fully-Modular_ATX_Power_Supply](http://pcpartpicker.com/part/evga-power-supply-120g10650xr)_|_$64.99_@_NCIX_US_\n",
      "US/contact-us|NOUN\n",
      "Pumba|NOUN\n",
      "booooooooooo|NOUN\n",
      "Kloppo|NOUN\n",
      "Aug_19|DATE\n",
      "^^/r/alot|PUNCT\n"
     ]
    }
   ],
   "source": [
    "for i in range(10):\n",
    "    number = topten_cos_sim[i]\n",
    "    name = FindWord(number)\n",
    "    print(name)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
