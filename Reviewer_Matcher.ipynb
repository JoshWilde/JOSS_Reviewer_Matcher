{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "162e2783-1e73-4e84-9a6d-98becf60f145",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from nltk import word_tokenize\n",
    "from nltk import download\n",
    "from nltk.corpus import stopwords\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "%matplotlib inline\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a507d08-65d6-4aa0-9ff3-3734503c039a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from JOSS_PDF_Cleaner import Clean_PDF\n",
    "import re\n",
    "from termcolor import colored\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8804f7bd-81aa-4856-92f8-a38934dbab17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import glob\n",
    "import string\n",
    "import glob\n",
    "from tqdm import tqdm \n",
    "#import pdfminer\n",
    "from pdfminer.high_level import extract_text\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk \n",
    "#nltk.download('stopwords')\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import nltk\n",
    "#nltk.download('punkt')\n",
    "from nltk import sent_tokenize\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "#nltk.download('wordnet')\n",
    "from nltk.probability import FreqDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bac7bdc7-55e9-482d-b60e-d869b88bd8cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/sunilmcesh/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/sunilmcesh/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/sunilmcesh/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#from Master_Methods import *\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import glob\n",
    "import string\n",
    "import glob\n",
    "from tqdm import tqdm \n",
    "#import pdfminer\n",
    "from pdfminer.high_level import extract_text\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk \n",
    "nltk.download('stopwords')\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk import sent_tokenize\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "from nltk.probability import FreqDist\n",
    "from JOSS_PDF_Cleaner import Clean_PDF\n",
    "import re\n",
    "from termcolor import colored\n",
    "import spacy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d7010c21-b0e3-40e2-821b-a440f0a7f99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7af63a-561a-4afb-82c4-3047dacc9529",
   "metadata": {},
   "source": [
    "# Import Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "66357afc-9d23-494b-a1ce-4f7f12682887",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Volumes/Seagate Backup Plus Drive/JOSS Project/joss-papers-master/joss-papers-master/joss.00592/10.21105.joss.00592.pdf\n"
     ]
    }
   ],
   "source": [
    "# https://zenodo.org/record/3631674#.YeglfP7P2Uk\n",
    "df = pd.read_csv('/Volumes/Seagate Backup Plus Drive/JOSS Project/wiki_tfidf_terms.csv')\n",
    "df_reviewers = pd.read_csv('../JOSS_Reviewer_Matcher/Data/JOSS Table Test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdbbee41-a44b-44df-892b-6125d96410b2",
   "metadata": {},
   "source": [
    "# Select Paper to find Reviewers for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6f2c8b7-a13b-4048-ad02-38feb953934b",
   "metadata": {},
   "outputs": [],
   "source": [
    "PAPER_OF_INTEREST_FNAME  = glob.glob('/Volumes/Seagate Backup Plus Drive/JOSS Project/joss-papers-master/*/*/*.pdf')\n",
    "K = 260\n",
    "Paper_interest = PAPER_OF_INTEREST_FNAME[K] # Replace with path to paper of interest\n",
    "print(Paper_interest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd9fce1-cbaf-4cde-9811-32a0ff76ad3e",
   "metadata": {},
   "source": [
    "# Printing Paper of Interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8da8f57f-0a85-42ad-98d5-adc31af6ac8d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def MakeGreenRedText(Paper_interest, printer=True):\n",
    "    texts = ''\n",
    "    arr = []\n",
    "    from pdfminer.high_level import extract_pages\n",
    "    from pdfminer.layout import LTTextContainer\n",
    "    for page_layout in extract_pages(Paper_interest):\n",
    "        for element in page_layout:\n",
    "            if isinstance(element, LTTextContainer):\n",
    "                score = Clean_PDF(element.get_text().lower())\n",
    "            #print(score)\n",
    "                if score == 0:\n",
    "                    if printer == True:\n",
    "                        print(colored(element.get_text().lower(), 'green'))\n",
    "                    arr.append(element.get_text())\n",
    "                    texts = texts  + element.get_text() + ' '\n",
    "                else:\n",
    "                    if printer == True:\n",
    "                        print(colored(element.get_text().lower(), 'red'))\n",
    "            #arr.append(element.get_text())\n",
    "            #texts = texts  + element.get_text() + ' '\n",
    "    arr = np.array(arr)\n",
    "    return texts, arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f0ae3840-553a-4cf6-9b1c-cc4db3241fc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31msummary\n",
      "\u001b[0m\n",
      "\u001b[31mdoi: 10.21105/joss.00592\n",
      "\u001b[0m\n",
      "\u001b[31msoftware\n",
      "\u001b[0m\n",
      "\u001b[31m• review\n",
      "• repository\n",
      "• archive\n",
      "\u001b[0m\n",
      "\u001b[31msubmitted: 12 december 2017\n",
      "published: 15 june 2018\n",
      "\u001b[0m\n",
      "\u001b[31mlicense\n",
      "authors of papers retain copyright\n",
      "and release the work under a cre-\n",
      "ative commons attribution 4.0 in-\n",
      "ternational license (cc-by).\n",
      "\u001b[0m\n",
      "\u001b[32maronnax: an idealised isopycnal ocean model\n",
      "\u001b[0m\n",
      "\u001b[31medward w. doddridge1 and alexey radul2\n",
      "\u001b[0m\n",
      "\u001b[31m1 earth, atmospheric and planetary science, massachusetts institute of technology, cambridge,\n",
      "ma, usa 2 brain and cognitive science, massachusetts institute of technology, cambridge, ma,\n",
      "usa\n",
      "\u001b[0m\n",
      "\u001b[32maronnax is a highly idealised model for simulating large-scale (100-1000km) flows in the\n",
      "ocean. aronnax is intended for theoretical and empirical oceanographers, as a (relatively)\n",
      "fast and easy-to-use simulation model, bridging the gap between pencil-and-paper on one\n",
      "hand, and more faithful (and complex) computational models on the other. the numerical\n",
      "core is written in fortran to improve performance, and wrapped in python to improve\n",
      "usability.\n",
      "\u001b[0m\n",
      "\u001b[32maronnax is an isopyncal model: it approximates the ocean as a number of discrete homo-\n",
      "geneous layers with spatially variable thicknesses. these layers are stacked vertically and\n",
      "the density difference between neighbouring layers is specified by the user. other widely\n",
      "used vertical coordinates require solving the equations of motion at specified vertical lev-\n",
      "els where the density is allowed to vary (s. m. griffies et al. 2000). representing the\n",
      "large-scale ocean circulation in layers contributes to aronnax’s speed: one needs only a\n",
      "few layers to achieve the same fidelity as 50 or more fixed vertical levels (stewart et al.\n",
      "2017).\n",
      "\u001b[0m\n",
      "\u001b[32maronnax serves three distinct purposes. firstly, many of the studies that use a model\n",
      "like aronnax do not provide the source code, see e.g. (davis, lique, and johnson 2014,\n",
      "février, sirven, and herbaut (2007), johnson and marshall (2002), stern (1998)). this\n",
      "increases the likelihood that coding errors go undetected, and requires that each research\n",
      "group spend time developing their own idealised model. aronnax solves these problems\n",
      "by providing an open source, tested model for the community to use. secondly, aronnax\n",
      "furthers the goals of scientific reproducibility since a simulation can be entirely specified\n",
      "with a set of input files and a version number. thirdly, aronnax provides an easy-to-use\n",
      "model that may be included in future modelling hierarchies with minimal effort, thereby\n",
      "enabling new research questions to be addressed.\n",
      "\u001b[0m\n",
      "\u001b[32mthere are a number of other publicly available ocean models. of these the most abundant\n",
      "are general circulation models and quasigeostrophic models. general circulation models\n",
      "such as nemo, gold, mom6, and mitgcm solve a less idealised version of the navier-\n",
      "stokes equations and can be coupled with sea ice and atmospheric models to create\n",
      "fully coupled climate models. because the underlying equations are derived with fewer\n",
      "approximations these models can more faithfully simulate a wider range of flow regimes.\n",
      "however, this comes at a price; general circulation models are extremely complex, with\n",
      "numerous free parameters that must be specified, often prior to compiling the source code.\n",
      "it is possible to use most of these models in idealised configurations, but doing so requires\n",
      "a substantial investment of time from the user, and non-trivial computing resources. in\n",
      "comparison, aronnax is easy to install and cheap to run.\n",
      "\u001b[0m\n",
      "\u001b[32mthe other abundant class of models is quasigeostrophic models. geostrophy is a balance\n",
      "between the coriolis force and the horizontal gradient of the pressure field; flows in\n",
      "which the coriolis force and the horizontal pressure gradient almost balance are known\n",
      "\u001b[0m\n",
      "\u001b[31mdoddridge et al., (2018). aronnax: an idealised isopycnal ocean model. journal of open source software, 3(26), 592.\n",
      "https://doi.org/10.21105/joss.00592\n",
      "\u001b[0m\n",
      "\u001b[31m1\n",
      "\u001b[0m\n",
      "\u001b[32mas quasigeostrophic. quasigeostrophic models of the ocean make use of this near balance\n",
      "and a number of other assumptions to simplify the equations of motion from a system of\n",
      "five coupled partial differential equations to a single partial differential equation (vallis\n",
      "2006). quasigeostrophic models range in complexity from qgcm, which includes the\n",
      "option of a coupled atmosphere, to doubly periodic quasigeostrophic turbulence models\n",
      "such as pyqg and qgmodel. while quasigeostrophic models are extremely useful, there\n",
      "are some problems for which they are ill-suited. for example, the adjustment of the ocean\n",
      "circulation often occurs through ageostrophic motions such as boundary waves (johnson\n",
      "and marshall 2002), which are not represented in quasigeostrophic models. in addition,\n",
      "quasigeostrophic models are limited in their representation of sloping bathymetry (depth\n",
      "of the sea floor). for these reasons it may be preferable to use an idealised non-linear\n",
      "model such as aronnax.\n",
      "\u001b[0m\n",
      "\u001b[32maronnax is mit licensed and can be retrieved from github at https://github.com/\n",
      "edoddridge/aronnax.\n",
      "\u001b[0m\n",
      "\u001b[31mreferences\n",
      "\u001b[0m\n",
      "\u001b[31mdavis, peter e d, camille lique, and helen l. johnson. 2014. “on the link between arctic\n",
      "sea ice decline and the freshwater content of the beaufort gyre: insights from a simple\n",
      "process model.” j. clim. 27 (21):8170–84. https://doi.org/10.1175/jcli-d-14-00090.1.\n",
      "\u001b[0m\n",
      "\u001b[31mfévrier, sabine, jérôme sirven, and christophe herbaut. 2007. “interaction of a coastal\n",
      "j. phys.\n",
      "kelvin wave with the mean state in the gulf stream separation area.”\n",
      "oceanogr. 37 (6):1429–44. https://doi.org/10.1175/jpo3062.1.\n",
      "\u001b[0m\n",
      "\u001b[31mgriffies, stephen m, claus böning, frank o. bryan, eric p. chassignet, rudiger gerdes,\n",
      "hiroyasu hasumi, anthony hirst, anne-marie treguier, and david webb. 2000. “de-\n",
      "velopments in ocean climate modelling.” ocean model. 2 (2000):123–92. http://www.\n",
      "sciencedirect.com/science/article/pii/s1463500300000147.\n",
      "\u001b[0m\n",
      "\u001b[31mjohnson, helen l., and david p. marshall.\n",
      "atlantic response to thermohaline variability.”\n",
      "http://dx.doi.org/10.1175/1520-0485(2002)032{\\%}3c1121:atftsa{\\%}3e2.0.co;2.\n",
      "\u001b[0m\n",
      "\u001b[32m“a theory for the surface\n",
      "oceanogr., 1121–32.\n",
      "\u001b[0m\n",
      "\u001b[32mj. phys.\n",
      "\u001b[0m\n",
      "\u001b[31m2002.\n",
      "\u001b[0m\n",
      "\u001b[31mstern, melvin e. 1998. “separation of a density current from the bottom of a con-\n",
      "tinental slope.” j. phys. oceanogr. 28 (10):2040–9. https://doi.org/10.1175/1520-\n",
      "0485(1998)028<2040:soadcf>2.0.co;2.\n",
      "\u001b[0m\n",
      "\u001b[31mstewart, k.d., a.mcc. hogg, s.m. griffies, a.p. heerdegen, m.l. ward, p. spence, and\n",
      "m.h. england. 2017. “vertical resolution of baroclinic modes in global ocean models.”\n",
      "ocean model. 113 (may). elsevier ltd:50–65. https://doi.org/10.1016/j.ocemod.2017.03.\n",
      "012.\n",
      "\u001b[0m\n",
      "\u001b[31mvallis, geoffrey k. 2006. atmospheric and oceanic fluid dynamics. cambridge, uk:\n",
      "cambridge university press.\n",
      "\u001b[0m\n",
      "\u001b[31mdoddridge et al., (2018). aronnax: an idealised isopycnal ocean model. journal of open source software, 3(26), 592.\n",
      "https://doi.org/10.21105/joss.00592\n",
      "\u001b[0m\n",
      "\u001b[31m2\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "texts, arr = MakeGreenRedText(Paper_interest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ad4c0fe-d5c1-4f79-afd5-d91f3a0c1d73",
   "metadata": {},
   "source": [
    "# Idea 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58093534-d906-4261-8d9f-3511748813b5",
   "metadata": {},
   "source": [
    "# Idea 1 Functions\n",
    "Hidden for clarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "94972f48-7f00-4204-b096-aa2d4f163897",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Idea 1\n",
    "def Get_Lemma_Words(POI_PDF):\n",
    "  text = str(POI_PDF)\n",
    "  text2 = text.split()\n",
    "  words_no_punc = []\n",
    "\n",
    "  for w in text2:\n",
    "    if w.isalpha():\n",
    "      words_no_punc.append(w.lower())\n",
    "  from nltk.corpus import stopwords\n",
    "  stopwords = stopwords.words('english')  \n",
    "  clean_words = []\n",
    "  for w in words_no_punc:\n",
    "    if w not in stopwords:\n",
    "      clean_words.append(w)\n",
    "  clean_words_arr = ''\n",
    "  for i in range(len(clean_words)):\n",
    "    clean_words_arr = clean_words_arr + ' ' + str(clean_words[i])\n",
    "\n",
    "  string_for_lemmatizing = clean_words_arr\n",
    "  lemmatizer = WordNetLemmatizer()\n",
    "  words_2 = word_tokenize(string_for_lemmatizing)\n",
    "  lemmatized_words = [lemmatizer.lemmatize(word) for word in words_2]\n",
    "\n",
    "  lemmatized_words_arr = ''\n",
    "  for i in range(len(lemmatized_words)):\n",
    "    lemmatized_words_arr = lemmatized_words_arr + ' ' + str(lemmatized_words[i])\n",
    "  words = word_tokenize(lemmatized_words_arr)\n",
    "\n",
    "  return words\n",
    "\n",
    "def Get_reviewer_sample_tf(Paper_interest, df_reviewers=df_reviewers,num_suggestions=5, num_top20=20):\n",
    "    POI_PDF = [extract_text(Paper_interest)]\n",
    "    text = str(POI_PDF)\n",
    "    words =  Get_Lemma_Words(POI_PDF)\n",
    "    #print(len(words))\n",
    "    fdist = FreqDist(words)\n",
    "    X = np.array(fdist.most_common())\n",
    "    top20_tf = X[:num_top20,0]\n",
    "    match_arr = Compare_topics(top20_tf, df_reviewers)\n",
    "    top5_reviewers = np.argsort(match_arr)[-num_suggestions:]\n",
    "\n",
    "    \n",
    "    all_usernames = []\n",
    "    all_domains = []\n",
    "    all_num_matched_words = []\n",
    "    all_matched_words = []\n",
    "    for i in range(num_suggestions):\n",
    "      K = -1*(i+1)\n",
    "      index = top5_reviewers[K]\n",
    "      #print(i)\n",
    "      t =df_reviewers.iloc[index+1]['Domains/topic areas you are comfortable reviewing'].lower()\n",
    "      \n",
    "      all_usernames.append(df_reviewers.username.iloc[index+1])\n",
    "      all_domains.append(t)\n",
    "      all_num_matched_words.append(match_arr[index])\n",
    "\n",
    "      uniarr = Split_columns(t)\n",
    "      matched_words = []\n",
    "      #print(uniarr)\n",
    "      for j in range(len(uniarr)):\n",
    "        for k in range(len(top20_tf)):\n",
    "          if uniarr[j] == top20_tf[k]:\n",
    "            matched_words.append(uniarr[j])\n",
    "      all_matched_words.append(matched_words)\n",
    "\n",
    "    #df_reviewers.username.iloc[+1]\n",
    "\n",
    "    return all_usernames, all_domains, all_num_matched_words, all_matched_words\n",
    "\n",
    "def Compare_topics(top20, df_reviewers):\n",
    "  length = df_reviewers.shape[0] - 1\n",
    "  match_arr = np.zeros(length)\n",
    "  for i in range(length):\n",
    "    if pd.isna(df_reviewers['Domains/topic areas you are comfortable reviewing'].str.lower().values[1+i]) == False:\n",
    "      t = df_reviewers['Domains/topic areas you are comfortable reviewing'].str.lower().values[1+i]\n",
    "      #print(i)\n",
    "      uniarr = Split_columns(t)\n",
    "      for j in range(len(uniarr)):\n",
    "        for k in range(len(top20)):\n",
    "          if uniarr[j] == top20[k]:\n",
    "            match_arr[i] = match_arr[i] + 1\n",
    "  return match_arr\n",
    "\n",
    "def Split_columns(t):\n",
    "  txt = \" \".join(\"\".join([\" \" if ch in string.punctuation else ch for ch in t]).split())\n",
    "  sol1 = np.char.split(txt, ' ')\n",
    "  txt_arr  = array_of_lists_to_array(sol1)\n",
    "  uniarr = np.unique(txt_arr)\n",
    "  return uniarr\n",
    "\n",
    "def array_of_lists_to_array(arr):\n",
    "    return np.apply_along_axis(lambda a: np.array(a[0]), -1, arr[..., None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "e1c9b134-8138-4cf4-b03b-b73b1ddf1417",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_usernames_Idea_1, all_domains_Idea_1, all_num_matched_words_Idea_1, all_matched_words_Idea_1 = Get_reviewer_sample_tf(Paper_interest)\n",
    "\n",
    "def summatation_bot_tf(all_usernames_Idea_1, all_domains_Idea_1, all_num_matched_words_Idea_1, all_matched_words_Idea_1):\n",
    "  length = len(all_usernames)\n",
    "  message = 'Hello. \\nI have found ' + str(length) + ' possible reviewers for this paper.' +'\\n\\n'\n",
    "  for i in range(length):\n",
    "    ps = 'I believe ' + colored(all_usernames[i], 'green') + ' will make a good reviewer for this paper because they have matched ' + colored(str(int(all_num_matched_words[i])), 'blue') +  ' words from their comfortable domain topics with the top 20 most frequent words in the paper. These matched words are ' + colored(str(all_matched_words[i]), 'blue') +'.\\nFrom their topics domain: ' + colored(str(all_domains[i].replace('\\n', ', ')), 'red') +'.\\n'\n",
    "    message = message + ps + '\\n'\n",
    "  print(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "486e292b-477f-47d5-aad0-f0ea84388a1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello. \n",
      "I have found 5 possible reviewers for this paper.\n",
      "\n",
      "I believe \u001b[32mritwikagarwal\u001b[0m will make a good reviewer for this paper because they have matched \u001b[34m3\u001b[0m words from their comfortable domain topics with the top 20 most frequent words in the paper. These matched words are \u001b[34m['open', 'source', 'use']\u001b[0m.\n",
      "From their topics domain: \u001b[31mict ,  emerging technologies,  advancement in programming languages,  use of ai and open source in climate change,  open source software\u001b[0m.\n",
      "\n",
      "I believe \u001b[32maaronpeikert\u001b[0m will make a good reviewer for this paper because they have matched \u001b[34m3\u001b[0m words from their comfortable domain topics with the top 20 most frequent words in the paper. These matched words are \u001b[34m['equation', 'model', 'open']\u001b[0m.\n",
      "From their topics domain: \u001b[31mmachine learning,  - model selection,  - structural equation modelling,  - multimodel inference,  - hierarchical data,  ,  open science,  - reproducibility,  - dynamic document generation,  - containers,  - version control\u001b[0m.\n",
      "\n",
      "I believe \u001b[32mashok-arora\u001b[0m will make a good reviewer for this paper because they have matched \u001b[34m2\u001b[0m words from their comfortable domain topics with the top 20 most frequent words in the paper. These matched words are \u001b[34m['open', 'source']\u001b[0m.\n",
      "From their topics domain: \u001b[31martificial intelligence, computer vision, open source software, cmake\u001b[0m.\n",
      "\n",
      "I believe \u001b[32mhello\u001b[0m will make a good reviewer for this paper because they have matched \u001b[34m2\u001b[0m words from their comfortable domain topics with the top 20 most frequent words in the paper. These matched words are \u001b[34m['open', 'source']\u001b[0m.\n",
      "From their topics domain: \u001b[31mopen source, big data, data science\u001b[0m.\n",
      "\n",
      "I believe \u001b[32mGregoryAshton\u001b[0m will make a good reviewer for this paper because they have matched \u001b[34m2\u001b[0m words from their comfortable domain topics with the top 20 most frequent words in the paper. These matched words are \u001b[34m['open', 'source']\u001b[0m.\n",
      "From their topics domain: \u001b[31mopen source software, statistics, bayesian inference/stochastic sampling, data visualization\u001b[0m.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "summatation_bot_tf(all_usernames, all_domains, all_num_matched_words, all_matched_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a48ce4-2ffb-4483-91ed-3572b06a182d",
   "metadata": {},
   "source": [
    "# Idea 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bf7df93-6142-4db8-8b5a-ac32cd44a338",
   "metadata": {},
   "source": [
    "# Idea 2 Functions\n",
    "Hidden for clarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3ecce64a-5028-4ad9-a6d6-3e7987a256bd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Idea 2\n",
    "\n",
    "def Get_reviewer_sample_tf_idf(Paper_interest, df=df, df_reviewers=df_reviewers, num_suggestions=5, num_top20=20):\n",
    "    POI_PDF = [extract_text(Paper_interest)]\n",
    "    text = str(POI_PDF)\n",
    "    words = Get_Lemma_Words(POI_PDF)\n",
    "    #print(len(words))\n",
    "    fdist = FreqDist(words)\n",
    "    X = np.array(fdist.most_common())\n",
    "    tf_idf_arr_names, tf_idf_arr_floats = determine_wiki_td_idf(X, df=df)\n",
    "    #print('determined wiki')\n",
    "    num_arr = np.array(tf_idf_arr_floats)\n",
    "    tf_idf_arr_names_arr = np.array(tf_idf_arr_names)\n",
    "    top20_tf_idf = tf_idf_arr_names_arr[np.argsort(num_arr)[:20]]\n",
    "    match_arr = Compare_topics(top20_tf_idf, df_reviewers)\n",
    "    top5_reviewers = np.argsort(match_arr)[-num_suggestions:]\n",
    "    \n",
    "    \n",
    "    all_usernames = []\n",
    "    all_domains = []\n",
    "    all_num_matched_words = []\n",
    "    all_matched_words = []\n",
    "    for i in range(num_suggestions):\n",
    "      K = -1*(i+1)\n",
    "      index = top5_reviewers[K]\n",
    "      #print(i)\n",
    "      t =df_reviewers.iloc[index+1]['Domains/topic areas you are comfortable reviewing'].lower()\n",
    "      \n",
    "      all_usernames.append(df_reviewers.username.iloc[index+1])\n",
    "      all_domains.append(t)\n",
    "      all_num_matched_words.append(match_arr[index])\n",
    "\n",
    "      uniarr = Split_columns(t)\n",
    "      matched_words = []\n",
    "      #print(uniarr)\n",
    "      for j in range(len(uniarr)):\n",
    "        for k in range(len(top20_tf_idf)):\n",
    "          if uniarr[j] == top20_tf_idf[k]:\n",
    "            matched_words.append(uniarr[j])\n",
    "      all_matched_words.append(matched_words)\n",
    "\n",
    "    #df_reviewers.username.iloc[+1]\n",
    "\n",
    "    return all_usernames, all_domains, all_num_matched_words, all_matched_words\n",
    "\n",
    "def determine_wiki_td_idf(x, df=df):\n",
    "    tf_idf_arr_names = []\n",
    "    tf_idf_arr_floats = []\n",
    "    for i in range(len(x)):\n",
    "        if df[df['token'] ==x[i][0]].frequency.empty == False:\n",
    "            wiki_tf = df[df['token'] ==x[i][0]].frequency.values[0]\n",
    "            doc_tf = int(x[i][1])\n",
    "            tf_idf = np.log(wiki_tf/doc_tf)\n",
    "            tf_idf_arr_names.append(x[i][0])\n",
    "            tf_idf_arr_floats.append(tf_idf)\n",
    "    return tf_idf_arr_names, tf_idf_arr_floats\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b2802726-c921-4979-a667-0abc6ad24a50",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_usernames, all_domains, all_num_matched_words, all_matched_words = Get_reviewer_sample_tf_idf(Paper_interest)\n",
    "\n",
    "def summatation_bot_tf_idf(all_usernames, all_domains, all_num_matched_words, all_matched_words):\n",
    "  length = len(all_usernames)\n",
    "  message = 'Hello. \\nI have found ' + str(length) + ' possible reviewers for this paper.' +'\\n\\n'\n",
    "  for i in range(length):\n",
    "    ps = 'I believe ' + colored(all_usernames[i], 'green') + ' will make a good reviewer for this paper because they have matched ' + colored(str(int(all_num_matched_words[i])), 'blue') +  ' words from their comfortable domain topics with the top 20 most frequent words in the paper. These matched words are ' + colored(str(all_matched_words[i]), 'blue') +'.\\nFrom their topics domain: ' + colored(str(all_domains[i].replace('\\n', ', ')), 'red') +'.\\n'\n",
    "    message = message + ps + '\\n'\n",
    "  print(message)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8e45fc8f-bf32-499d-8e21-d01fd91289a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello. \n",
      "I have found 5 possible reviewers for this paper.\n",
      "\n",
      "I believe \u001b[32meteq\u001b[0m will make a good reviewer for this paper because they have matched \u001b[34m1\u001b[0m words from their comfortable domain topics with the top 20 most frequent words in the paper. These matched words are \u001b[34m['reproducibility']\u001b[0m.\n",
      "From their topics domain: \u001b[31masttronomy/astrophysics, other physics depending on complexity, reproducibility-related tools\u001b[0m.\n",
      "\n",
      "I believe \u001b[32mcsadorf\u001b[0m will make a good reviewer for this paper because they have matched \u001b[34m1\u001b[0m words from their comfortable domain topics with the top 20 most frequent words in the paper. These matched words are \u001b[34m['github']\u001b[0m.\n",
      "From their topics domain: \u001b[31mdata management,  workflow management,  molecular dynamics,  metropolis monte carlo,  crystallization,  machine learning,  neural networks,  parallelism,  cuda,  mpi,  high-performance clusters,  supercomputers,  github actions,  circle ci,  pandas,  project jupyter,  materials science,  chemical engineering,  scientific software\u001b[0m.\n",
      "\n",
      "I believe \u001b[32mSaumikDana\u001b[0m will make a good reviewer for this paper because they have matched \u001b[34m1\u001b[0m words from their comfortable domain topics with the top 20 most frequent words in the paper. These matched words are \u001b[34m['fortran']\u001b[0m.\n",
      "From their topics domain: \u001b[31mgeology, geophysics, earth surface dynamics, hydrology, landslides, natural hazards, terrain analysis ,  computational science and engineering, geophysics, high-performance computing ,  computational fracture mechanics, applied mathematics, c++, asynchronous and task-based programming ,  mechanics, computational science, biomechanics, computational oncology, applied mathematics ,  applied math, numerical optimization, numerical linear algebra, fortran, python, matlab, julia, teaching\u001b[0m.\n",
      "\n",
      "I believe \u001b[32mzbeekman\u001b[0m will make a good reviewer for this paper because they have matched \u001b[34m1\u001b[0m words from their comfortable domain topics with the top 20 most frequent words in the paper. These matched words are \u001b[34m['fortran']\u001b[0m.\n",
      "From their topics domain: \u001b[31mcomputational fluid dynamics (cfd),  fortran tools, utilities, libraries, solvers, programs,  ci/cd & devops for computational science and engineering,  hpc applications, frameworks, & tools,  numerical solution of pdes,  performance analysis & performance engineering tools,  multi-physics frameworks & applications,  legacy application refactoring tools,  parallel & pgas runtimes/rtls,  package managers,  macos tools & utilities\u001b[0m.\n",
      "\n",
      "I believe \u001b[32mbenmarwick\u001b[0m will make a good reviewer for this paper because they have matched \u001b[34m1\u001b[0m words from their comfortable domain topics with the top 20 most frequent words in the paper. These matched words are \u001b[34m['reproducibility']\u001b[0m.\n",
      "From their topics domain: \u001b[31marchaeology, social science, geoscience, humanities, reproducibility\u001b[0m.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "summatation_bot_tf_idf(all_usernames, all_domains, all_num_matched_words, all_matched_words)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ccc22cc9-071e-4ac5-88c0-90af99c902f9",
   "metadata": {},
   "source": [
    "# Following code is separated by Python package used rather than Idea"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a2878ef-1b88-44be-a5b3-1c0933f021f1",
   "metadata": {},
   "source": [
    "# Gensim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4459753e-e42a-4fc9-b5b9-fd0dc1dec8ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "model = gensim.models.KeyedVectors.load_word2vec_format('/Volumes/Seagate Backup Plus Drive/JOSS Project/GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59d36df6-6c68-4235-8110-b999d05e11c1",
   "metadata": {},
   "source": [
    "# Gensim Functions\n",
    "Hidden for Clarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3f0161f-5cb0-4a91-a21b-23f84e3ae5be",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_gensim(text):\n",
    "    stop_words = stopwords.words('english')\n",
    "    text = text.lower()\n",
    "    doc = word_tokenize(text)\n",
    "    doc = [word for word in doc if word not in stop_words]\n",
    "    doc = [word for word in doc if word.isalpha()] #restricts string to alphabetic characters only\n",
    "    return doc\n",
    "\n",
    "def W2V_Gensim_Processing(texts,model, print_outside_corpus=True):\n",
    "    t = texts\n",
    "    t = t.replace('-\\\\n','')\n",
    "    t = t.replace('-\\n','')\n",
    "    t = t.replace('\\\\n',' ')\n",
    "    t = t.replace('\\n', ' ')\n",
    "    texts = t\n",
    "    #print(t)\n",
    "\n",
    "    #download('punkt') #tokenizer, run once\n",
    "    #download('stopwords') #stopwords dictionary, run once\n",
    "    stop_words = stopwords.words('english')\n",
    "\n",
    "    texts = [texts]\n",
    "    corpus = [preprocess_gensim(text) for text in texts]\n",
    "\n",
    "    sum_vector_text = np.zeros((300))\n",
    "    counter = 0\n",
    "    for i in range(len(corpus[0])):\n",
    "        if corpus[0][i] in model.vocab:\n",
    "            sum_vector_text = sum_vector_text + model.wv[corpus[0][i]]\n",
    "            counter = counter + 1\n",
    "        else:\n",
    "            if print_outside_corpus == True:\n",
    "                print(corpus[0][i])\n",
    "    average_vector_text = sum_vector_text/ counter\n",
    "    return average_vector_text\n",
    "\n",
    "\n",
    "def GetReviewerSample_W2V_Gensim(paper_vec, df_reviewers=df_reviewers):\n",
    "    warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
    "    all_usernames = []\n",
    "    all_domains = []\n",
    "    all_cosine_sims = []\n",
    "    for j in range(df_reviewers.shape[0]-1):\n",
    "        if pd.isna(df_reviewers.iloc[j+1]['Domains/topic areas you are comfortable reviewing']) == False:\n",
    "            reviewer_interests = df_reviewers.iloc[j+1]['Domains/topic areas you are comfortable reviewing'].lower()\n",
    "            reviewer_interests.replace('/',' ')\n",
    "            reviewer_corpus = [preprocess_gensim(reviewer_interests)]\n",
    "            if bool(reviewer_corpus[0]) == True:\n",
    "        #print(reviewer_corpus)\n",
    "                sum_vector_text = np.zeros((300))\n",
    "                counter = 0\n",
    "                for i in range(len(reviewer_corpus[0])):\n",
    "                    if reviewer_corpus[0][i] in model.vocab:\n",
    "                        sum_vector_text = sum_vector_text + model.wv[reviewer_corpus[0][i]]\n",
    "                        counter = counter + 1\n",
    "                    else:\n",
    "                        print(reviewer_corpus[0][i])\n",
    "                if counter > 0:\n",
    "                    average_Reviewer_vector_text = sum_vector_text/ counter\n",
    "                    all_usernames.append(df_reviewers.username.iloc[j+1])\n",
    "                    all_domains.append(reviewer_interests)\n",
    "                    all_cosine_sims.append(cosine_similarity(np.array([paper_vec]), np.array([average_Reviewer_vector_text]))[0,0])\n",
    "    return np.array(all_usernames), np.array(all_domains), np.array(all_cosine_sims)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8614627-0664-4fdb-bd70-6365faed452b",
   "metadata": {},
   "source": [
    "# Idea 3 TF (Gensim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a077ceef-c0ed-4275-8cc8-01f5c6da3f8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Get_Lemma_Words(POI_PDF):\n",
    "  text = str(POI_PDF)\n",
    "  text2 = text.split()\n",
    "  words_no_punc = []\n",
    "\n",
    "  for w in text2:\n",
    "    if w.isalpha():\n",
    "      words_no_punc.append(w.lower())\n",
    "  from nltk.corpus import stopwords\n",
    "  stopwords = stopwords.words('english')  \n",
    "  clean_words = []\n",
    "  for w in words_no_punc:\n",
    "    if w not in stopwords:\n",
    "      clean_words.append(w)\n",
    "  clean_words_arr = ''\n",
    "  for i in range(len(clean_words)):\n",
    "    clean_words_arr = clean_words_arr + ' ' + str(clean_words[i])\n",
    "\n",
    "  string_for_lemmatizing = clean_words_arr\n",
    "  lemmatizer = WordNetLemmatizer()\n",
    "  words_2 = word_tokenize(string_for_lemmatizing)\n",
    "  lemmatized_words = [lemmatizer.lemmatize(word) for word in words_2]\n",
    "\n",
    "  lemmatized_words_arr = ''\n",
    "  for i in range(len(lemmatized_words)):\n",
    "    lemmatized_words_arr = lemmatized_words_arr + ' ' + str(lemmatized_words[i])\n",
    "  words = word_tokenize(lemmatized_words_arr)\n",
    "\n",
    "  return words\n",
    "\n",
    "def Get_Top_Words_tf(Paper_interest, df_reviewers=df_reviewers,num_suggestions=5, num_top20=20):\n",
    "    POI_PDF = [extract_text(Paper_interest)]\n",
    "    text = str(POI_PDF)\n",
    "    words =  Get_Lemma_Words(POI_PDF)\n",
    "    #print(len(words))\n",
    "    fdist = FreqDist(words)\n",
    "    X = np.array(fdist.most_common())\n",
    "    top20_tf = X[:num_top20,0]\n",
    "\n",
    "    #df_reviewers.username.iloc[+1]\n",
    "\n",
    "    return top20_tf\n",
    "\n",
    "def Compare_topics(top20, df_reviewers):\n",
    "  length = df_reviewers.shape[0] - 1\n",
    "  match_arr = np.zeros(length)\n",
    "  for i in range(length):\n",
    "    if pd.isna(df_reviewers['Domains/topic areas you are comfortable reviewing'].str.lower().values[1+i]) == False:\n",
    "      t = df_reviewers['Domains/topic areas you are comfortable reviewing'].str.lower().values[1+i]\n",
    "      #print(i)\n",
    "      uniarr = Split_columns(t)\n",
    "      for j in range(len(uniarr)):\n",
    "        for k in range(len(top20)):\n",
    "          if uniarr[j] == top20[k]:\n",
    "            match_arr[i] = match_arr[i] + 1\n",
    "  return match_arr\n",
    "\n",
    "def Split_columns(t):\n",
    "  txt = \" \".join(\"\".join([\" \" if ch in string.punctuation else ch for ch in t]).split())\n",
    "  sol1 = np.char.split(txt, ' ')\n",
    "  txt_arr  = array_of_lists_to_array(sol1)\n",
    "  uniarr = np.unique(txt_arr)\n",
    "  return uniarr\n",
    "\n",
    " def array_of_lists_to_array(arr):\n",
    "    return np.apply_along_axis(lambda a: np.array(a[0]), -1, arr[..., None])\n",
    "\n",
    "def summatation_bot(all_usernames, all_domains, all_num_matched_words, all_matched_words):\n",
    "  length = len(all_usernames)\n",
    "  message = 'Hello. \\nI have found ' + str(length) + ' possible reviewers for this paper.' +'\\n\\n'\n",
    "  for i in range(length):\n",
    "    ps = 'I believe ' + all_usernames[i] + ' will make a good reviewer for this paper because they have matched ' + str(int(all_num_matched_words[i])) +  ' words from their comfortable domain topics with the top 20 most frequent words in the paper. These matched words are ' + str(all_matched_words[i]) +'.\\nFrom their topics domain: ' + str(all_domains[i].replace('\\n', ', ')) +'.\\n'\n",
    "    message = message + ps + '\\n'\n",
    "  print(message)\n",
    "\n",
    "def preprocess_gensim(text):\n",
    "    stop_words = stopwords.words('english')\n",
    "    text = text.lower()\n",
    "    doc = word_tokenize(text)\n",
    "    doc = [word for word in doc if word not in stop_words]\n",
    "    doc = [word for word in doc if word.isalpha()] #restricts string to alphabetic characters only\n",
    "    return doc\n",
    "\n",
    "def W2V_Gensim_Processing_tf(top_arr, model, print_outside_corpus=True):\n",
    "    texts = ''\n",
    "    for i in range(len(top_arr)):\n",
    "        texts = texts + top_arr[i] + ' ' \n",
    "    #print(t)\n",
    "\n",
    "    #download('punkt') #tokenizer, run once\n",
    "    #download('stopwords') #stopwords dictionary, run once\n",
    "    stop_words = stopwords.words('english')\n",
    "\n",
    "    texts = [texts]\n",
    "    corpus = [preprocess_gensim(text) for text in texts]\n",
    "\n",
    "    sum_vector_text = np.zeros((300))\n",
    "    counter = 0\n",
    "    for i in range(len(corpus[0])):\n",
    "        if corpus[0][i] in model.vocab:\n",
    "            sum_vector_text = sum_vector_text + model.wv[corpus[0][i]]\n",
    "            counter = counter + 1\n",
    "        else:\n",
    "            if print_outside_corpus == True:\n",
    "                print(corpus[0][i])\n",
    "    average_vector_text = sum_vector_text/ counter\n",
    "    return average_vector_text\n",
    "\n",
    "\n",
    "def GetReviewerSample_W2V_Gensim(paper_vec, df_reviewers=df_reviewers):\n",
    "    warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
    "    all_usernames = []\n",
    "    all_domains = []\n",
    "    all_cosine_sims = []\n",
    "    for j in range(df_reviewers.shape[0]-1):\n",
    "        if pd.isna(df_reviewers.iloc[j+1]['Domains/topic areas you are comfortable reviewing']) == False:\n",
    "            reviewer_interests = df_reviewers.iloc[j+1]['Domains/topic areas you are comfortable reviewing'].lower()\n",
    "            reviewer_interests.replace('/',' ')\n",
    "            reviewer_corpus = [preprocess_gensim(reviewer_interests)]\n",
    "            if bool(reviewer_corpus[0]) == True:\n",
    "        #print(reviewer_corpus)\n",
    "                sum_vector_text = np.zeros((300))\n",
    "                counter = 0\n",
    "                for i in range(len(reviewer_corpus[0])):\n",
    "                    if reviewer_corpus[0][i] in model.vocab:\n",
    "                        sum_vector_text = sum_vector_text + model.wv[reviewer_corpus[0][i]]\n",
    "                        counter = counter + 1\n",
    "                    else:\n",
    "                        print(reviewer_corpus[0][i])\n",
    "                if counter > 0:\n",
    "                    average_Reviewer_vector_text = sum_vector_text/ counter\n",
    "                    all_usernames.append(df_reviewers.username.iloc[j+1])\n",
    "                    all_domains.append(reviewer_interests)\n",
    "                    all_cosine_sims.append(cosine_similarity(np.array([paper_vec]), np.array([average_Reviewer_vector_text]))[0,0])\n",
    "    return np.array(all_usernames), np.array(all_domains), np.array(all_cosine_sims)\n",
    "\n",
    "def TopReviewers_W2V_Gensim(number, all_usernames, all_domains, all_cosine_sims):\n",
    "    message = 'Hello.\\n I have found ' +str(number) + ' possible reviewers for this paper.'+ '\\n\\n'\n",
    "    for J in range(number):\n",
    "        index = np.argsort(all_cosine_sims)[-1-J]\n",
    "        #print(index)\n",
    "        ps = 'I believe '+ colored(str(all_usernames[index]), 'green') + ' will be a good reviewer for this paper. Their domain interests and this paper have a cosine similairity score of ' + colored(str(all_cosine_sims[index])[:6], 'blue') + '. This reviewers domain interests are ' + colored(str(all_domains[index].replace('\\n', ',')), 'red')\n",
    "        message = message + ps + '\\n\\n'\n",
    "    print(message)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c86d1f06-1cc2-4202-9df1-d6f5f10eb82d",
   "metadata": {},
   "outputs": [],
   "source": [
    "top20_tf = Get_Top_Words_tf(PAPER_OF_INTEREST_FNAME[Q])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "876748a5-28fc-4118-a4c6-fd70f8f3766f",
   "metadata": {},
   "outputs": [],
   "source": [
    "top20_tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9289e3f7-b0c6-4bba-8aad-31c1c9b26f1d",
   "metadata": {},
   "outputs": [],
   "source": [
    "average_vector_text = W2V_Gensim_Processing_tf(top20_tf, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "476e9e80-9f0d-49d5-8d41-d5bec58be8bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_usernames_W2V_Gensim, all_domains_W2V_Gensim, all_cosine_sims_W2V_Gensim = GetReviewerSample_W2V_Gensim(average_vector_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27d7f4a0-4603-4be4-8d74-0c66778036c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "TopReviewers_W2V_Gensim(5,all_usernames_W2V_Gensim, all_domains_W2V_Gensim, all_cosine_sims_W2V_Gensim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ddc2128-03bf-4c3c-8163-11af6e4c9e1e",
   "metadata": {},
   "source": [
    "# Idea 3 TF-IDF (Gensim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "778643d7-8400-42f0-82c2-dc722d88387e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Get_Top_Words_tf_idf(Paper_interest, df=df, df_reviewers=df_reviewers, num_suggestions=5, num_top20=20):\n",
    "    POI_PDF = [extract_text(Paper_interest)]\n",
    "    text = str(POI_PDF)\n",
    "    words = Get_Lemma_Words(POI_PDF)\n",
    "    #print(len(words))\n",
    "    fdist = FreqDist(words)\n",
    "    X = np.array(fdist.most_common())\n",
    "    tf_idf_arr_names, tf_idf_arr_floats = determine_wiki_td_idf(X, df=df)\n",
    "    #print('determined wiki')\n",
    "    num_arr = np.array(tf_idf_arr_floats)\n",
    "    tf_idf_arr_names_arr = np.array(tf_idf_arr_names)\n",
    "    top20_tf_idf = tf_idf_arr_names_arr[np.argsort(num_arr)[:num_top20]]\n",
    "    return top20_tf_idf\n",
    "\n",
    "def determine_wiki_td_idf(x, df=df):\n",
    "    tf_idf_arr_names = []\n",
    "    tf_idf_arr_floats = []\n",
    "    for i in range(len(x)):\n",
    "        if df[df['token'] ==x[i][0]].frequency.empty == False:\n",
    "            wiki_tf = df[df['token'] ==x[i][0]].frequency.values[0]\n",
    "            doc_tf = int(x[i][1])\n",
    "            tf_idf = np.log(wiki_tf/doc_tf)\n",
    "            tf_idf_arr_names.append(x[i][0])\n",
    "            tf_idf_arr_floats.append(tf_idf)\n",
    "    return tf_idf_arr_names, tf_idf_arr_floats\n",
    "\n",
    "def summatation_bot(all_usernames, all_domains, all_num_matched_words, all_matched_words):\n",
    "  length = len(all_usernames)\n",
    "  message = 'Hello. \\nI have found ' + str(length) + ' possible reviewers for this paper.' +'\\n\\n'\n",
    "  for i in range(length):\n",
    "    ps = 'I believe ' + all_usernames[i] + ' will make a good reviewer for this paper because they have matched ' + str(int(all_num_matched_words[i])) +  ' words from their comfortable domain topics with the top 20 most frequent words in the paper. These matched words are ' + str(all_matched_words[i]) +'.\\nFrom their topics domain: ' + str(all_domains[i].replace('\\n', ', ')) +'.\\n'\n",
    "    message = message + ps + '\\n'\n",
    "  print(message)\n",
    "\n",
    "def preprocess_gensim(text):\n",
    "    stop_words = stopwords.words('english')\n",
    "    text = text.lower()\n",
    "    doc = word_tokenize(text)\n",
    "    doc = [word for word in doc if word not in stop_words]\n",
    "    doc = [word for word in doc if word.isalpha()] #restricts string to alphabetic characters only\n",
    "    return doc\n",
    "\n",
    "def W2V_Gensim_Processing_tf(top_arr, model, print_outside_corpus=True):\n",
    "    texts = ''\n",
    "    for i in range(len(top_arr)):\n",
    "        texts = texts + top_arr[i] + ' ' \n",
    "    #print(t)\n",
    "\n",
    "    #download('punkt') #tokenizer, run once\n",
    "    #download('stopwords') #stopwords dictionary, run once\n",
    "    stop_words = stopwords.words('english')\n",
    "\n",
    "    texts = [texts]\n",
    "    corpus = [preprocess_gensim(text) for text in texts]\n",
    "\n",
    "    sum_vector_text = np.zeros((300))\n",
    "    counter = 0\n",
    "    for i in range(len(corpus[0])):\n",
    "        if corpus[0][i] in model.vocab:\n",
    "            sum_vector_text = sum_vector_text + model.wv[corpus[0][i]]\n",
    "            counter = counter + 1\n",
    "        else:\n",
    "            if print_outside_corpus == True:\n",
    "                print(corpus[0][i])\n",
    "    average_vector_text = sum_vector_text/ counter\n",
    "    return average_vector_text\n",
    "\n",
    "\n",
    "def GetReviewerSample_W2V_Gensim(paper_vec, df_reviewers=df_reviewers):\n",
    "    warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
    "    all_usernames = []\n",
    "    all_domains = []\n",
    "    all_cosine_sims = []\n",
    "    for j in range(df_reviewers.shape[0]-1):\n",
    "        if pd.isna(df_reviewers.iloc[j+1]['Domains/topic areas you are comfortable reviewing']) == False:\n",
    "            reviewer_interests = df_reviewers.iloc[j+1]['Domains/topic areas you are comfortable reviewing'].lower()\n",
    "            reviewer_interests.replace('/',' ')\n",
    "            reviewer_corpus = [preprocess_gensim(reviewer_interests)]\n",
    "            if bool(reviewer_corpus[0]) == True:\n",
    "        #print(reviewer_corpus)\n",
    "                sum_vector_text = np.zeros((300))\n",
    "                counter = 0\n",
    "                for i in range(len(reviewer_corpus[0])):\n",
    "                    if reviewer_corpus[0][i] in model.vocab:\n",
    "                        sum_vector_text = sum_vector_text + model.wv[reviewer_corpus[0][i]]\n",
    "                        counter = counter + 1\n",
    "                    else:\n",
    "                        print(reviewer_corpus[0][i])\n",
    "                if counter > 0:\n",
    "                    average_Reviewer_vector_text = sum_vector_text/ counter\n",
    "                    all_usernames.append(df_reviewers.username.iloc[j+1])\n",
    "                    all_domains.append(reviewer_interests)\n",
    "                    all_cosine_sims.append(cosine_similarity(np.array([paper_vec]), np.array([average_Reviewer_vector_text]))[0,0])\n",
    "    return np.array(all_usernames), np.array(all_domains), np.array(all_cosine_sims)\n",
    "\n",
    "def TopReviewers_W2V_Gensim(number, all_usernames, all_domains, all_cosine_sims):\n",
    "    message = 'Hello.\\n I have found ' +str(number) + ' possible reviewers for this paper.'+ '\\n\\n'\n",
    "    for J in range(number):\n",
    "        index = np.argsort(all_cosine_sims)[-1-J]\n",
    "        #print(index)\n",
    "        ps = 'I believe '+ colored(str(all_usernames[index]), 'green') + ' will be a good reviewer for this paper. Their domain interests and this paper have a cosine similairity score of ' + colored(str(all_cosine_sims[index])[:6], 'blue') + '. This reviewers domain interests are ' + colored(str(all_domains[index].replace('\\n', ',')), 'red')\n",
    "        message = message + ps + '\\n\\n'\n",
    "    print(message)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a1db242-574c-473f-b797-f0542e772e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "top20_tf_idf = Get_Top_Words_tf_idf(PAPER_OF_INTEREST_FNAME[Q])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a2369c0-f523-4ca7-a07e-2df877d63ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "top20_tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66d1b1ae-e7f0-48f2-9b6b-377d62fbea70",
   "metadata": {},
   "outputs": [],
   "source": [
    "average_vector_text = W2V_Gensim_Processing_tf(top20_tf_idf, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cf9f571-8d94-40ac-86de-63d8809ad7f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_usernames_W2V_Gensim, all_domains_W2V_Gensim, all_cosine_sims_W2V_Gensim = GetReviewerSample_W2V_Gensim(average_vector_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a2fd4e-73d4-4dd8-a3a3-fd2f9b3061ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "TopReviewers_W2V_Gensim(5,all_usernames_W2V_Gensim, all_domains_W2V_Gensim, all_cosine_sims_W2V_Gensim)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b311f16a-03ad-418c-af32-35686db39605",
   "metadata": {},
   "source": [
    "# Idea 4 (Gensim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27215f25-84f6-4e9d-ba6d-e89e1d43978a",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts, arr = MakeGreenRedText(Paper_interest,False)\n",
    "average_vector_text = W2V_Gensim_Processing(texts, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7b4b0a4-9fa7-420e-a02e-a880280101ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_usernames_W2V_Gensim, all_domains_W2V_Gensim, all_cosine_sims_W2V_Gensim = GetReviewerSample_W2V_Gensim(average_vector_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f162389-f3eb-429a-90e5-8bcf996e6bd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def TopReviewers_W2V_Gensim(number, all_usernames, all_domains, all_cosine_sims):\n",
    "    message = 'Hello.\\n I have found ' +str(number) + ' possible reviewers for this paper.'+ '\\n\\n'\n",
    "    for J in range(number):\n",
    "        index = np.argsort(all_cosine_sims)[-1-J]\n",
    "        #print(index)\n",
    "        ps = 'I believe '+ colored(str(all_usernames[index]), 'green') + ' will be a good reviewer for this paper. Their domain interests and this paper have a cosine similairity score of ' + colored(str(all_cosine_sims[index])[:6], 'blue') + '. This reviewers domain interests are ' + colored(str(all_domains[index].replace('\\n', ',')), 'red')\n",
    "        message = message + ps + '\\n\\n'\n",
    "    print(message)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "632978d0-ccdb-42b8-8ad9-60764bee3e14",
   "metadata": {},
   "outputs": [],
   "source": [
    "TopReviewers_W2V_Gensim(5,all_usernames_W2V_Gensim, all_domains_W2V_Gensim, all_cosine_sims_W2V_Gensim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a749e926-457d-4fad-8630-f2292e63ee35",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cd5d7fd-4f00-4b98-8189-1426b6d92a11",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e819988d-c78f-46b3-b636-189d576890aa",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "306d28be-91cc-4cdf-b3bf-e8cf74764bc1",
   "metadata": {},
   "source": [
    "# Idea 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c521a30-dbd4-4f26-a21f-b7c9cbc01cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "def MakingPaperVector_Idea6_Gensim(arr, model, printer=True):\n",
    "    sum_vector_text = np.zeros((300))\n",
    "    count = 0\n",
    "    for J in range(len(arr)):\n",
    "        texts = arr[J]\n",
    "        texts = texts\n",
    "        texts = texts.replace('-\\\\n','')\n",
    "        texts = texts.replace('-\\n','')\n",
    "        texts = texts.replace('\\\\n',' ')\n",
    "        texts = texts.replace('\\n', ' ')\n",
    "    \n",
    "        texts = [texts]\n",
    "        corpus = [preprocess_gensim(text) for text in texts]\n",
    "    \n",
    "        sum_line_vector_text = np.zeros((300))\n",
    "        counter = 0\n",
    "        for i in range(len(corpus[0])):\n",
    "            if corpus[0][i] in model.vocab:\n",
    "                sum_vector_text = sum_vector_text + model.wv[corpus[0][i]]\n",
    "                counter = counter + 1\n",
    "            else:\n",
    "                if printer == True:\n",
    "                    print(corpus[0][i])\n",
    "        average_line_vector_text = sum_line_vector_text/ counter\n",
    "        sum_vector_text = sum_vector_text + average_line_vector_text\n",
    "        count = count + 1\n",
    "    average_vector_text = sum_vector_text / count\n",
    "    \n",
    "    return average_vector_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a24bb826-c4cb-4b3e-b98e-e099ae0fbb53",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts, arr = MakeGreenRedText(Paper_interest,False)\n",
    "average_vector_text = MakingPaperVector_Idea6_Gensim(arr, model, printer=True)\n",
    "\n",
    "all_usernames, all_domains, all_cosine_sims = GetReviewerSample_W2V(average_vector_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6c4a79e-da47-4ca0-9ca3-256008eed688",
   "metadata": {},
   "outputs": [],
   "source": [
    "TopReviewers_W2V_Gensim(5,all_usernames_W2V_Gensim, all_domains_W2V_Gensim, all_cosine_sims_W2V_Gensim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d881dff5-9d4f-4f97-960b-1b04e2b19ae5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2500f122-41d9-4ec6-8156-a279e8454d5c",
   "metadata": {},
   "source": [
    "# spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d1b96c2-31c0-4e30-8e96-2e4b2dba008f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3d938ae-2939-47c8-8def-0840e5da772a",
   "metadata": {},
   "source": [
    "# spaCy Functions\n",
    "Hidden for Clarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a8a598f-dbef-407a-94ec-802d7273c500",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetReviewer_Vectors_spacy(model,df_reviewers=df_reviewers):\n",
    "    reviewer_vectors = np.zeros(((df_reviewers.shape[0]-1),300))\n",
    "    for i in range(df_reviewers.shape[0]-1):\n",
    "        #if i%10 == 0:\n",
    "        #    print(i)\n",
    "        if pd.isna(df_reviewers['Domains/topic areas you are comfortable reviewing'].iloc[1:].values[i]) == False:\n",
    "            review_text = df_reviewers['Domains/topic areas you are comfortable reviewing'].iloc[1:].values[i].lower()\n",
    "            review_text = review_text.replace('-\\\\n','')\n",
    "            review_text = review_text.replace('\\\\n',' ')\n",
    "            review_text = review_text.replace('\\n', ' ')\n",
    "        \n",
    "            review_arr = []\n",
    "            for token in model(review_text):\n",
    "                if token.is_alpha == True:\n",
    "                    if token.is_stop == False:\n",
    "                        review_arr.append(str(token.lemma_).lower())\n",
    "            review_arr = np.array(review_arr)\n",
    "        \n",
    "            review_str = ''\n",
    "            for j in np.unique(review_arr):\n",
    "                review_str = review_str + j +' '\n",
    "        \n",
    "        #print(model(review_str).vector.shape)\n",
    "        #print(reviewer_vectors.shape)\n",
    "            reviewer_vectors[i] = model(review_str).vector\n",
    "    \n",
    "    return reviewer_vectors\n",
    "\n",
    "def GetCosineSims_spacy(doc_vec, review_vec, df_reviewers=df_reviewers):\n",
    "    all_usernames = []\n",
    "    all_domains = []\n",
    "    all_cosine_sims = []\n",
    "    for j in range(len(review_vec)):\n",
    "        if pd.isna(df_reviewers.iloc[j+1]['Domains/topic areas you are comfortable reviewing']) == False:\n",
    "            all_cosine_sims.append(cosine_similarity(np.array([doc_vec]), np.array([review_vec[j]]))[0,0])\n",
    "            all_domains.append(df_reviewers.iloc[j+1]['Domains/topic areas you are comfortable reviewing'].lower())\n",
    "            all_usernames.append(df_reviewers.iloc[j+1].username)\n",
    "    all_usernames= np.array(all_usernames)\n",
    "    all_domains= np.array(all_domains)\n",
    "    all_cosine_sims= np.array(all_cosine_sims)\n",
    "    \n",
    "    return all_usernames, all_domains, all_cosine_sims\n",
    "\n",
    "#def TopReviewers_spacy(number=5, all_usernames=all_usernames, all_domains=all_domains, all_cosine_sims=all_cosine_sims):\n",
    "def TopReviewers_spacy(number, all_usernames, all_domains, all_cosine_sims):\n",
    "    message = 'Hello.\\n I have found ' +str(number) + ' possible reviewers for this paper.'+ '\\n\\n'\n",
    "    for J in range(number):\n",
    "        index = np.argsort(all_cosine_sims)[-1-J]\n",
    "        #print(index)\n",
    "        ps = 'I believe '+ colored(str(all_usernames[index]), 'green') + ' will be a good reviewer for this paper. Their domain interests and this paper have a cosine similairity score of ' + colored(str(all_cosine_sims[index])[:6], 'blue') + '. This reviewers domain interests are ' + colored(str(all_domains[index].replace('\\n', ',')), 'red')\n",
    "        message = message + ps + '\\n\\n'\n",
    "    print(message)        \n",
    "    \n",
    "    \n",
    "def W2V_spaCy_Processing(texts, green_text=True, lemma=True, unique=True):\n",
    "    texts = texts.replace('-\\\\n','')\n",
    "    texts = texts.replace('-\\n','')\n",
    "    texts = texts.replace('\\\\n',' ')\n",
    "    texts = texts.replace('\\n', ' ')\n",
    "    \n",
    "    model = spacy.load('en_core_web_lg')\n",
    "    reviewer_vectors = GetReviewer_Vectors_spacy(model)\n",
    "    \n",
    "    # Only Green Text: \n",
    "    if green_text == True:\n",
    "        print('### Only Green Text - No Other Preprocessing ###')\n",
    "        all_usernames, all_domains, all_cosine_sims = GetCosineSims_spacy(model(texts).vector, reviewer_vectors)\n",
    "        TopReviewers_spacy(5, all_usernames, all_domains, all_cosine_sims)\n",
    "        \n",
    "    doc = model(texts)\n",
    "    doc_arr = []\n",
    "    for token in doc:\n",
    "        if token.is_alpha == True:\n",
    "            if token.is_stop == False:\n",
    "                doc_arr.append(str(token.lemma_).lower())\n",
    "    doc_arr = np.array(doc_arr)\n",
    "    \n",
    "    doc_arr_mod = ''\n",
    "    for i in doc_arr:\n",
    "        doc_arr_mod = doc_arr_mod + i +' '\n",
    "    \n",
    "    # Green Text and Lemmasation\n",
    "    if lemma == True:\n",
    "        print('### Green Text & Lemmasation ###')\n",
    "        all_usernames2, all_domains2, all_cosine_sims2 = GetCosineSims_spacy(model(doc_arr_mod).vector, reviewer_vectors)\n",
    "        TopReviewers_spacy(5, all_usernames2, all_domains2, all_cosine_sims2)\n",
    "        \n",
    "    if unique == True:\n",
    "        print('### Green Text & Lemmasation & Unique Words ###')\n",
    "        doc_str = ''\n",
    "        for i in np.unique(doc_arr):\n",
    "            doc_str = doc_str + i +' '\n",
    "        all_usernames3, all_domains3, all_cosine_sims3 = GetCosineSims_spacy(model(doc_str).vector, reviewer_vectors)\n",
    "        TopReviewers_spacy(5,all_usernames3, all_domains3, all_cosine_sims3)\n",
    "        \n",
    "def GetReviewer_Vectors(df_reviewers=df_reviewers):\n",
    "    reviewer_vectors = np.zeros(((df_reviewers.shape[0]-1),300))\n",
    "    for i in range(df_reviewers.shape[0]-1):\n",
    "        #if i%10 == 0:\n",
    "        #    print(i)\n",
    "        if pd.isna(df_reviewers['Domains/topic areas you are comfortable reviewing'].iloc[1:].values[i]) == False:\n",
    "            review_text = df_reviewers['Domains/topic areas you are comfortable reviewing'].iloc[1:].values[i].lower()\n",
    "            review_text = review_text.replace('-\\\\n','')\n",
    "            review_text = review_text.replace('\\\\n',' ')\n",
    "            review_text = review_text.replace('\\n', ' ')\n",
    "        \n",
    "            review_arr = []\n",
    "            for token in model(review_text):\n",
    "                if token.is_alpha == True:\n",
    "                    if token.is_stop == False:\n",
    "                        review_arr.append(str(token.lemma_).lower())\n",
    "            review_arr = np.array(review_arr)\n",
    "        \n",
    "            review_str = ''\n",
    "            for j in np.unique(review_arr):\n",
    "                review_str = review_str + j +' '\n",
    "        \n",
    "        #print(model(review_str).vector.shape)\n",
    "        #print(reviewer_vectors.shape)\n",
    "            reviewer_vectors[i] = model(review_str).vector\n",
    "    \n",
    "    return reviewer_vectors\n",
    "\n",
    "def GetCosineSims(doc_vec, review_vec, df_reviewers=df_reviewers):\n",
    "    all_usernames = []\n",
    "    all_domains = []\n",
    "    all_cosine_sims = []\n",
    "    for j in range(len(review_vec)):\n",
    "        if pd.isna(df_reviewers.iloc[j+1]['Domains/topic areas you are comfortable reviewing']) == False:\n",
    "            all_cosine_sims.append(cosine_similarity(np.array([doc_vec]), np.array([review_vec[j]]))[0,0])\n",
    "            all_domains.append(df_reviewers.iloc[j+1]['Domains/topic areas you are comfortable reviewing'].lower())\n",
    "            all_usernames.append(df_reviewers.iloc[j+1].username)\n",
    "    all_usernames= np.array(all_usernames)\n",
    "    all_domains= np.array(all_domains)\n",
    "    all_cosine_sims= np.array(all_cosine_sims)\n",
    "    \n",
    "    return all_usernames, all_domains, all_cosine_sims\n",
    "\n",
    "def Get_Paper_Vector_Idea6_SpaCy(arr,model):\n",
    "    sum_vector_text = np.zeros((300))\n",
    "    count = 0\n",
    "    for J in range(len(arr)):\n",
    "        sum_line_vector_text = model(str(arr[J])).vector\n",
    "        counter = len(arr[J].split())\n",
    "        \n",
    "        average_line_vector_text = sum_line_vector_text/ counter\n",
    "        sum_vector_text = sum_vector_text + average_line_vector_text\n",
    "        count = count + 1\n",
    "        \n",
    "    average_vector_text = sum_vector_text / count\n",
    "    return average_vector_text\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "972af138-ac82-4b90-a794-5c17869c656a",
   "metadata": {},
   "source": [
    "# Idea 3 TF (spaCy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c8a0445-01ce-45c1-9b3c-08a385acb30d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Get_Lemma_Words(POI_PDF):\n",
    "  text = str(POI_PDF)\n",
    "  text2 = text.split()\n",
    "  words_no_punc = []\n",
    "\n",
    "  for w in text2:\n",
    "    if w.isalpha():\n",
    "      words_no_punc.append(w.lower())\n",
    "  from nltk.corpus import stopwords\n",
    "  stopwords = stopwords.words('english')  \n",
    "  clean_words = []\n",
    "  for w in words_no_punc:\n",
    "    if w not in stopwords:\n",
    "      clean_words.append(w)\n",
    "  clean_words_arr = ''\n",
    "  for i in range(len(clean_words)):\n",
    "    clean_words_arr = clean_words_arr + ' ' + str(clean_words[i])\n",
    "\n",
    "  string_for_lemmatizing = clean_words_arr\n",
    "  lemmatizer = WordNetLemmatizer()\n",
    "  words_2 = word_tokenize(string_for_lemmatizing)\n",
    "  lemmatized_words = [lemmatizer.lemmatize(word) for word in words_2]\n",
    "\n",
    "  lemmatized_words_arr = ''\n",
    "  for i in range(len(lemmatized_words)):\n",
    "    lemmatized_words_arr = lemmatized_words_arr + ' ' + str(lemmatized_words[i])\n",
    "  words = word_tokenize(lemmatized_words_arr)\n",
    "\n",
    "  return words\n",
    "\n",
    "def Get_Top_Words_tf(Paper_interest, df_reviewers=df_reviewers,num_suggestions=5, num_top20=20):\n",
    "    POI_PDF = [extract_text(Paper_interest)]\n",
    "    text = str(POI_PDF)\n",
    "    words =  Get_Lemma_Words(POI_PDF)\n",
    "    #print(len(words))\n",
    "    fdist = FreqDist(words)\n",
    "    X = np.array(fdist.most_common())\n",
    "    top20_tf = X[:num_top20,0]\n",
    "\n",
    "    #df_reviewers.username.iloc[+1]\n",
    "\n",
    "    return top20_tf\n",
    "\n",
    "def Compare_topics(top20, df_reviewers):\n",
    "  length = df_reviewers.shape[0] - 1\n",
    "  match_arr = np.zeros(length)\n",
    "  for i in range(length):\n",
    "    if pd.isna(df_reviewers['Domains/topic areas you are comfortable reviewing'].str.lower().values[1+i]) == False:\n",
    "      t = df_reviewers['Domains/topic areas you are comfortable reviewing'].str.lower().values[1+i]\n",
    "      #print(i)\n",
    "      uniarr = Split_columns(t)\n",
    "      for j in range(len(uniarr)):\n",
    "        for k in range(len(top20)):\n",
    "          if uniarr[j] == top20[k]:\n",
    "            match_arr[i] = match_arr[i] + 1\n",
    "  return match_arr\n",
    "\n",
    "def Split_columns(t):\n",
    "  txt = \" \".join(\"\".join([\" \" if ch in string.punctuation else ch for ch in t]).split())\n",
    "  sol1 = np.char.split(txt, ' ')\n",
    "  txt_arr  = array_of_lists_to_array(sol1)\n",
    "  uniarr = np.unique(txt_arr)\n",
    "  return uniarr\n",
    "\n",
    " def array_of_lists_to_array(arr):\n",
    "    return np.apply_along_axis(lambda a: np.array(a[0]), -1, arr[..., None])\n",
    "\n",
    "def summatation_bot(all_usernames, all_domains, all_num_matched_words, all_matched_words):\n",
    "  length = len(all_usernames)\n",
    "  message = 'Hello. \\nI have found ' + str(length) + ' possible reviewers for this paper.' +'\\n\\n'\n",
    "  for i in range(length):\n",
    "    ps = 'I believe ' + all_usernames[i] + ' will make a good reviewer for this paper because they have matched ' + str(int(all_num_matched_words[i])) +  ' words from their comfortable domain topics with the top 20 most frequent words in the paper. These matched words are ' + str(all_matched_words[i]) +'.\\nFrom their topics domain: ' + str(all_domains[i].replace('\\n', ', ')) +'.\\n'\n",
    "    message = message + ps + '\\n'\n",
    "  print(message)\n",
    "\n",
    "\n",
    "def GetReviewer_Vectors(df_reviewers=df_reviewers):\n",
    "    reviewer_vectors = np.zeros(((df_reviewers.shape[0]-1),300))\n",
    "    for i in range(df_reviewers.shape[0]-1):\n",
    "        #if i%10 == 0:\n",
    "        #    print(i)\n",
    "        if pd.isna(df_reviewers['Domains/topic areas you are comfortable reviewing'].iloc[1:].values[i]) == False:\n",
    "            review_text = df_reviewers['Domains/topic areas you are comfortable reviewing'].iloc[1:].values[i].lower()\n",
    "            review_text = review_text.replace('-\\\\n','')\n",
    "            review_text = review_text.replace('\\\\n',' ')\n",
    "            review_text = review_text.replace('\\n', ' ')\n",
    "        \n",
    "            review_arr = []\n",
    "            for token in model(review_text):\n",
    "                if token.is_alpha == True:\n",
    "                    if token.is_stop == False:\n",
    "                        review_arr.append(str(token.lemma_).lower())\n",
    "            review_arr = np.array(review_arr)\n",
    "        \n",
    "            review_str = ''\n",
    "            for j in np.unique(review_arr):\n",
    "                review_str = review_str + j +' '\n",
    "        \n",
    "        #print(model(review_str).vector.shape)\n",
    "        #print(reviewer_vectors.shape)\n",
    "            reviewer_vectors[i] = model(review_str).vector\n",
    "    \n",
    "    return reviewer_vectors\n",
    "\n",
    "def GetCosineSims(doc_vec, review_vec, df_reviewers=df_reviewers):\n",
    "    all_usernames = []\n",
    "    all_domains = []\n",
    "    all_cosine_sims = []\n",
    "    for j in range(len(review_vec)):\n",
    "        if pd.isna(df_reviewers.iloc[j+1]['Domains/topic areas you are comfortable reviewing']) == False:\n",
    "            all_cosine_sims.append(cosine_similarity(np.array([doc_vec]), np.array([review_vec[j]]))[0,0])\n",
    "            all_domains.append(df_reviewers.iloc[j+1]['Domains/topic areas you are comfortable reviewing'].lower())\n",
    "            all_usernames.append(df_reviewers.iloc[j+1].username)\n",
    "    all_usernames= np.array(all_usernames)\n",
    "    all_domains= np.array(all_domains)\n",
    "    all_cosine_sims= np.array(all_cosine_sims)\n",
    "    \n",
    "    return all_usernames, all_domains, all_cosine_sims\n",
    "\n",
    "\n",
    "def TopReviewers(number=5, all_usernames=all_usernames2, all_domains=all_domains2, all_cosine_sims=all_cosine_sims2):\n",
    "    message = 'Hello.\\n I have found ' +str(number) + ' possible reviewers for this paper.'+ '\\n\\n'\n",
    "    for J in range(number):\n",
    "        index = np.argsort(all_cosine_sims)[-1-J]\n",
    "        #print(index)\n",
    "        ps = 'I believe '+ colored(str(all_usernames[index]), 'green') + ' will be a good reviewer for this paper. Their domain interests and this paper have a cosine similairity score of ' + colored(str(all_cosine_sims[index])[:6], 'blue') + '. This reviewers domain interests are ' + colored(str(all_domains[index].replace('\\n', ',')), 'red')\n",
    "        message = message + ps + '\\n\\n'\n",
    "    print(message)    \n",
    "    \n",
    "def TopReviewers(number=5, all_usernames=all_usernames2, all_domains=all_domains2, all_cosine_sims=all_cosine_sims2):\n",
    "    message = 'Hello.\\n I have found ' +str(number) + ' possible reviewers for this paper.'+ '\\n\\n'\n",
    "    for J in range(number):\n",
    "        index = np.argsort(all_cosine_sims)[-1-J]\n",
    "        #print(index)\n",
    "        ps = 'I believe '+ colored(str(all_usernames[index]), 'green') + ' will be a good reviewer for this paper. Their domain interests and this paper have a cosine similairity score of ' + colored(str(all_cosine_sims[index])[:6], 'blue') + '. This reviewers domain interests are ' + colored(str(all_domains[index].replace('\\n', ',')), 'red')\n",
    "        message = message + ps + '\\n\\n'\n",
    "    print(message)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07cfdf15-3f8c-47f6-b641-8a47dbbd930c",
   "metadata": {},
   "outputs": [],
   "source": [
    "top20_tf = Get_Top_Words_tf(PAPER_OF_INTEREST_FNAME[Q])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e0caba-c2f5-48a2-ad98-1b24f99360b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "top20_tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2659297c-c2d0-4ca7-9f4f-1ea4ba3da83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviewer_vectors = GetReviewer_Vectors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc3bd34f-618c-456d-8f6b-eddbf2da41c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_top20= ''\n",
    "for i in top20_tf:\n",
    "    doc_top20 = doc_top20 + i +' '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5cf9b26c-109d-4684-abbe-88bf967b0638",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_usernames2, all_domains2, all_cosine_sims2 = GetCosineSims(model(doc_top20).vector, reviewer_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "608ef328-2f6c-4f07-aa8b-a16ae36c1e32",
   "metadata": {},
   "outputs": [],
   "source": [
    "TopReviewers()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d17a634f-7c14-4812-8ad1-af5659043fc2",
   "metadata": {},
   "source": [
    "# Idea 3 TF-IDF (spaCy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f498dda4-8749-4eb2-92de-55f324617162",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Get_Lemma_Words(POI_PDF):\n",
    "  text = str(POI_PDF)\n",
    "  text2 = text.split()\n",
    "  words_no_punc = []\n",
    "\n",
    "  for w in text2:\n",
    "    if w.isalpha():\n",
    "      words_no_punc.append(w.lower())\n",
    "  from nltk.corpus import stopwords\n",
    "  stopwords = stopwords.words('english')  \n",
    "  clean_words = []\n",
    "  for w in words_no_punc:\n",
    "    if w not in stopwords:\n",
    "      clean_words.append(w)\n",
    "  clean_words_arr = ''\n",
    "  for i in range(len(clean_words)):\n",
    "    clean_words_arr = clean_words_arr + ' ' + str(clean_words[i])\n",
    "\n",
    "  string_for_lemmatizing = clean_words_arr\n",
    "  lemmatizer = WordNetLemmatizer()\n",
    "  words_2 = word_tokenize(string_for_lemmatizing)\n",
    "  lemmatized_words = [lemmatizer.lemmatize(word) for word in words_2]\n",
    "\n",
    "  lemmatized_words_arr = ''\n",
    "  for i in range(len(lemmatized_words)):\n",
    "    lemmatized_words_arr = lemmatized_words_arr + ' ' + str(lemmatized_words[i])\n",
    "  words = word_tokenize(lemmatized_words_arr)\n",
    "\n",
    "  return words\n",
    "\n",
    "def Get_Top_Words_tf_idf(Paper_interest, df=df, df_reviewers=df_reviewers, num_suggestions=5, num_top20=20):\n",
    "    POI_PDF = [extract_text(Paper_interest)]\n",
    "    text = str(POI_PDF)\n",
    "    words = Get_Lemma_Words(POI_PDF)\n",
    "    #print(len(words))\n",
    "    fdist = FreqDist(words)\n",
    "    X = np.array(fdist.most_common())\n",
    "    tf_idf_arr_names, tf_idf_arr_floats = determine_wiki_td_idf(X, df=df)\n",
    "    #print('determined wiki')\n",
    "    num_arr = np.array(tf_idf_arr_floats)\n",
    "    tf_idf_arr_names_arr = np.array(tf_idf_arr_names)\n",
    "    top20_tf_idf = tf_idf_arr_names_arr[np.argsort(num_arr)[:num_top20]]\n",
    "    return top20_tf_idf\n",
    "\n",
    "def Compare_topics(top20, df_reviewers):\n",
    "  length = df_reviewers.shape[0] - 1\n",
    "  match_arr = np.zeros(length)\n",
    "  for i in range(length):\n",
    "    if pd.isna(df_reviewers['Domains/topic areas you are comfortable reviewing'].str.lower().values[1+i]) == False:\n",
    "      t = df_reviewers['Domains/topic areas you are comfortable reviewing'].str.lower().values[1+i]\n",
    "      #print(i)\n",
    "      uniarr = Split_columns(t)\n",
    "      for j in range(len(uniarr)):\n",
    "        for k in range(len(top20)):\n",
    "          if uniarr[j] == top20[k]:\n",
    "            match_arr[i] = match_arr[i] + 1\n",
    "  return match_arr\n",
    "\n",
    "def Split_columns(t):\n",
    "  txt = \" \".join(\"\".join([\" \" if ch in string.punctuation else ch for ch in t]).split())\n",
    "  sol1 = np.char.split(txt, ' ')\n",
    "  txt_arr  = array_of_lists_to_array(sol1)\n",
    "  uniarr = np.unique(txt_arr)\n",
    "  return uniarr\n",
    "\n",
    " def array_of_lists_to_array(arr):\n",
    "    return np.apply_along_axis(lambda a: np.array(a[0]), -1, arr[..., None])\n",
    "\n",
    "def determine_wiki_td_idf(x, df=df):\n",
    "    tf_idf_arr_names = []\n",
    "    tf_idf_arr_floats = []\n",
    "    for i in range(len(x)):\n",
    "        if df[df['token'] ==x[i][0]].frequency.empty == False:\n",
    "            wiki_tf = df[df['token'] ==x[i][0]].frequency.values[0]\n",
    "            doc_tf = int(x[i][1])\n",
    "            tf_idf = np.log(wiki_tf/doc_tf)\n",
    "            tf_idf_arr_names.append(x[i][0])\n",
    "            tf_idf_arr_floats.append(tf_idf)\n",
    "    return tf_idf_arr_names, tf_idf_arr_floats\n",
    "\n",
    "def summatation_bot(all_usernames, all_domains, all_num_matched_words, all_matched_words):\n",
    "  length = len(all_usernames)\n",
    "  message = 'Hello. \\nI have found ' + str(length) + ' possible reviewers for this paper.' +'\\n\\n'\n",
    "  for i in range(length):\n",
    "    ps = 'I believe ' + all_usernames[i] + ' will make a good reviewer for this paper because they have matched ' + str(int(all_num_matched_words[i])) +  ' words from their comfortable domain topics with the top 20 most frequent words in the paper. These matched words are ' + str(all_matched_words[i]) +'.\\nFrom their topics domain: ' + str(all_domains[i].replace('\\n', ', ')) +'.\\n'\n",
    "    message = message + ps + '\\n'\n",
    "  print(message)\n",
    "\n",
    "def GetReviewer_Vectors(df_reviewers=df_reviewers):\n",
    "    reviewer_vectors = np.zeros(((df_reviewers.shape[0]-1),300))\n",
    "    for i in range(df_reviewers.shape[0]-1):\n",
    "        #if i%10 == 0:\n",
    "        #    print(i)\n",
    "        if pd.isna(df_reviewers['Domains/topic areas you are comfortable reviewing'].iloc[1:].values[i]) == False:\n",
    "            review_text = df_reviewers['Domains/topic areas you are comfortable reviewing'].iloc[1:].values[i].lower()\n",
    "            review_text = review_text.replace('-\\\\n','')\n",
    "            review_text = review_text.replace('\\\\n',' ')\n",
    "            review_text = review_text.replace('\\n', ' ')\n",
    "        \n",
    "            review_arr = []\n",
    "            for token in model(review_text):\n",
    "                if token.is_alpha == True:\n",
    "                    if token.is_stop == False:\n",
    "                        review_arr.append(str(token.lemma_).lower())\n",
    "            review_arr = np.array(review_arr)\n",
    "        \n",
    "            review_str = ''\n",
    "            for j in np.unique(review_arr):\n",
    "                review_str = review_str + j +' '\n",
    "        \n",
    "        #print(model(review_str).vector.shape)\n",
    "        #print(reviewer_vectors.shape)\n",
    "            reviewer_vectors[i] = model(review_str).vector\n",
    "    \n",
    "    return reviewer_vectors\n",
    "\n",
    "def GetCosineSims(doc_vec, review_vec, df_reviewers=df_reviewers):\n",
    "    all_usernames = []\n",
    "    all_domains = []\n",
    "    all_cosine_sims = []\n",
    "    for j in range(len(review_vec)):\n",
    "        if pd.isna(df_reviewers.iloc[j+1]['Domains/topic areas you are comfortable reviewing']) == False:\n",
    "            all_cosine_sims.append(cosine_similarity(np.array([doc_vec]), np.array([review_vec[j]]))[0,0])\n",
    "            all_domains.append(df_reviewers.iloc[j+1]['Domains/topic areas you are comfortable reviewing'].lower())\n",
    "            all_usernames.append(df_reviewers.iloc[j+1].username)\n",
    "    all_usernames= np.array(all_usernames)\n",
    "    all_domains= np.array(all_domains)\n",
    "    all_cosine_sims= np.array(all_cosine_sims)\n",
    "    \n",
    "    return all_usernames, all_domains, all_cosine_sims\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "318522ac-a001-40a3-8379-9ef63165b5ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "top20_tf_idf = Get_Top_Words_tf_idf(PAPER_OF_INTEREST_FNAME[Q])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e1cd08-0799-4615-a2dd-eacd859cccd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "top20_tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d9a53d9-cfe4-4b56-ac65-621d37507cea",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_top20= ''\n",
    "for i in top20_tf_idf:\n",
    "    doc_top20 = doc_top20 + i +' '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "798932bc-ef1a-4c2a-baa8-a69e4e0437c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_usernames2, all_domains2, all_cosine_sims2 = GetCosineSims(model(doc_top20).vector, reviewer_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dbd5020-4a4f-4243-b521-6dc391f0c1a9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df0ad432-e4ba-4204-b9ab-4096b94b956e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "175fefec-40d4-43bd-bcf3-30fc3000ec5e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e70d4ba7-7734-4985-a383-90b35f93c910",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2326e34e-bce3-4ef5-8d7b-a384a755b0c5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "e7b1fb42-dd7e-4fb9-a250-c577577874f8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "ed3f0a94-318e-4bfe-ba5c-f5f46468f7db",
   "metadata": {},
   "source": [
    "# Idea 4 (spaCy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "dff4455c-addb-4081-bb22-2ed29487c065",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "### Only Green Text - No Other Preprocessing ###\n",
      "Hello.\n",
      " I have found 5 possible reviewers for this paper.\n",
      "\n",
      "I believe \u001b[32malexpghayes\u001b[0m will be a good reviewer for this paper. Their domain interests and this paper have a cosine similairity score of \u001b[34m0.8541\u001b[0m. This reviewers domain interests are \u001b[31mi'm interested in making sure that r packages for modelling have useful and intuitive interfaces and documentation. i'm not interested in double checking theory and correctness, but making sure that a new user can quickly and easily get the results they want.\u001b[0m\n",
      "\n",
      "I believe \u001b[32mmichaelberks\u001b[0m will be a good reviewer for this paper. Their domain interests and this paper have a cosine similairity score of \u001b[34m0.8437\u001b[0m. This reviewers domain interests are \u001b[31mmedical imaging, image processing, compute vision, machine learning/ai applied to images, general scientific/numerical data processing (unless requiring specialist domain specific knowledge outside of the areas listed above)\u001b[0m\n",
      "\n",
      "I believe \u001b[32musethedata\u001b[0m will be a good reviewer for this paper. Their domain interests and this paper have a cosine similairity score of \u001b[34m0.8327\u001b[0m. This reviewers domain interests are \u001b[31mmost things with a science application, but particularly earth and environmental sciences, remote sensing, and chemistry. from a computing perspective, i'm strongest in information/data architecture and back-end.\u001b[0m\n",
      "\n",
      "I believe \u001b[32mrmorgan10\u001b[0m will be a good reviewer for this paper. Their domain interests and this paper have a cosine similairity score of \u001b[34m0.8188\u001b[0m. This reviewers domain interests are \u001b[31mmachine learning, deep learning, image classification, time series data, astrophysics, cosmology\u001b[0m\n",
      "\n",
      "I believe \u001b[32mr3w0p\u001b[0m will be a good reviewer for this paper. Their domain interests and this paper have a cosine similairity score of \u001b[34m0.8116\u001b[0m. This reviewers domain interests are \u001b[31minternet of things, complex event processing, fault tolerance, machine learning\u001b[0m\n",
      "\n",
      "\n",
      "### Green Text & Lemmasation ###\n",
      "Hello.\n",
      " I have found 5 possible reviewers for this paper.\n",
      "\n",
      "I believe \u001b[32mTomGoffrey\u001b[0m will be a good reviewer for this paper. Their domain interests and this paper have a cosine similairity score of \u001b[34m0.9073\u001b[0m. This reviewers domain interests are \u001b[31mastrophysics (my main area of expertise is in stellar physics, but i also have some background in planetary physics), plasma physics, hydrodynamics (shock capturing and modelling in particular), magnetohydrodynamics (i have worked a lot on non-ideal mhd in particular), radiation transport, (large )linear systems, time-integration methods, finite volume methods, arbitrary/lagrangian meshes, multi-material flow.\u001b[0m\n",
      "\n",
      "I believe \u001b[32maaronpeikert\u001b[0m will be a good reviewer for this paper. Their domain interests and this paper have a cosine similairity score of \u001b[34m0.9042\u001b[0m. This reviewers domain interests are \u001b[31mmachine learning, - model selection, - structural equation modelling, - multimodel inference, - hierarchical data, , open science, - reproducibility, - dynamic document generation, - containers, - version control\u001b[0m\n",
      "\n",
      "I believe \u001b[32mmichaelberks\u001b[0m will be a good reviewer for this paper. Their domain interests and this paper have a cosine similairity score of \u001b[34m0.8824\u001b[0m. This reviewers domain interests are \u001b[31mmedical imaging, image processing, compute vision, machine learning/ai applied to images, general scientific/numerical data processing (unless requiring specialist domain specific knowledge outside of the areas listed above)\u001b[0m\n",
      "\n",
      "I believe \u001b[32malexpghayes\u001b[0m will be a good reviewer for this paper. Their domain interests and this paper have a cosine similairity score of \u001b[34m0.8674\u001b[0m. This reviewers domain interests are \u001b[31mi'm interested in making sure that r packages for modelling have useful and intuitive interfaces and documentation. i'm not interested in double checking theory and correctness, but making sure that a new user can quickly and easily get the results they want.\u001b[0m\n",
      "\n",
      "I believe \u001b[32mlucask07\u001b[0m will be a good reviewer for this paper. Their domain interests and this paper have a cosine similairity score of \u001b[34m0.8670\u001b[0m. This reviewers domain interests are \u001b[31minstrument control, (possibly) image analysis, experiment data analysis (physics or ee based)\u001b[0m\n",
      "\n",
      "\n",
      "### Green Text & Lemmasation & Unique Words ###\n",
      "Hello.\n",
      " I have found 5 possible reviewers for this paper.\n",
      "\n",
      "I believe \u001b[32mTomGoffrey\u001b[0m will be a good reviewer for this paper. Their domain interests and this paper have a cosine similairity score of \u001b[34m0.9030\u001b[0m. This reviewers domain interests are \u001b[31mastrophysics (my main area of expertise is in stellar physics, but i also have some background in planetary physics), plasma physics, hydrodynamics (shock capturing and modelling in particular), magnetohydrodynamics (i have worked a lot on non-ideal mhd in particular), radiation transport, (large )linear systems, time-integration methods, finite volume methods, arbitrary/lagrangian meshes, multi-material flow.\u001b[0m\n",
      "\n",
      "I believe \u001b[32mmichaelberks\u001b[0m will be a good reviewer for this paper. Their domain interests and this paper have a cosine similairity score of \u001b[34m0.8993\u001b[0m. This reviewers domain interests are \u001b[31mmedical imaging, image processing, compute vision, machine learning/ai applied to images, general scientific/numerical data processing (unless requiring specialist domain specific knowledge outside of the areas listed above)\u001b[0m\n",
      "\n",
      "I believe \u001b[32maaronpeikert\u001b[0m will be a good reviewer for this paper. Their domain interests and this paper have a cosine similairity score of \u001b[34m0.8865\u001b[0m. This reviewers domain interests are \u001b[31mmachine learning, - model selection, - structural equation modelling, - multimodel inference, - hierarchical data, , open science, - reproducibility, - dynamic document generation, - containers, - version control\u001b[0m\n",
      "\n",
      "I believe \u001b[32malexpghayes\u001b[0m will be a good reviewer for this paper. Their domain interests and this paper have a cosine similairity score of \u001b[34m0.8816\u001b[0m. This reviewers domain interests are \u001b[31mi'm interested in making sure that r packages for modelling have useful and intuitive interfaces and documentation. i'm not interested in double checking theory and correctness, but making sure that a new user can quickly and easily get the results they want.\u001b[0m\n",
      "\n",
      "I believe \u001b[32musethedata\u001b[0m will be a good reviewer for this paper. Their domain interests and this paper have a cosine similairity score of \u001b[34m0.8785\u001b[0m. This reviewers domain interests are \u001b[31mmost things with a science application, but particularly earth and environmental sciences, remote sensing, and chemistry. from a computing perspective, i'm strongest in information/data architecture and back-end.\u001b[0m\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "texts, arr = MakeGreenRedText(Paper_interest, False)\n",
    "W2V_spaCy_Processing(texts, green_text=True, lemma=True, unique=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "552b3724-7e74-4a6c-8cb3-01bd22d70329",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "245c7232-0f8e-4ab8-bb56-10cdeff87b9e",
   "metadata": {},
   "source": [
    "# Idea 6 spaCy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89abb72c-b96d-4672-a6e5-8b8180cbd307",
   "metadata": {},
   "source": [
    "# Idea 6 spaCy Functions\n",
    "Hidden for clarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a7f191-84d9-43ad-84ea-2dbf9d5fd6c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def TopReviewers(number=5, all_usernames=all_usernames, all_domains=all_domains, all_cosine_sims=all_cosine_sims):\n",
    "    message = 'Hello.\\n I have found ' +str(number) + ' possible reviewers for this paper.'+ '\\n\\n'\n",
    "    for J in range(number):\n",
    "        index = np.argsort(all_cosine_sims)[-1-J]\n",
    "        #print(index)\n",
    "        ps = 'I believe '+ colored(str(all_usernames[index]), 'green') + ' will be a good reviewer for this paper. Their domain interests and this paper have a cosine similairity score of ' + colored(str(all_cosine_sims[index])[:6], 'blue') + '. This reviewers domain interests are ' + colored(str(all_domains[index].replace('\\n', ',')), 'red')\n",
    "        message = message + ps + '\\n\\n'\n",
    "    print(message)  \n",
    "    \n",
    "def Idea6_lemma_Text(arr):\n",
    "    text_list = []\n",
    "    for J in range(len(arr)):\n",
    "        doc = model(str(arr[J]))\n",
    "        \n",
    "        doc_arr = []\n",
    "        for token in doc:\n",
    "            if token.is_alpha == True:\n",
    "                if token.is_stop == False:\n",
    "                    doc_arr.append(str(token.lemma_).lower())\n",
    "        doc_arr = np.array(doc_arr)\n",
    "        \n",
    "        doc_arr_mod = ''\n",
    "        for i in doc_arr:\n",
    "            doc_arr_mod = doc_arr_mod + i +' '\n",
    "        #print(doc_arr_mod)\n",
    "        text_list.append(doc_arr_mod)\n",
    "    return text_list\n",
    "\n",
    "def Idea6_unique_lemma_Text(arr):\n",
    "    text_list = []\n",
    "    for J in range(len(arr)):\n",
    "        doc = model(str(arr[J]))\n",
    "        \n",
    "        doc_arr = []\n",
    "        for token in doc:\n",
    "            if token.is_alpha == True:\n",
    "                if token.is_stop == False:\n",
    "                    doc_arr.append(str(token.lemma_).lower())\n",
    "        doc_arr = np.array(doc_arr)\n",
    "        doc_str = ''\n",
    "        for i in np.unique(doc_arr):\n",
    "            doc_str = doc_str + i +' '\n",
    "        text_list.append(doc_str)\n",
    "    return text_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7cd95136-f4e6-432e-89c3-b94eee795061",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviewer_vectors = GetReviewer_Vectors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae6fd130-0529-4fcd-86f7-2ab2d91b5b01",
   "metadata": {},
   "outputs": [],
   "source": [
    "average_vector_text = Get_Paper_Vector_Idea6_SpaCy(arr,model)\n",
    "all_usernames, all_domains, all_cosine_sims = GetCosineSims(average_vector_text, reviewer_vectors)\n",
    "  \n",
    "TopReviewers()    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd10de16-b4b3-4bb9-927c-4841d05eae0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_list_lemma = Idea6_lemma_Text(arr)\n",
    "average_vector_text2 = Get_Paper_Vector_Idea6_SpaCy(text_list_lemma,model)\n",
    "all_usernames2, all_domains2, all_cosine_sims2 = GetCosineSims(average_vector_text2, reviewer_vectors)\n",
    "TopReviewers(5, all_usernames2, all_domains2, all_cosine_sims2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148fdfef-4897-4c39-b035-0a714e8a3d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "text_list_unique_lemma = Idea6_unique_lemma_Text(arr)\n",
    "average_vector_text3 = Get_Paper_Vector_Idea6_SpaCy(text_list_unique_lemma,model)\n",
    "all_usernames3, all_domains3, all_cosine_sims3 = GetCosineSims(average_vector_text3, reviewer_vectors)\n",
    "TopReviewers(5,all_usernames3, all_domains3, all_cosine_sims3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dd4a20e-6f15-4c8d-836a-e533f627e932",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84f1861c-44f8-45d8-9218-8a744ec06293",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4140566e-f41f-4d79-8ec9-64042c1a0ef0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4c655080-9d25-42c1-a4f5-e068e362ad9e",
   "metadata": {},
   "source": [
    "# Idea 9 (Sense2Vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd8a050e-f0a7-4364-8d12-6c28dba86a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sense2vec import Sense2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f4d441c-c475-488a-ad58-eb480f53a2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "s2v = Sense2Vec().from_disk(\"../../s2v_old\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887c8790-62a0-4269-bcd6-72e0aa8a6902",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62e7997c-c7d6-4d3f-8697-20ce5713f83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts, arr = MakeGreenRedText(Paper_interest,False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dccafe60-955c-4d66-9399-59d37d068891",
   "metadata": {},
   "source": [
    "# Idea 9 (Sense2Vec) Functions\n",
    "Hidden for clarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7e9443-1944-402f-ba44-901c44547d3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetPaperVector_Sense2Vec(text):\n",
    "    doc = model(text)\n",
    "    doc_arr = []\n",
    "    pos_arr = []\n",
    "    tag_arr = []\n",
    "    dep_arr = []\n",
    "    for token in doc:\n",
    "        if token.is_alpha == True:\n",
    "            if token.is_stop == False:\n",
    "                doc_arr.append(str(token.lemma_).lower())\n",
    "                pos_arr.append(str(token.pos_))\n",
    "                tag_arr.append(str(token.tag_))\n",
    "                dep_arr.append(str(token.dep_))\n",
    "            \n",
    "    doc_arr = np.array(doc_arr)\n",
    "    pos_arr = np.array(pos_arr)\n",
    "    tag_arr = np.array(tag_arr)\n",
    "    dep_arr = np.array(dep_arr)\n",
    "\n",
    "    word_vec = np.zeros((128))\n",
    "    counter = 0\n",
    "    for P in range(len(doc_arr)):\n",
    "        best = s2v.get_best_sense(doc_arr[P])\n",
    "        if best != None:\n",
    "            vector = s2v[best]\n",
    "            word_vec = word_vec + vector\n",
    "            counter = counter + 1\n",
    "    average_word_vec = word_vec / counter\n",
    "    \n",
    "    return average_word_vec\n",
    "\n",
    "\n",
    "\n",
    "def GetReviewerSample_Sense2Vec(paper_vec, df_reviewers=df_reviewers):\n",
    "    all_usernames = []\n",
    "    all_domains = []\n",
    "    all_cosine_sims = []\n",
    "    for j in range(df_reviewers.shape[0]-1):\n",
    "        if pd.isna(df_reviewers.iloc[j+1]['Domains/topic areas you are comfortable reviewing']) == False:\n",
    "            reviewer_interests = df_reviewers.iloc[j+1]['Domains/topic areas you are comfortable reviewing'].lower()\n",
    "            reviewer_interests.replace('/',' ')\n",
    "            doc = model(reviewer_interests)\n",
    "            reviewer_arr = []\n",
    "            for token in doc:\n",
    "                if token.is_alpha == True:\n",
    "                    if token.is_stop == False:\n",
    "                        reviewer_arr.append(str(token.lemma_).lower())\n",
    "            reviewer_arr = np.array(reviewer_arr)\n",
    "            word_vec = np.zeros((128))\n",
    "            counter = 0\n",
    "            for P in range(len(reviewer_arr)):\n",
    "                best = s2v.get_best_sense(reviewer_arr[P])\n",
    "                if best != None:\n",
    "                    vector = s2v[best]\n",
    "                    word_vec = word_vec + vector\n",
    "                    counter = counter + 1\n",
    "            \n",
    "            if counter > 0:\n",
    "                average_reviewer_vec = word_vec / counter\n",
    "          \n",
    "                all_usernames.append(df_reviewers.username.iloc[j+1])\n",
    "                all_domains.append(reviewer_interests)\n",
    "                all_cosine_sims.append(cosine_similarity(np.array([paper_vec]), np.array([average_reviewer_vec]))[0,0])\n",
    "    return np.array(all_usernames), np.array(all_domains), np.array(all_cosine_sims)\n",
    " \n",
    "def TopReviewers(number=5, all_usernames=all_usernames, all_domains=all_domains, all_cosine_sims=all_cosine_sims):\n",
    "    message = 'Hello.\\nI have found ' +str(number) + ' possible reviewers for this paper.'+ '\\n\\n'\n",
    "    for J in range(number):\n",
    "        index = np.argsort(all_cosine_sims)[-1-J]\n",
    "        #print(index)\n",
    "        ps = 'I believe '+ colored(str(all_usernames[index]), 'green') + ' will be a good reviewer for this paper. Their domain interests and this paper have a cosine similairity score of ' + colored(str(all_cosine_sims[index])[:6], 'blue') + '. This reviewers domain interests are ' + colored(str(all_domains[index].replace('\\n', ',')), 'red')\n",
    "        message = message + ps + '\\n\\n'\n",
    "    print(message) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9769905-9376-449f-b0d4-dfeb57af91ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_usernames, all_domains, all_cosine_sims = GetReviewerSample_Sense2Vec(average_word_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da49a740-7fa7-42a7-9d6d-98ba52d9d226",
   "metadata": {},
   "outputs": [],
   "source": [
    "TopReviewers()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
