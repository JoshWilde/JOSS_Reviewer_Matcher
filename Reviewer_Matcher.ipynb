{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "162e2783-1e73-4e84-9a6d-98becf60f145",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from nltk import word_tokenize\n",
    "from nltk import download\n",
    "from nltk.corpus import stopwords\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "%matplotlib inline\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6a507d08-65d6-4aa0-9ff3-3734503c039a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from JOSS_PDF_Cleaner import Clean_PDF\n",
    "import re\n",
    "from termcolor import colored\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8804f7bd-81aa-4856-92f8-a38934dbab17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import glob\n",
    "import string\n",
    "import glob\n",
    "from tqdm import tqdm \n",
    "#import pdfminer\n",
    "from pdfminer.high_level import extract_text\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk \n",
    "#nltk.download('stopwords')\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import nltk\n",
    "#nltk.download('punkt')\n",
    "from nltk import sent_tokenize\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "#nltk.download('wordnet')\n",
    "from nltk.probability import FreqDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "bac7bdc7-55e9-482d-b60e-d869b88bd8cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/sunilmcesh/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/sunilmcesh/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/sunilmcesh/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "#from Master_Methods import *\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import glob\n",
    "import string\n",
    "import glob\n",
    "from tqdm import tqdm \n",
    "#import pdfminer\n",
    "from pdfminer.high_level import extract_text\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk \n",
    "nltk.download('stopwords')\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk import sent_tokenize\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "from nltk.probability import FreqDist\n",
    "from JOSS_PDF_Cleaner import Clean_PDF\n",
    "import re\n",
    "from termcolor import colored\n",
    "import spacy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d7010c21-b0e3-40e2-821b-a440f0a7f99a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca7af63a-561a-4afb-82c4-3047dacc9529",
   "metadata": {},
   "source": [
    "# Import Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "66357afc-9d23-494b-a1ce-4f7f12682887",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://zenodo.org/record/3631674#.YeglfP7P2Uk\n",
    "df = pd.read_csv('/Volumes/Seagate Backup Plus Drive/JOSS Project/wiki_tfidf_terms.csv')\n",
    "df_reviewers = pd.read_csv('../JOSS_Reviewer_Matcher/Data/JOSS Table Test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdbbee41-a44b-44df-892b-6125d96410b2",
   "metadata": {},
   "source": [
    "# Select Paper to find Reviewers for"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "e6f2c8b7-a13b-4048-ad02-38feb953934b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Volumes/Seagate Backup Plus Drive/JOSS Project/joss-papers-master/joss-papers-master/joss.01539/10.21105.joss.01539.pdf\n"
     ]
    }
   ],
   "source": [
    "#PAPER_OF_INTEREST_FNAME  = glob.glob('/Volumes/Seagate Backup Plus Drive/JOSS Project/joss-papers-master/*/*/*.pdf')\n",
    "#K = 260\n",
    "#Paper_interest = PAPER_OF_INTEREST_FNAME[K] # Replace with path to paper of interest\n",
    "Paper_interest = '/Volumes/Seagate Backup Plus Drive/JOSS Project/joss-papers-master/joss-papers-master/joss.01539/10.21105.joss.01539.pdf'\n",
    "print(Paper_interest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9fd9fce1-cbaf-4cde-9811-32a0ff76ad3e",
   "metadata": {},
   "source": [
    "# Printing Paper of Interest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "8da8f57f-0a85-42ad-98d5-adc31af6ac8d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def MakeGreenRedText(Paper_interest, printer=True):\n",
    "    texts = ''\n",
    "    arr = []\n",
    "    from pdfminer.high_level import extract_pages\n",
    "    from pdfminer.layout import LTTextContainer\n",
    "    for page_layout in extract_pages(Paper_interest):\n",
    "        for element in page_layout:\n",
    "            if isinstance(element, LTTextContainer):\n",
    "                score = Clean_PDF(element.get_text().lower())\n",
    "            #print(score)\n",
    "                if score == 0:\n",
    "                    if printer == True:\n",
    "                        print(colored(element.get_text().lower(), 'green'))\n",
    "                    arr.append(element.get_text())\n",
    "                    texts = texts  + element.get_text() + ' '\n",
    "                else:\n",
    "                    if printer == True:\n",
    "                        print(colored(element.get_text().lower(), 'red'))\n",
    "            #arr.append(element.get_text())\n",
    "            #texts = texts  + element.get_text() + ' '\n",
    "    arr = np.array(arr)\n",
    "    return texts, arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "f0ae3840-553a-4cf6-9b1c-cc4db3241fc6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mdoi: 10.21105/joss.01539\n",
      "\u001b[0m\n",
      "\u001b[31msoftware\n",
      "\u001b[0m\n",
      "\u001b[31m• review\n",
      "• repository\n",
      "• archive\n",
      "\u001b[0m\n",
      "\u001b[31msubmitted: 25 may 2019\n",
      "published: 24 september 2019\n",
      "\u001b[0m\n",
      "\u001b[31mlicense\n",
      "authors of papers retain\n",
      "copyright and release the work\n",
      "under a creative commons\n",
      "attribution 4.0 international\n",
      "license (cc-by).\n",
      "\u001b[0m\n",
      "\u001b[32mpulse: a python package based on fenics for solving\n",
      "problems in cardiac mechanics\n",
      "\u001b[0m\n",
      "\u001b[32mhenrik nicolay topnes finsberg1\n",
      "\u001b[0m\n",
      "\u001b[31m1 simula research laboratory, oslo, norway\n",
      "\u001b[0m\n",
      "\u001b[31msummary\n",
      "\u001b[0m\n",
      "\u001b[32mpulse is a software package based on fenics (logg, mardal, & wells, 2012) that aims to\n",
      "solve problems in cardiac mechanics (but is easily extended to solve more general problems in\n",
      "continuum mechanics). pulse is a result of the author’s phd thesis (h. n. finsberg, 2017),\n",
      "where most of the relevant background for the code can be found.\n",
      "\u001b[0m\n",
      "\u001b[32mwhile fenics offers a general framework for solving pdes, pulse specifically targets problems\n",
      "in continuum mechanics. therefore, most of the code for applying compatible boundary\n",
      "conditions, formulating the governing equations, choosing appropriate spaces for the solutions\n",
      "and applying iterative strategies, etc., are already implemented, so that the user can focus on\n",
      "the actual problem he/she wants to solve rather than implementing all the necessary code for\n",
      "formulating and solving the underlying equations.\n",
      "\u001b[0m\n",
      "\u001b[32mthe user can pick any of the built-in meshes or choose a custom user-defined mesh. the user\n",
      "also need to provide appropriate markers for the boundaries where the boundary conditions\n",
      "will be applied, as well as microstructural information (i.e., information about muscle fiber\n",
      "orientations) if an anisotropic model is to be used. examples of how to create custom idealized\n",
      "geometries as well as appropriate microstructure can be found in another repository called\n",
      "ldrb, which uses the laplace-dirichlet rule-based (ldrb) algorithm (bayer, blake, plank, &\n",
      "trayanova, 2012) for assigning myocardial fiber orientations.\n",
      "\u001b[0m\n",
      "\u001b[32mnext the user needs to select a material model or create a custom material model, and define\n",
      "appropriate boundary conditions (dirichlet, neumann, or robin boundary conditions). finally\n",
      "a mechanicsproblem is built using the geometry, material, and boundary conditions. figure\n",
      "1 shows the different components involved as well as how they are related.\n",
      "\u001b[0m\n",
      "\u001b[31mfinsberg, (2019). pulse: a python package based on fenics for solving problems in cardiac mechanics. journal of open source software,\n",
      "4(41), 1539. https://doi.org/10.21105/joss.01539\n",
      "\u001b[0m\n",
      "\u001b[31m1\n",
      "\u001b[0m\n",
      "\u001b[31mfigure 1: visualization of the different components that are part of the pulse library.\n",
      "\u001b[0m\n",
      "\u001b[32mthe problem is solved using some iterative strategy, either with an incremental load technique\n",
      "with fixed or adaptive stepping and/or using with a continuation technique (h. n. finsberg,\n",
      "2017).\n",
      "\u001b[0m\n",
      "\u001b[31mit is also possible to estimate the unloaded zero-pressure geometry (bols et al., 2013). this\n",
      "is of particular importance if the geometry being used is taken from a medical image of a\n",
      "patient. in this case, the geometry is subjected to some load due to the blood pressure, and\n",
      "therefore in order to correctly assess the stresses, one need to first find the unloaded geometry.\n",
      "\u001b[0m\n",
      "\u001b[31mpapers using this code includes (h. finsberg et al., 2018a) and (h. finsberg et al., 2018b).\n",
      "\u001b[0m\n",
      "\u001b[31ma collection of different demos showing how to use the pulse library is found in the repository,\n",
      "including an implementation of a cardiac mechanics bechmark (land et al., 2015), how to\n",
      "use a custom material model, and how to use a compressible model rather than the default\n",
      "incompressible model.\n",
      "\u001b[0m\n",
      "\u001b[31mreferences\n",
      "\u001b[0m\n",
      "\u001b[31mbayer, j. d., blake, r. c., plank, g., & trayanova, n. a. (2012). a novel rule-based algorithm\n",
      "for assigning myocardial fiber orientation to computational heart models. annals of biomedical\n",
      "engineering, 40(10), 2243–2254. doi:10.1007/s10439-012-0593-5\n",
      "\u001b[0m\n",
      "\u001b[31mbols, j., degroote, j., trachet, b., verhegghe, b., segers, p., & vierendeels, j. (2013). a\n",
      "computational method to assess the in vivo stresses and unloaded configuration of patient-\n",
      "specific blood vessels. journal of computational and applied mathematics, 246, 10–17.\n",
      "doi:10.1016/j.cam.2012.10.034\n",
      "\u001b[0m\n",
      "\u001b[31mfinsberg, h. n. (2017). patient-specific computational modeling of cardiac mechanics. series\n",
      "of dissertations submitted to the faculty of mathematics and natural sciences, university of\n",
      "oslo.\n",
      "\u001b[0m\n",
      "\u001b[31mfinsberg, h., balaban, g., ross, s., håland, t. f., odland, h. h., sundnes, j., & wall,\n",
      "s. (2018a). estimating cardiac contraction through high resolution data assimilation of a\n",
      "personalized mechanical model. journal of computational science, 24, 85–90. doi:10.1016/\n",
      "j.jocs.2017.07.013\n",
      "\u001b[0m\n",
      "\u001b[31mfinsberg, (2019). pulse: a python package based on fenics for solving problems in cardiac mechanics. journal of open source software,\n",
      "4(41), 1539. https://doi.org/10.21105/joss.01539\n",
      "\u001b[0m\n",
      "\u001b[31m2\n",
      "\u001b[0m\n",
      "\u001b[31mfinsberg, h., xi, c., tan, j. l., zhong, l., genet, m., sundnes, j., lee, l. c., et al.\n",
      "(2018b). efficient estimation of personalized biventricular mechanical function employing\n",
      "gradient-based optimization. international journal for numerical methods in biomedical en-\n",
      "gineering, 34(7), e2982. doi:10.1002/cnm.2982\n",
      "\u001b[0m\n",
      "\u001b[31mland, s., gurev, v., arens, s., augustin, c. m., baron, l., blake, r., bradley, c., et al. (2015).\n",
      "verification of cardiac mechanics software: benchmark problems and solutions for testing\n",
      "active and passive material behaviour. proceedings of the royal society a: mathematical,\n",
      "physical and engineering sciences, 471(2184), 20150641. doi:10.1098/rspa.2015.0641\n",
      "\u001b[0m\n",
      "\u001b[31mlogg, a., mardal, k.-a., & wells, g. (2012). automated solution of differential equations by\n",
      "the finite element method: the fenics book (vol. 84). springer science & business media.\n",
      "\u001b[0m\n",
      "\u001b[31mfinsberg, (2019). pulse: a python package based on fenics for solving problems in cardiac mechanics. journal of open source software,\n",
      "4(41), 1539. https://doi.org/10.21105/joss.01539\n",
      "\u001b[0m\n",
      "\u001b[31m3\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "texts, arr = MakeGreenRedText(Paper_interest)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f113e5dc-08a7-4d15-9cc2-8e6b4362c257",
   "metadata": {},
   "source": [
    "# All Functions\n",
    "Hidden for clarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "f4b5ccb6-ef6c-4ddb-a2a8-ea381623715d",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "################# Idea 1 #####################\n",
    "def Get_Lemma_Words(POI_PDF):\n",
    "  text = str(POI_PDF)\n",
    "  text2 = text.split()\n",
    "  words_no_punc = []\n",
    "\n",
    "  for w in text2:\n",
    "    if w.isalpha():\n",
    "      words_no_punc.append(w.lower())\n",
    "  from nltk.corpus import stopwords\n",
    "  stopwords = stopwords.words('english')  \n",
    "  clean_words = []\n",
    "  for w in words_no_punc:\n",
    "    if w not in stopwords:\n",
    "      clean_words.append(w)\n",
    "  clean_words_arr = ''\n",
    "  for i in range(len(clean_words)):\n",
    "    clean_words_arr = clean_words_arr + ' ' + str(clean_words[i])\n",
    "\n",
    "  string_for_lemmatizing = clean_words_arr\n",
    "  lemmatizer = WordNetLemmatizer()\n",
    "  words_2 = word_tokenize(string_for_lemmatizing)\n",
    "  lemmatized_words = [lemmatizer.lemmatize(word) for word in words_2]\n",
    "\n",
    "  lemmatized_words_arr = ''\n",
    "  for i in range(len(lemmatized_words)):\n",
    "    lemmatized_words_arr = lemmatized_words_arr + ' ' + str(lemmatized_words[i])\n",
    "  words = word_tokenize(lemmatized_words_arr)\n",
    "\n",
    "  return words\n",
    "\n",
    "def Get_reviewer_sample_tf(Paper_interest, df_reviewers=df_reviewers,num_suggestions=5, num_top20=20):\n",
    "    POI_PDF = [extract_text(Paper_interest)]\n",
    "    text = str(POI_PDF)\n",
    "    words =  Get_Lemma_Words(POI_PDF)\n",
    "    #print(len(words))\n",
    "    fdist = FreqDist(words)\n",
    "    X = np.array(fdist.most_common())\n",
    "    top20_tf = X[:num_top20,0]\n",
    "    match_arr = Compare_topics(top20_tf, df_reviewers)\n",
    "    top5_reviewers = np.argsort(match_arr)[-num_suggestions:]\n",
    "\n",
    "    \n",
    "    all_usernames = []\n",
    "    all_domains = []\n",
    "    all_num_matched_words = []\n",
    "    all_matched_words = []\n",
    "    for i in range(num_suggestions):\n",
    "      K = -1*(i+1)\n",
    "      index = top5_reviewers[K]\n",
    "      #print(i)\n",
    "      t =df_reviewers.iloc[index+1]['Domains/topic areas you are comfortable reviewing'].lower()\n",
    "      \n",
    "      all_usernames.append(df_reviewers.username.iloc[index+1])\n",
    "      all_domains.append(t)\n",
    "      all_num_matched_words.append(match_arr[index])\n",
    "\n",
    "      uniarr = Split_columns(t)\n",
    "      matched_words = []\n",
    "      #print(uniarr)\n",
    "      for j in range(len(uniarr)):\n",
    "        for k in range(len(top20_tf)):\n",
    "          if uniarr[j] == top20_tf[k]:\n",
    "            matched_words.append(uniarr[j])\n",
    "      all_matched_words.append(matched_words)\n",
    "\n",
    "    #df_reviewers.username.iloc[+1]\n",
    "\n",
    "    return all_usernames, all_domains, all_num_matched_words, all_matched_words\n",
    "\n",
    "def Compare_topics(top20, df_reviewers):\n",
    "  length = df_reviewers.shape[0] - 1\n",
    "  match_arr = np.zeros(length)\n",
    "  for i in range(length):\n",
    "    if pd.isna(df_reviewers['Domains/topic areas you are comfortable reviewing'].str.lower().values[1+i]) == False:\n",
    "      t = df_reviewers['Domains/topic areas you are comfortable reviewing'].str.lower().values[1+i]\n",
    "      #print(i)\n",
    "      uniarr = Split_columns(t)\n",
    "      for j in range(len(uniarr)):\n",
    "        for k in range(len(top20)):\n",
    "          if uniarr[j] == top20[k]:\n",
    "            match_arr[i] = match_arr[i] + 1\n",
    "  return match_arr\n",
    "\n",
    "def Split_columns(t):\n",
    "  txt = \" \".join(\"\".join([\" \" if ch in string.punctuation else ch for ch in t]).split())\n",
    "  sol1 = np.char.split(txt, ' ')\n",
    "  txt_arr  = array_of_lists_to_array(sol1)\n",
    "  uniarr = np.unique(txt_arr)\n",
    "  return uniarr\n",
    "\n",
    "def array_of_lists_to_array(arr):\n",
    "    return np.apply_along_axis(lambda a: np.array(a[0]), -1, arr[..., None])\n",
    "\n",
    "\n",
    "def summatation_bot_tf(all_usernames, all_domains, all_num_matched_words, all_matched_words):\n",
    "  length = len(all_usernames)\n",
    "  message = 'Hello. \\nI have found ' + str(length) + ' possible reviewers for this paper.' +'\\n\\n'\n",
    "  for i in range(length):\n",
    "    ps = 'I believe ' + colored(all_usernames[i], 'green') + ' will make a good reviewer for this paper because they have matched ' + colored(str(int(all_num_matched_words[i])), 'blue') +  ' words from their comfortable domain topics with the top 20 most frequent words in the paper. These matched words are ' + colored(str(all_matched_words[i]), 'blue') +'.\\nFrom their topics domain: ' + colored(str(all_domains[i].replace('\\n', ', ')), 'red') +'.\\n'\n",
    "    message = message + ps + '\\n'\n",
    "  print(message)\n",
    "\n",
    "#########################################################################################################################################################\n",
    "\n",
    "#########################################################################################################################################################\n",
    "\n",
    "################# Idea 2 #####################\n",
    "\n",
    "# Idea 2\n",
    "\n",
    "def Get_reviewer_sample_tf_idf(Paper_interest, df=df, df_reviewers=df_reviewers, num_suggestions=5, num_top20=20):\n",
    "    POI_PDF = [extract_text(Paper_interest)]\n",
    "    text = str(POI_PDF)\n",
    "    words = Get_Lemma_Words(POI_PDF)\n",
    "    #print(len(words))\n",
    "    fdist = FreqDist(words)\n",
    "    X = np.array(fdist.most_common())\n",
    "    tf_idf_arr_names, tf_idf_arr_floats = determine_wiki_td_idf(X, df=df)\n",
    "    #print('determined wiki')\n",
    "    num_arr = np.array(tf_idf_arr_floats)\n",
    "    tf_idf_arr_names_arr = np.array(tf_idf_arr_names)\n",
    "    top20_tf_idf = tf_idf_arr_names_arr[np.argsort(num_arr)[:20]]\n",
    "    match_arr = Compare_topics(top20_tf_idf, df_reviewers)\n",
    "    top5_reviewers = np.argsort(match_arr)[-num_suggestions:]\n",
    "    \n",
    "    \n",
    "    all_usernames = []\n",
    "    all_domains = []\n",
    "    all_num_matched_words = []\n",
    "    all_matched_words = []\n",
    "    for i in range(num_suggestions):\n",
    "      K = -1*(i+1)\n",
    "      index = top5_reviewers[K]\n",
    "      #print(i)\n",
    "      t =df_reviewers.iloc[index+1]['Domains/topic areas you are comfortable reviewing'].lower()\n",
    "      \n",
    "      all_usernames.append(df_reviewers.username.iloc[index+1])\n",
    "      all_domains.append(t)\n",
    "      all_num_matched_words.append(match_arr[index])\n",
    "\n",
    "      uniarr = Split_columns(t)\n",
    "      matched_words = []\n",
    "      #print(uniarr)\n",
    "      for j in range(len(uniarr)):\n",
    "        for k in range(len(top20_tf_idf)):\n",
    "          if uniarr[j] == top20_tf_idf[k]:\n",
    "            matched_words.append(uniarr[j])\n",
    "      all_matched_words.append(matched_words)\n",
    "\n",
    "    #df_reviewers.username.iloc[+1]\n",
    "\n",
    "    return all_usernames, all_domains, all_num_matched_words, all_matched_words\n",
    "\n",
    "def determine_wiki_td_idf(x, df=df):\n",
    "    tf_idf_arr_names = []\n",
    "    tf_idf_arr_floats = []\n",
    "    for i in range(len(x)):\n",
    "        if df[df['token'] ==x[i][0]].frequency.empty == False:\n",
    "            wiki_tf = df[df['token'] ==x[i][0]].frequency.values[0]\n",
    "            doc_tf = int(x[i][1])\n",
    "            tf_idf = np.log(wiki_tf/doc_tf)\n",
    "            tf_idf_arr_names.append(x[i][0])\n",
    "            tf_idf_arr_floats.append(tf_idf)\n",
    "    return tf_idf_arr_names, tf_idf_arr_floats\n",
    "\n",
    "def summatation_bot_tf_idf(all_usernames, all_domains, all_num_matched_words, all_matched_words):\n",
    "  length = len(all_usernames)\n",
    "  message = 'Hello. \\nI have found ' + str(length) + ' possible reviewers for this paper.' +'\\n\\n'\n",
    "  for i in range(length):\n",
    "    ps = 'I believe ' + colored(all_usernames[i], 'green') + ' will make a good reviewer for this paper because they have matched ' + colored(str(int(all_num_matched_words[i])), 'blue') +  ' words from their comfortable domain topics with the top 20 most frequent words in the paper. These matched words are ' + colored(str(all_matched_words[i]), 'blue') +'.\\nFrom their topics domain: ' + colored(str(all_domains[i].replace('\\n', ', ')), 'red') +'.\\n'\n",
    "    message = message + ps + '\\n'\n",
    "  print(message)\n",
    "\n",
    "\n",
    "#########################################################################################################################################################\n",
    "\n",
    "#########################################################################################################################################################\n",
    "\n",
    "################# Gensim ####################\n",
    "\n",
    "def preprocess_gensim(text):\n",
    "    stop_words = stopwords.words('english')\n",
    "    text = text.lower()\n",
    "    doc = word_tokenize(text)\n",
    "    doc = [word for word in doc if word not in stop_words]\n",
    "    doc = [word for word in doc if word.isalpha()] #restricts string to alphabetic characters only\n",
    "    return doc\n",
    "\n",
    "def W2V_Gensim_Processing(texts,model, print_outside_corpus=True):\n",
    "    t = texts\n",
    "    t = t.replace('-\\\\n','')\n",
    "    t = t.replace('-\\n','')\n",
    "    t = t.replace('\\\\n',' ')\n",
    "    t = t.replace('\\n', ' ')\n",
    "    texts = t\n",
    "    #print(t)\n",
    "\n",
    "    #download('punkt') #tokenizer, run once\n",
    "    #download('stopwords') #stopwords dictionary, run once\n",
    "    stop_words = stopwords.words('english')\n",
    "\n",
    "    texts = [texts]\n",
    "    corpus = [preprocess_gensim(text) for text in texts]\n",
    "\n",
    "    sum_vector_text = np.zeros((300))\n",
    "    counter = 0\n",
    "    for i in range(len(corpus[0])):\n",
    "        if corpus[0][i] in model.vocab:\n",
    "            sum_vector_text = sum_vector_text + model.wv[corpus[0][i]]\n",
    "            counter = counter + 1\n",
    "        else:\n",
    "            if print_outside_corpus == True:\n",
    "                print(corpus[0][i])\n",
    "    average_vector_text = sum_vector_text/ counter\n",
    "    return average_vector_text\n",
    "\n",
    "\n",
    "def GetReviewerSample_W2V_Gensim(paper_vec, df_reviewers=df_reviewers):\n",
    "    warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
    "    all_usernames = []\n",
    "    all_domains = []\n",
    "    all_cosine_sims = []\n",
    "    for j in range(df_reviewers.shape[0]-1):\n",
    "        if pd.isna(df_reviewers.iloc[j+1]['Domains/topic areas you are comfortable reviewing']) == False:\n",
    "            reviewer_interests = df_reviewers.iloc[j+1]['Domains/topic areas you are comfortable reviewing'].lower()\n",
    "            reviewer_interests.replace('/',' ')\n",
    "            reviewer_corpus = [preprocess_gensim(reviewer_interests)]\n",
    "            if bool(reviewer_corpus[0]) == True:\n",
    "        #print(reviewer_corpus)\n",
    "                sum_vector_text = np.zeros((300))\n",
    "                counter = 0\n",
    "                for i in range(len(reviewer_corpus[0])):\n",
    "                    if reviewer_corpus[0][i] in model.vocab:\n",
    "                        sum_vector_text = sum_vector_text + model.wv[reviewer_corpus[0][i]]\n",
    "                        counter = counter + 1\n",
    "                    else:\n",
    "                        print(reviewer_corpus[0][i])\n",
    "                if counter > 0:\n",
    "                    average_Reviewer_vector_text = sum_vector_text/ counter\n",
    "                    all_usernames.append(df_reviewers.username.iloc[j+1])\n",
    "                    all_domains.append(reviewer_interests)\n",
    "                    all_cosine_sims.append(cosine_similarity(np.array([paper_vec]), np.array([average_Reviewer_vector_text]))[0,0])\n",
    "    return np.array(all_usernames), np.array(all_domains), np.array(all_cosine_sims)\n",
    "\n",
    "\n",
    "#########################################################################################################################################################\n",
    "\n",
    "#########################################################################################################################################################\n",
    "\n",
    "################# Idea 3 TF Gensim ####################\n",
    "\n",
    "\n",
    "def Get_Top_Words_tf(Paper_interest, df_reviewers=df_reviewers,num_suggestions=5, num_top20=20):\n",
    "    POI_PDF = [extract_text(Paper_interest)]\n",
    "    text = str(POI_PDF)\n",
    "    words =  Get_Lemma_Words(POI_PDF)\n",
    "    #print(len(words))\n",
    "    fdist = FreqDist(words)\n",
    "    X = np.array(fdist.most_common())\n",
    "    top20_tf = X[:num_top20,0]\n",
    "\n",
    "    #df_reviewers.username.iloc[+1]\n",
    "\n",
    "    return top20_tf\n",
    "\n",
    "\n",
    "\n",
    "def Split_columns(t):\n",
    "  txt = \" \".join(\"\".join([\" \" if ch in string.punctuation else ch for ch in t]).split())\n",
    "  sol1 = np.char.split(txt, ' ')\n",
    "  txt_arr  = array_of_lists_to_array(sol1)\n",
    "  uniarr = np.unique(txt_arr)\n",
    "  return uniarr\n",
    "\n",
    "\n",
    "def summatation_bot(all_usernames, all_domains, all_num_matched_words, all_matched_words):\n",
    "  length = len(all_usernames)\n",
    "  message = 'Hello. \\nI have found ' + str(length) + ' possible reviewers for this paper.' +'\\n\\n'\n",
    "  for i in range(length):\n",
    "    ps = 'I believe ' + all_usernames[i] + ' will make a good reviewer for this paper because they have matched ' + str(int(all_num_matched_words[i])) +  ' words from their comfortable domain topics with the top 20 most frequent words in the paper. These matched words are ' + str(all_matched_words[i]) +'.\\nFrom their topics domain: ' + str(all_domains[i].replace('\\n', ', ')) +'.\\n'\n",
    "    message = message + ps + '\\n'\n",
    "  print(message)\n",
    "\n",
    "\n",
    "\n",
    "def W2V_Gensim_Processing_tf(top_arr, model, print_outside_corpus=True):\n",
    "    texts = ''\n",
    "    for i in range(len(top_arr)):\n",
    "        texts = texts + top_arr[i] + ' ' \n",
    "    #print(t)\n",
    "\n",
    "    #download('punkt') #tokenizer, run once\n",
    "    #download('stopwords') #stopwords dictionary, run once\n",
    "    stop_words = stopwords.words('english')\n",
    "\n",
    "    texts = [texts]\n",
    "    corpus = [preprocess_gensim(text) for text in texts]\n",
    "\n",
    "    sum_vector_text = np.zeros((300))\n",
    "    counter = 0\n",
    "    for i in range(len(corpus[0])):\n",
    "        if corpus[0][i] in model.vocab:\n",
    "            sum_vector_text = sum_vector_text + model.wv[corpus[0][i]]\n",
    "            counter = counter + 1\n",
    "        else:\n",
    "            if print_outside_corpus == True:\n",
    "                print(corpus[0][i])\n",
    "    average_vector_text = sum_vector_text/ counter\n",
    "    return average_vector_text\n",
    "\n",
    "    \n",
    "#########################################################################################################################################################\n",
    "\n",
    "#########################################################################################################################################################\n",
    "\n",
    "################# Idea 3 TF-IDF Gensim ####################\n",
    "\n",
    "def Get_Top_Words_tf_idf(Paper_interest, df=df, df_reviewers=df_reviewers, num_suggestions=5, num_top20=20):\n",
    "    POI_PDF = [extract_text(Paper_interest)]\n",
    "    text = str(POI_PDF)\n",
    "    words = Get_Lemma_Words(POI_PDF)\n",
    "    #print(len(words))\n",
    "    fdist = FreqDist(words)\n",
    "    X = np.array(fdist.most_common())\n",
    "    tf_idf_arr_names, tf_idf_arr_floats = determine_wiki_td_idf(X, df=df)\n",
    "    #print('determined wiki')\n",
    "    num_arr = np.array(tf_idf_arr_floats)\n",
    "    tf_idf_arr_names_arr = np.array(tf_idf_arr_names)\n",
    "    top20_tf_idf = tf_idf_arr_names_arr[np.argsort(num_arr)[:num_top20]]\n",
    "    return top20_tf_idf\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#########################################################################################################################################################\n",
    "\n",
    "#########################################################################################################################################################\n",
    "\n",
    "################# Idea 4 Gensim ####################\n",
    "\n",
    "def TopReviewers_W2V_Gensim(number, all_usernames, all_domains, all_cosine_sims):\n",
    "    message = 'Hello.\\n I have found ' +str(number) + ' possible reviewers for this paper.'+ '\\n\\n'\n",
    "    for J in range(number):\n",
    "        index = np.argsort(all_cosine_sims)[-1-J]\n",
    "        #print(index)\n",
    "        ps = 'I believe '+ colored(str(all_usernames[index]), 'green') + ' will be a good reviewer for this paper. Their domain interests and this paper have a cosine similairity score of ' + colored(str(all_cosine_sims[index])[:6], 'blue') + '. This reviewers domain interests are ' + colored(str(all_domains[index].replace('\\n', ',')), 'red')\n",
    "        message = message + ps + '\\n\\n'\n",
    "    print(message)    \n",
    "\n",
    "#########################################################################################################################################################\n",
    "\n",
    "#########################################################################################################################################################\n",
    "\n",
    "################# Idea 6 Gensim ####################\n",
    "\n",
    "def MakingPaperVector_Idea6_Gensim(arr, model, printer=True):\n",
    "    sum_vector_text = np.zeros((300))\n",
    "    count = 0\n",
    "    for J in range(len(arr)):\n",
    "        texts = arr[J]\n",
    "        texts = texts\n",
    "        texts = texts.replace('-\\\\n','')\n",
    "        texts = texts.replace('-\\n','')\n",
    "        texts = texts.replace('\\\\n',' ')\n",
    "        texts = texts.replace('\\n', ' ')\n",
    "    \n",
    "        texts = [texts]\n",
    "        corpus = [preprocess_gensim(text) for text in texts]\n",
    "    \n",
    "        sum_line_vector_text = np.zeros((300))\n",
    "        counter = 0\n",
    "        for i in range(len(corpus[0])):\n",
    "            if corpus[0][i] in model.vocab:\n",
    "                sum_vector_text = sum_vector_text + model.wv[corpus[0][i]]\n",
    "                counter = counter + 1\n",
    "            else:\n",
    "                if printer == True:\n",
    "                    print(corpus[0][i])\n",
    "        average_line_vector_text = sum_line_vector_text/ counter\n",
    "        sum_vector_text = sum_vector_text + average_line_vector_text\n",
    "        count = count + 1\n",
    "    average_vector_text = sum_vector_text / count\n",
    "    \n",
    "    return average_vector_text\n",
    "\n",
    "\n",
    "#########################################################################################################################################################\n",
    "\n",
    "#########################################################################################################################################################\n",
    "\n",
    "################# spaCy ####################\n",
    "\n",
    "def GetReviewer_Vectors_spacy(model,df_reviewers=df_reviewers):\n",
    "    reviewer_vectors = np.zeros(((df_reviewers.shape[0]-1),300))\n",
    "    for i in range(df_reviewers.shape[0]-1):\n",
    "        #if i%10 == 0:\n",
    "        #    print(i)\n",
    "        if pd.isna(df_reviewers['Domains/topic areas you are comfortable reviewing'].iloc[1:].values[i]) == False:\n",
    "            review_text = df_reviewers['Domains/topic areas you are comfortable reviewing'].iloc[1:].values[i].lower()\n",
    "            review_text = review_text.replace('-\\\\n','')\n",
    "            review_text = review_text.replace('\\\\n',' ')\n",
    "            review_text = review_text.replace('\\n', ' ')\n",
    "        \n",
    "            review_arr = []\n",
    "            for token in model(review_text):\n",
    "                if token.is_alpha == True:\n",
    "                    if token.is_stop == False:\n",
    "                        review_arr.append(str(token.lemma_).lower())\n",
    "            review_arr = np.array(review_arr)\n",
    "        \n",
    "            review_str = ''\n",
    "            for j in np.unique(review_arr):\n",
    "                review_str = review_str + j +' '\n",
    "        \n",
    "        #print(model(review_str).vector.shape)\n",
    "        #print(reviewer_vectors.shape)\n",
    "            reviewer_vectors[i] = model(review_str).vector\n",
    "    \n",
    "    return reviewer_vectors\n",
    "\n",
    "def GetCosineSims_spacy(doc_vec, review_vec, df_reviewers=df_reviewers):\n",
    "    all_usernames = []\n",
    "    all_domains = []\n",
    "    all_cosine_sims = []\n",
    "    for j in range(len(review_vec)):\n",
    "        if pd.isna(df_reviewers.iloc[j+1]['Domains/topic areas you are comfortable reviewing']) == False:\n",
    "            all_cosine_sims.append(cosine_similarity(np.array([doc_vec]), np.array([review_vec[j]]))[0,0])\n",
    "            all_domains.append(df_reviewers.iloc[j+1]['Domains/topic areas you are comfortable reviewing'].lower())\n",
    "            all_usernames.append(df_reviewers.iloc[j+1].username)\n",
    "    all_usernames= np.array(all_usernames)\n",
    "    all_domains= np.array(all_domains)\n",
    "    all_cosine_sims= np.array(all_cosine_sims)\n",
    "    \n",
    "    return all_usernames, all_domains, all_cosine_sims\n",
    "\n",
    "#def TopReviewers_spacy(number=5, all_usernames=all_usernames, all_domains=all_domains, all_cosine_sims=all_cosine_sims):\n",
    "def TopReviewers_spacy(number, all_usernames, all_domains, all_cosine_sims):\n",
    "    message = 'Hello.\\n I have found ' +str(number) + ' possible reviewers for this paper.'+ '\\n\\n'\n",
    "    for J in range(number):\n",
    "        index = np.argsort(all_cosine_sims)[-1-J]\n",
    "        #print(index)\n",
    "        ps = 'I believe '+ colored(str(all_usernames[index]), 'green') + ' will be a good reviewer for this paper. Their domain interests and this paper have a cosine similairity score of ' + colored(str(all_cosine_sims[index])[:6], 'blue') + '. This reviewers domain interests are ' + colored(str(all_domains[index].replace('\\n', ',')), 'red')\n",
    "        message = message + ps + '\\n\\n'\n",
    "    print(message)        \n",
    "    \n",
    "    \n",
    "def W2V_spaCy_Processing(texts, green_text=True, lemma=True, unique=True):\n",
    "    texts = texts.replace('-\\\\n','')\n",
    "    texts = texts.replace('-\\n','')\n",
    "    texts = texts.replace('\\\\n',' ')\n",
    "    texts = texts.replace('\\n', ' ')\n",
    "    \n",
    "    model = spacy.load('en_core_web_lg')\n",
    "    reviewer_vectors = GetReviewer_Vectors_spacy(model)\n",
    "    \n",
    "    # Only Green Text: \n",
    "    if green_text == True:\n",
    "        print('### Only Green Text - No Other Preprocessing ###')\n",
    "        all_usernames, all_domains, all_cosine_sims = GetCosineSims_spacy(model(texts).vector, reviewer_vectors)\n",
    "        TopReviewers_spacy(5, all_usernames, all_domains, all_cosine_sims)\n",
    "        \n",
    "    doc = model(texts)\n",
    "    doc_arr = []\n",
    "    for token in doc:\n",
    "        if token.is_alpha == True:\n",
    "            if token.is_stop == False:\n",
    "                doc_arr.append(str(token.lemma_).lower())\n",
    "    doc_arr = np.array(doc_arr)\n",
    "    \n",
    "    doc_arr_mod = ''\n",
    "    for i in doc_arr:\n",
    "        doc_arr_mod = doc_arr_mod + i +' '\n",
    "    \n",
    "    # Green Text and Lemmasation\n",
    "    if lemma == True:\n",
    "        print('### Green Text & Lemmasation ###')\n",
    "        all_usernames2, all_domains2, all_cosine_sims2 = GetCosineSims_spacy(model(doc_arr_mod).vector, reviewer_vectors)\n",
    "        TopReviewers_spacy(5, all_usernames2, all_domains2, all_cosine_sims2)\n",
    "        \n",
    "    if unique == True:\n",
    "        print('### Green Text & Lemmasation & Unique Words ###')\n",
    "        doc_str = ''\n",
    "        for i in np.unique(doc_arr):\n",
    "            doc_str = doc_str + i +' '\n",
    "        all_usernames3, all_domains3, all_cosine_sims3 = GetCosineSims_spacy(model(doc_str).vector, reviewer_vectors)\n",
    "        TopReviewers_spacy(5,all_usernames3, all_domains3, all_cosine_sims3)\n",
    "        \n",
    "def GetReviewer_Vectors(df_reviewers=df_reviewers):\n",
    "    reviewer_vectors = np.zeros(((df_reviewers.shape[0]-1),300))\n",
    "    for i in range(df_reviewers.shape[0]-1):\n",
    "        #if i%10 == 0:\n",
    "        #    print(i)\n",
    "        if pd.isna(df_reviewers['Domains/topic areas you are comfortable reviewing'].iloc[1:].values[i]) == False:\n",
    "            review_text = df_reviewers['Domains/topic areas you are comfortable reviewing'].iloc[1:].values[i].lower()\n",
    "            review_text = review_text.replace('-\\\\n','')\n",
    "            review_text = review_text.replace('\\\\n',' ')\n",
    "            review_text = review_text.replace('\\n', ' ')\n",
    "        \n",
    "            review_arr = []\n",
    "            for token in model(review_text):\n",
    "                if token.is_alpha == True:\n",
    "                    if token.is_stop == False:\n",
    "                        review_arr.append(str(token.lemma_).lower())\n",
    "            review_arr = np.array(review_arr)\n",
    "        \n",
    "            review_str = ''\n",
    "            for j in np.unique(review_arr):\n",
    "                review_str = review_str + j +' '\n",
    "        \n",
    "        #print(model(review_str).vector.shape)\n",
    "        #print(reviewer_vectors.shape)\n",
    "            reviewer_vectors[i] = model(review_str).vector\n",
    "    \n",
    "    return reviewer_vectors\n",
    "\n",
    "def GetCosineSims(doc_vec, review_vec, df_reviewers=df_reviewers):\n",
    "    all_usernames = []\n",
    "    all_domains = []\n",
    "    all_cosine_sims = []\n",
    "    for j in range(len(review_vec)):\n",
    "        if pd.isna(df_reviewers.iloc[j+1]['Domains/topic areas you are comfortable reviewing']) == False:\n",
    "            all_cosine_sims.append(cosine_similarity(np.array([doc_vec]), np.array([review_vec[j]]))[0,0])\n",
    "            all_domains.append(df_reviewers.iloc[j+1]['Domains/topic areas you are comfortable reviewing'].lower())\n",
    "            all_usernames.append(df_reviewers.iloc[j+1].username)\n",
    "    all_usernames= np.array(all_usernames)\n",
    "    all_domains= np.array(all_domains)\n",
    "    all_cosine_sims= np.array(all_cosine_sims)\n",
    "    \n",
    "    return all_usernames, all_domains, all_cosine_sims\n",
    "\n",
    "def Get_Paper_Vector_Idea6_SpaCy(arr,model):\n",
    "    sum_vector_text = np.zeros((300))\n",
    "    count = 0\n",
    "    for J in range(len(arr)):\n",
    "        sum_line_vector_text = model(str(arr[J])).vector\n",
    "        counter = len(arr[J].split())\n",
    "        \n",
    "        average_line_vector_text = sum_line_vector_text/ counter\n",
    "        sum_vector_text = sum_vector_text + average_line_vector_text\n",
    "        count = count + 1\n",
    "        \n",
    "    average_vector_text = sum_vector_text / count\n",
    "    return average_vector_text\n",
    "\n",
    "\n",
    "#########################################################################################################################################################\n",
    "\n",
    "#########################################################################################################################################################\n",
    "\n",
    "################# Idea 3 TF spaCy ####################\n",
    "    \n",
    "\n",
    "\n",
    "#########################################################################################################################################################\n",
    "\n",
    "#########################################################################################################################################################\n",
    "\n",
    "################# Idea 3 TF-IDF spaCy ####################\n",
    "\n",
    "\n",
    "\n",
    "#########################################################################################################################################################\n",
    "\n",
    "#########################################################################################################################################################\n",
    "\n",
    "################# Idea 4 spaCy ####################\n",
    "\n",
    "# No Functions\n",
    "\n",
    "#########################################################################################################################################################\n",
    "\n",
    "#########################################################################################################################################################\n",
    "\n",
    "################# Idea 6 spaCy ####################\n",
    "\n",
    "\n",
    "    \n",
    "def Idea6_lemma_Text(arr):\n",
    "    text_list = []\n",
    "    for J in range(len(arr)):\n",
    "        doc = model(str(arr[J]))\n",
    "        \n",
    "        doc_arr = []\n",
    "        for token in doc:\n",
    "            if token.is_alpha == True:\n",
    "                if token.is_stop == False:\n",
    "                    doc_arr.append(str(token.lemma_).lower())\n",
    "        doc_arr = np.array(doc_arr)\n",
    "        \n",
    "        doc_arr_mod = ''\n",
    "        for i in doc_arr:\n",
    "            doc_arr_mod = doc_arr_mod + i +' '\n",
    "        #print(doc_arr_mod)\n",
    "        text_list.append(doc_arr_mod)\n",
    "    return text_list\n",
    "\n",
    "def Idea6_unique_lemma_Text(arr):\n",
    "    text_list = []\n",
    "    for J in range(len(arr)):\n",
    "        doc = model(str(arr[J]))\n",
    "        \n",
    "        doc_arr = []\n",
    "        for token in doc:\n",
    "            if token.is_alpha == True:\n",
    "                if token.is_stop == False:\n",
    "                    doc_arr.append(str(token.lemma_).lower())\n",
    "        doc_arr = np.array(doc_arr)\n",
    "        doc_str = ''\n",
    "        for i in np.unique(doc_arr):\n",
    "            doc_str = doc_str + i +' '\n",
    "        text_list.append(doc_str)\n",
    "    return text_list\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#########################################################################################################################################################\n",
    "\n",
    "#########################################################################################################################################################\n",
    "\n",
    "################# Idea 9 Sense2Vec ####################\n",
    "\n",
    "\n",
    "def GetPaperVector_Sense2Vec(text):\n",
    "    doc = model(text)\n",
    "    doc_arr = []\n",
    "    pos_arr = []\n",
    "    tag_arr = []\n",
    "    dep_arr = []\n",
    "    for token in doc:\n",
    "        if token.is_alpha == True:\n",
    "            if token.is_stop == False:\n",
    "                doc_arr.append(str(token.lemma_).lower())\n",
    "                pos_arr.append(str(token.pos_))\n",
    "                tag_arr.append(str(token.tag_))\n",
    "                dep_arr.append(str(token.dep_))\n",
    "            \n",
    "    doc_arr = np.array(doc_arr)\n",
    "    pos_arr = np.array(pos_arr)\n",
    "    tag_arr = np.array(tag_arr)\n",
    "    dep_arr = np.array(dep_arr)\n",
    "\n",
    "    word_vec = np.zeros((128))\n",
    "    counter = 0\n",
    "    for P in range(len(doc_arr)):\n",
    "        best = s2v.get_best_sense(doc_arr[P])\n",
    "        if best != None:\n",
    "            vector = s2v[best]\n",
    "            word_vec = word_vec + vector\n",
    "            counter = counter + 1\n",
    "    average_word_vec = word_vec / counter\n",
    "    \n",
    "    return average_word_vec\n",
    "\n",
    "\n",
    "\n",
    "def GetReviewerSample_Sense2Vec(paper_vec, df_reviewers=df_reviewers):\n",
    "    all_usernames = []\n",
    "    all_domains = []\n",
    "    all_cosine_sims = []\n",
    "    for j in range(df_reviewers.shape[0]-1):\n",
    "        if pd.isna(df_reviewers.iloc[j+1]['Domains/topic areas you are comfortable reviewing']) == False:\n",
    "            reviewer_interests = df_reviewers.iloc[j+1]['Domains/topic areas you are comfortable reviewing'].lower()\n",
    "            reviewer_interests.replace('/',' ')\n",
    "            doc = model(reviewer_interests)\n",
    "            reviewer_arr = []\n",
    "            for token in doc:\n",
    "                if token.is_alpha == True:\n",
    "                    if token.is_stop == False:\n",
    "                        reviewer_arr.append(str(token.lemma_).lower())\n",
    "            reviewer_arr = np.array(reviewer_arr)\n",
    "            word_vec = np.zeros((128))\n",
    "            counter = 0\n",
    "            for P in range(len(reviewer_arr)):\n",
    "                best = s2v.get_best_sense(reviewer_arr[P])\n",
    "                if best != None:\n",
    "                    vector = s2v[best]\n",
    "                    word_vec = word_vec + vector\n",
    "                    counter = counter + 1\n",
    "            \n",
    "            if counter > 0:\n",
    "                average_reviewer_vec = word_vec / counter\n",
    "          \n",
    "                all_usernames.append(df_reviewers.username.iloc[j+1])\n",
    "                all_domains.append(reviewer_interests)\n",
    "                all_cosine_sims.append(cosine_similarity(np.array([paper_vec]), np.array([average_reviewer_vec]))[0,0])\n",
    "    return np.array(all_usernames), np.array(all_domains), np.array(all_cosine_sims)\n",
    " \n",
    "def TopReviewers(number, all_usernames, all_domains, all_cosine_sims):\n",
    "    message = 'Hello.\\nI have found ' +str(number) + ' possible reviewers for this paper.'+ '\\n\\n'\n",
    "    for J in range(number):\n",
    "        index = np.argsort(all_cosine_sims)[-1-J]\n",
    "        #print(index)\n",
    "        ps = 'I believe '+ colored(str(all_usernames[index]), 'green') + ' will be a good reviewer for this paper. Their domain interests and this paper have a cosine similairity score of ' + colored(str(all_cosine_sims[index])[:6], 'blue') + '. This reviewers domain interests are ' + colored(str(all_domains[index].replace('\\n', ',')), 'red')\n",
    "        message = message + ps + '\\n\\n'\n",
    "    print(message) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3a48ce4-2ffb-4483-91ed-3572b06a182d",
   "metadata": {},
   "source": [
    "# Idea 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "b2802726-c921-4979-a667-0abc6ad24a50",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "all_usernames_Idea_2, all_domains_Idea_2, all_num_matched_words_Idea_2, all_matched_words_Idea_2 = Get_reviewer_sample_tf_idf(Paper_interest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "8e45fc8f-bf32-499d-8e21-d01fd91289a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello. \n",
      "I have found 5 possible reviewers for this paper.\n",
      "\n",
      "I believe \u001b[32msbacchio\u001b[0m will make a good reviewer for this paper because they have matched \u001b[34m2\u001b[0m words from their comfortable domain topics with the top 20 most frequent words in the paper. These matched words are \u001b[34m['computational', 'iterative']\u001b[0m.\n",
      "From their topics domain: \u001b[31mcomputational physics,  high-performance computing,  lattice qcd,  quantum field theory,  iterative linear solvers\u001b[0m.\n",
      "\n",
      "I believe \u001b[32mSaumikDana\u001b[0m will make a good reviewer for this paper because they have matched \u001b[34m2\u001b[0m words from their comfortable domain topics with the top 20 most frequent words in the paper. These matched words are \u001b[34m['computational', 'python']\u001b[0m.\n",
      "From their topics domain: \u001b[31mgeology, geophysics, earth surface dynamics, hydrology, landslides, natural hazards, terrain analysis ,  computational science and engineering, geophysics, high-performance computing ,  computational fracture mechanics, applied mathematics, c++, asynchronous and task-based programming ,  mechanics, computational science, biomechanics, computational oncology, applied mathematics ,  applied math, numerical optimization, numerical linear algebra, fortran, python, matlab, julia, teaching\u001b[0m.\n",
      "\n",
      "I believe \u001b[32mtacaswell\u001b[0m will make a good reviewer for this paper because they have matched \u001b[34m1\u001b[0m words from their comfortable domain topics with the top 20 most frequent words in the paper. These matched words are \u001b[34m['python']\u001b[0m.\n",
      "From their topics domain: \u001b[31mplotting in python, soft matter physics, xray science\u001b[0m.\n",
      "\n",
      "I believe \u001b[32mma-sadeghi\u001b[0m will make a good reviewer for this paper because they have matched \u001b[34m1\u001b[0m words from their comfortable domain topics with the top 20 most frequent words in the paper. These matched words are \u001b[34m['computational']\u001b[0m.\n",
      "From their topics domain: \u001b[31mporous materials,  high performance computing,  computational fluid dynamics,  transport phenomena,  electrochemistry\u001b[0m.\n",
      "\n",
      "I believe \u001b[32marunmano121\u001b[0m will make a good reviewer for this paper because they have matched \u001b[34m1\u001b[0m words from their comfortable domain topics with the top 20 most frequent words in the paper. These matched words are \u001b[34m['python']\u001b[0m.\n",
      "From their topics domain: \u001b[31mdata analytics,  artificial intelligence,  big data,  statistical modeling,  analytics,  regression,  machine learning,  deep learning,  neural networks,  gradient boosting,  nondestructive testing,  structural health monitoring,  python\u001b[0m.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "summatation_bot_tf_idf(all_usernames_Idea_2, all_domains_Idea_2, all_num_matched_words_Idea_2, all_matched_words_Idea_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2500f122-41d9-4ec6-8156-a279e8454d5c",
   "metadata": {},
   "source": [
    "# spaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "4d1b96c2-31c0-4e30-8e96-2e4b2dba008f",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "972af138-ac82-4b90-a794-5c17869c656a",
   "metadata": {},
   "source": [
    "# Idea 3 TF (spaCy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "07cfdf15-3f8c-47f6-b641-8a47dbbd930c",
   "metadata": {},
   "outputs": [],
   "source": [
    "top20_tf = Get_Top_Words_tf(Paper_interest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "34e0caba-c2f5-48a2-ad98-1b24f99360b3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['cardiac', 'problem', 'fenics', 'journal', 'et', 'package',\n",
       "       'based', 'solving', 'boundary', 'python', 'pulse', 'code',\n",
       "       'custom', 'material', 'using', 'computational', 'mechanic',\n",
       "       'appropriate', 'user', 'need'], dtype='<U21')"
      ]
     },
     "execution_count": 152,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top20_tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "2659297c-c2d0-4ca7-9f4f-1ea4ba3da83f",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviewer_vectors = GetReviewer_Vectors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "id": "fc3bd34f-618c-456d-8f6b-eddbf2da41c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_top20= ''\n",
    "for i in top20_tf:\n",
    "    doc_top20 = doc_top20 + i +' '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "5cf9b26c-109d-4684-abbe-88bf967b0638",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_usernames_spacy_Idea_3_TF, all_domains_spacy_Idea_3_TF, all_cosine_sims_spacy_Idea_3_TF = GetCosineSims(model(doc_top20).vector, reviewer_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "608ef328-2f6c-4f07-aa8b-a16ae36c1e32",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello.\n",
      "I have found 5 possible reviewers for this paper.\n",
      "\n",
      "I believe \u001b[32mmichaelberks\u001b[0m will be a good reviewer for this paper. Their domain interests and this paper have a cosine similairity score of \u001b[34m0.8870\u001b[0m. This reviewers domain interests are \u001b[31mmedical imaging, image processing, compute vision, machine learning/ai applied to images, general scientific/numerical data processing (unless requiring specialist domain specific knowledge outside of the areas listed above)\u001b[0m\n",
      "\n",
      "I believe \u001b[32maaronpeikert\u001b[0m will be a good reviewer for this paper. Their domain interests and this paper have a cosine similairity score of \u001b[34m0.8647\u001b[0m. This reviewers domain interests are \u001b[31mmachine learning, - model selection, - structural equation modelling, - multimodel inference, - hierarchical data, , open science, - reproducibility, - dynamic document generation, - containers, - version control\u001b[0m\n",
      "\n",
      "I believe \u001b[32mjarrah42\u001b[0m will be a good reviewer for this paper. Their domain interests and this paper have a cosine similairity score of \u001b[34m0.8605\u001b[0m. This reviewers domain interests are \u001b[31mhigh performance/scientific computing, operating systems, distributed systems, parallel programming, software engineering, user interfaces, debugging, performance analysis, development tools, security, web development, internet of things\u001b[0m\n",
      "\n",
      "I believe \u001b[32mzeroset\u001b[0m will be a good reviewer for this paper. Their domain interests and this paper have a cosine similairity score of \u001b[34m0.8583\u001b[0m. This reviewers domain interests are \u001b[31mscientific computing, simulation software, high performance computing, parallel computing, template meta programming, multi agent simulation, research software\u001b[0m\n",
      "\n",
      "I believe \u001b[32malexpghayes\u001b[0m will be a good reviewer for this paper. Their domain interests and this paper have a cosine similairity score of \u001b[34m0.8561\u001b[0m. This reviewers domain interests are \u001b[31mi'm interested in making sure that r packages for modelling have useful and intuitive interfaces and documentation. i'm not interested in double checking theory and correctness, but making sure that a new user can quickly and easily get the results they want.\u001b[0m\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "TopReviewers(5, all_usernames_spacy_Idea_3_TF, all_domains_spacy_Idea_3_TF, all_cosine_sims_spacy_Idea_3_TF)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c655080-9d25-42c1-a4f5-e068e362ad9e",
   "metadata": {},
   "source": [
    "# Idea 9 (Sense2Vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "cd8a050e-f0a7-4364-8d12-6c28dba86a57",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sense2vec import Sense2Vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "id": "7f4d441c-c475-488a-ad58-eb480f53a2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "s2v = Sense2Vec().from_disk(\"../s2v_old\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "id": "887c8790-62a0-4269-bcd6-72e0aa8a6902",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "id": "62e7997c-c7d6-4d3f-8697-20ce5713f83b",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts, arr = MakeGreenRedText(Paper_interest,False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "id": "f25c36dd-7952-4910-a69e-cc9eb1a1907a",
   "metadata": {},
   "outputs": [],
   "source": [
    "average_word_vec = GetPaperVector_Sense2Vec(texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "id": "d9769905-9376-449f-b0d4-dfeb57af91ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_usernames_sense2vec, all_domains_sense2vec, all_cosine_sims_sense2vec = GetReviewerSample_Sense2Vec(average_word_vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "da49a740-7fa7-42a7-9d6d-98ba52d9d226",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello.\n",
      "I have found 5 possible reviewers for this paper.\n",
      "\n",
      "I believe \u001b[32maaronpeikert\u001b[0m will be a good reviewer for this paper. Their domain interests and this paper have a cosine similairity score of \u001b[34m0.9277\u001b[0m. This reviewers domain interests are \u001b[31mmachine learning, - model selection, - structural equation modelling, - multimodel inference, - hierarchical data, , open science, - reproducibility, - dynamic document generation, - containers, - version control\u001b[0m\n",
      "\n",
      "I believe \u001b[32mtorressa\u001b[0m will be a good reviewer for this paper. Their domain interests and this paper have a cosine similairity score of \u001b[34m0.9271\u001b[0m. This reviewers domain interests are \u001b[31moptimization: modelling (lps, milps, mmilps), solution approaches (heuristics, hyper-heuristics, column generation), applications (aviation, vehicle routing, scheduling)\u001b[0m\n",
      "\n",
      "I believe \u001b[32malexpghayes\u001b[0m will be a good reviewer for this paper. Their domain interests and this paper have a cosine similairity score of \u001b[34m0.9260\u001b[0m. This reviewers domain interests are \u001b[31mi'm interested in making sure that r packages for modelling have useful and intuitive interfaces and documentation. i'm not interested in double checking theory and correctness, but making sure that a new user can quickly and easily get the results they want.\u001b[0m\n",
      "\n",
      "I believe \u001b[32mmichaelberks\u001b[0m will be a good reviewer for this paper. Their domain interests and this paper have a cosine similairity score of \u001b[34m0.9254\u001b[0m. This reviewers domain interests are \u001b[31mmedical imaging, image processing, compute vision, machine learning/ai applied to images, general scientific/numerical data processing (unless requiring specialist domain specific knowledge outside of the areas listed above)\u001b[0m\n",
      "\n",
      "I believe \u001b[32mTomGoffrey\u001b[0m will be a good reviewer for this paper. Their domain interests and this paper have a cosine similairity score of \u001b[34m0.9181\u001b[0m. This reviewers domain interests are \u001b[31mastrophysics (my main area of expertise is in stellar physics, but i also have some background in planetary physics), plasma physics, hydrodynamics (shock capturing and modelling in particular), magnetohydrodynamics (i have worked a lot on non-ideal mhd in particular), radiation transport, (large )linear systems, time-integration methods, finite volume methods, arbitrary/lagrangian meshes, multi-material flow.\u001b[0m\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "TopReviewers(5, all_usernames_sense2vec, all_domains_sense2vec, all_cosine_sims_sense2vec )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "445089dc-75dc-4996-a43b-ab75c52b9738",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "963b9bdb-5dda-484e-bf50-85701ee3248b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
