{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9ab6d0d6-578d-466d-a04c-99185f7ed880",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim import utils\n",
    "import numpy as np\n",
    "import sys\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from nltk import word_tokenize\n",
    "from nltk import download\n",
    "from nltk.corpus import stopwords\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bfb96164-72a7-47d2-b51c-04d0c89e1189",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/sunilmcesh/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/sunilmcesh/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/sunilmcesh/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import glob\n",
    "import string\n",
    "import glob\n",
    "from tqdm import tqdm \n",
    "#import pdfminer\n",
    "from pdfminer.high_level import extract_text\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk \n",
    "nltk.download('stopwords')\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk import sent_tokenize\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "from nltk.probability import FreqDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d77d550-c6df-478a-a436-9f7c7ff9a4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "PAPER_OF_INTEREST_FNAME  = glob.glob('/Volumes/Seagate Backup Plus Drive/JOSS Project/joss-papers-master/*/*/*.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b16ff6e-32a5-44ca-b766-ef195a0248d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Volumes/Seagate Backup Plus Drive/JOSS Project/joss-papers-master/joss-papers-master/joss.00011/10.21105.joss.00011.pdf\n"
     ]
    }
   ],
   "source": [
    "K = 0\n",
    "print(PAPER_OF_INTEREST_FNAME[K])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "652a88a1-6165-4bdc-a101-6172352bea46",
   "metadata": {},
   "outputs": [],
   "source": [
    "Paper_interest = PAPER_OF_INTEREST_FNAME[K]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ccdb9a04-ab22-443a-afc9-a5c2065d637b",
   "metadata": {},
   "outputs": [],
   "source": [
    "POI_PDF = [extract_text(Paper_interest)]\n",
    "text = str(POI_PDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a87f42ec-5a7f-4016-9b04-974ef54c0ad8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"['carl: a likelihood-free inference toolbox\\\\n\\\\nGilles Louppe1, Kyle Cranmer1, and Juan Pavez2\\\\n\\\\nDOI: 10.21105/joss.00011\\\\n\\\\n1 New York University 2 Federico Santa María University\\\\n\\\\nSummary\\\\n\\\\nSoftware\\\\n\\\\n• Review\\\\n• Repository\\\\n• Archive\\\\n\\\\nLicence\\\\nAuthors of JOSS papers retain\\\\ncopyright and release the work un-\\\\nder a Creative Commons Attri-\\\\nbution 4.0 International License\\\\n(CC-BY).\\\\n\\\\nCarl is a toolbox for likelihood-free inference in Python.\\\\n\\\\nThe likelihood function is the central object that summarizes the information from an\\\\nexperiment needed for inference of model parameters. It is key to many areas of science\\\\nthat report the results of classical hypothesis tests or confidence intervals using the (gen-\\\\neralized or profile) likelihood ratio as a test statistic. At the same time, with the advance\\\\nof computing technology, it has become increasingly common that a simulator (or gener-\\\\native model) is used to describe complex processes that tie parameters of an underlying\\\\ntheory and measurement apparatus to high-dimensional observations. However, directly\\\\nevaluating the likelihood function in these cases is often impossible or is computationally\\\\nimpractical.\\\\n\\\\nIn this context, the goal of this package is to provide tools for the likelihood-free setup,\\\\nincluding likelihood (or density) ratio estimation algorithms, along with helpers to carry\\\\nout inference on top of these.\\\\n\\\\nApproximating likelihood ratios with calibrated classifiers\\\\n\\\\nMethodological details regarding likelihood-free inference with calibrated classifiers can\\\\nbe found in the companion paper (Cranmer, Pavez, and Louppe 2016).\\\\n\\\\nFuture development aims at providing further density ratio estimation algorithms, along\\\\nwith alternative algorithms for the likelihood-free setup, such as Approximate Bayesian\\\\nComputation (ABC).\\\\n\\\\nFuture works\\\\n\\\\nReferences\\\\n\\\\nCranmer, Kyle, Juan Pavez, and Gilles Louppe. 2016. “Approximating Likelihood Ratios\\\\nwith Calibrated Discriminative Classifiers,” March. http://arxiv.org/abs/1506.02169v2.\\\\n\\\\nLouppe et al., (2016). carl: a likelihood-free inference toolbox. Journal of Open Source Software, 1(1), 11, doi:10.21105/joss.00011\\\\n\\\\n1\\\\n\\\\n\\\\x0c']\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1a121e5-63e8-4535-b922-248f6cf7cbe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = text.replace('-\\\\n','')\n",
    "text = text.replace('-\\n','')\n",
    "text = text.replace('\\\\n',' ')\n",
    "text = text.replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b41222b9-e1b6-4870-a87e-f567000b4302",
   "metadata": {},
   "outputs": [],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "16c8325b-4c98-40af-aefb-ebbb77ce72b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = gensim.models.KeyedVectors.load_word2vec_format('/Volumes/Seagate Backup Plus Drive/JOSS Project/GoogleNews-vectors-negative300.bin', binary=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c1d743e1-7362-4598-b8e1-4f3ce64477ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/sunilmcesh/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/sunilmcesh/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "download('punkt') #tokenizer, run once\n",
    "download('stopwords') #stopwords dictionary, run once\n",
    "stop_words = stopwords.words('english')\n",
    "\n",
    "def preprocess(text):\n",
    "    text = text.lower()\n",
    "    doc = word_tokenize(text)\n",
    "    doc = [word for word in doc if word not in stop_words]\n",
    "    doc = [word for word in doc if word.isalpha()] #restricts string to alphabetic characters only\n",
    "    return doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e54dd617-5377-4912-ac66-8bf40a5fc03a",
   "metadata": {},
   "outputs": [],
   "source": [
    "texts = (POI_PDF)\n",
    "\n",
    "corpus = [preprocess(text) for text in texts]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9359032d-9993-4117-978a-bd44f0de01a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'carl'"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "579e750c-570d-47cd-ba28-bf18b212466a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_docs(corpus, texts,  condition_on_doc):\n",
    "    \"\"\"\n",
    "    Filter corpus, texts and labels given the function condition_on_doc which takes\n",
    "    a doc.\n",
    "    The document doc is kept if condition_on_doc(doc) is true.\n",
    "    \"\"\"\n",
    "    number_of_docs = len(corpus)\n",
    "\n",
    "    if texts is not None:\n",
    "        texts = [text for (text, doc) in zip(texts, corpus)\n",
    "                 if condition_on_doc(doc)]\n",
    "\n",
    "    #labels = [i for (i, doc) in zip(labels, corpus) if condition_on_doc(doc)]\n",
    "    corpus = [doc for doc in corpus if condition_on_doc(doc)]\n",
    "\n",
    "    print(\"{} docs removed\".format(number_of_docs - len(corpus)))\n",
    "\n",
    "    return (corpus, texts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "711f0cae-6493-492f-8951-d220174c0af9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 docs removed\n"
     ]
    }
   ],
   "source": [
    "corpus, texts = filter_docs(corpus, texts,  lambda doc: (len(doc) != 0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dcf77b22-8f4b-437d-ae74-480f60122b37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def document_vector(word2vec_model, doc):\n",
    "    # remove out-of-vocabulary words\n",
    "    doc = [word for word in doc if word in word2vec_model.vocab]\n",
    "    return np.mean(word2vec_model[doc], axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4c90404f-35f4-4d51-9d76-1ec7d8c82fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_vector_representation(word2vec_model, doc):\n",
    "    \"\"\"check if at least one word of the document is in the\n",
    "    word2vec dictionary\"\"\"\n",
    "    return not all(word not in word2vec_model.vocab for word in doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bbad1356-b19a-4d19-b890-c0237a535196",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 docs removed\n"
     ]
    }
   ],
   "source": [
    "corpus, texts = filter_docs(corpus, texts,  lambda doc: has_vector_representation(model, doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "caef99bf-be23-4320-9804-fa8b58404b25",
   "metadata": {},
   "outputs": [],
   "source": [
    "x =[]\n",
    "for doc in corpus[0]: #look up each doc in model\n",
    "    x.append(document_vector(model, doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "53df68e9-cbdd-4e61-8835-facf832f48f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = np.array(x) #list to array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "1425b785-e7d4-44da-aece-6a38e48a0a7f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'software'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "90f37441-4695-445f-8a8a-30632e5989b6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['abc', 'advance', 'aims', 'algorithms', 'along', 'alternative',\n",
       "       'apparatus', 'approximate', 'approximating', 'archive', 'areas',\n",
       "       'ative', 'authors', 'bayesian', 'become', 'bution', 'calibrated',\n",
       "       'carl', 'carry', 'cases', 'central', 'classical', 'classifiers',\n",
       "       'common', 'commons', 'companion', 'complex', 'computation',\n",
       "       'computationally', 'computing', 'confidence', 'context',\n",
       "       'copyright', 'cranmer', 'creative', 'density', 'der', 'describe',\n",
       "       'details', 'development', 'directly', 'discriminative', 'doi',\n",
       "       'eralized', 'estimation', 'et', 'evaluating', 'experiment',\n",
       "       'federico', 'found', 'function', 'future', 'gilles', 'goal',\n",
       "       'helpers', 'however', 'http', 'hypothesis', 'impossible',\n",
       "       'impractical', 'including', 'increasingly', 'inference',\n",
       "       'information', 'international', 'intervals', 'joss', 'journal',\n",
       "       'juan', 'key', 'kyle', 'licence', 'license', 'likelihood',\n",
       "       'louppe', 'many', 'march', 'maría', 'measurement',\n",
       "       'methodological', 'model', 'needed', 'new', 'object',\n",
       "       'observations', 'often', 'open', 'package', 'paper', 'papers',\n",
       "       'parameters', 'pavez', 'processes', 'profile', 'provide',\n",
       "       'providing', 'python', 'ratio', 'ratios', 'references',\n",
       "       'regarding', 'release', 'report', 'repository', 'results',\n",
       "       'retain', 'review', 'santa', 'science', 'setup', 'simulator',\n",
       "       'software', 'source', 'statistic', 'summarizes', 'summary',\n",
       "       'technology', 'test', 'tests', 'theory', 'tie', 'time', 'toolbox',\n",
       "       'tools', 'top', 'underlying', 'university', 'used', 'using',\n",
       "       'work', 'works', 'york'], dtype='<U15')"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(np.array(corpus[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "958f8deb-7c57-40b8-af2a-0e3f02ff685b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x2 =[]\n",
    "for doc in np.unique(np.array(corpus[0])): #look up each doc in model\n",
    "    x2.append(document_vector(model, doc))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "687c849c-d455-455d-b261-74f959b159b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "X2 = np.array(x2) #list to array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "14978b5e-12ab-4972-868b-8945af45d92e",
   "metadata": {},
   "outputs": [],
   "source": [
    "average_word2vec_vector2 = X2.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3e7001d3-8aff-49a2-9f03-153e4878fb28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('h', 0.7580714225769043),\n",
       " ('o', 0.7527846097946167),\n",
       " ('r', 0.7336361408233643),\n",
       " ('f', 0.7306424379348755),\n",
       " ('b', 0.7227460145950317),\n",
       " ('ts', 0.7164600491523743),\n",
       " ('¬', 0.7156660556793213),\n",
       " ('ar', 0.7137598395347595),\n",
       " ('t', 0.6964871883392334),\n",
       " ('d', 0.6955010890960693)]"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.similar_by_vector(average_word2vec_vector2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "5b184c73-28bf-4c94-ac27-e4e446649fb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(132, 300)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "78779473-6ff6-4be9-a20f-807b6028f395",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(173, 300)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "4e0c6b0e-a149-41ac-8d43-7c38954f2c5c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(300,)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.mean(axis=0).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f20eab88-049e-4616-a4f1-643771224f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "average_word2vec_vector = X.mean(axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "7e6c8c44-c2ea-4fb7-b62a-6af1a6135947",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('h', 0.760456383228302),\n",
       " ('o', 0.7575953006744385),\n",
       " ('r', 0.7365941405296326),\n",
       " ('f', 0.7355577349662781),\n",
       " ('b', 0.7251432538032532),\n",
       " ('¬', 0.7178463339805603),\n",
       " ('ts', 0.7164381146430969),\n",
       " ('ar', 0.714815080165863),\n",
       " ('w', 0.6953476667404175),\n",
       " ('d', 0.6944249272346497)]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.similar_by_vector(average_word2vec_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "5022c29d-57e7-49bf-92e8-3d5a275bca1e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('h', 0.778066098690033),\n",
       " ('o', 0.7700537443161011),\n",
       " ('t', 0.7625486254692078),\n",
       " ('r', 0.7494496703147888),\n",
       " ('ts', 0.7428569197654724),\n",
       " ('u', 0.7180385589599609),\n",
       " ('¬', 0.7121788859367371),\n",
       " ('f', 0.7076941728591919),\n",
       " ('cks', 0.7059786319732666),\n",
       " ('ta', 0.698557436466217)]"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.similar_by_vector(X[20])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "614dbb07-4815-4c2f-a54c-9a7ae205a5f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('theJournal', 0.6163490414619446),\n",
       " ('theAmerican_Journal', 0.5941680669784546),\n",
       " ('Annals', 0.5925253629684448),\n",
       " ('Jounal', 0.5823094248771667),\n",
       " ('journal', 0.5741861462593079),\n",
       " ('journal_Annals', 0.5684102773666382),\n",
       " ('PATRICK_JACKSON', 0.5683403015136719),\n",
       " ('Physiology_Gastrointestinal', 0.5631081461906433),\n",
       " ('By_LULADEY_B.', 0.5580130219459534),\n",
       " ('Roentgenology_AJR', 0.557891845703125),\n",
       " ('Gazette', 0.5572773814201355),\n",
       " ('TADESSE', 0.5548046231269836),\n",
       " ('•_GANNETT_STAFF_WRITER', 0.5463052988052368),\n",
       " ('currier@sturgisjournal.com', 0.545045793056488),\n",
       " ('Herald', 0.5421846508979797),\n",
       " ('journal_Archives', 0.5398844480514526),\n",
       " ('journal_Cell_Metabolism', 0.5383172035217285),\n",
       " ('Bulletin', 0.5375578999519348),\n",
       " ('ESTEBAN_PARRA', 0.5362104177474976),\n",
       " ('Clinical_Nutrition_Vol', 0.5346276164054871)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar('Journal', topn=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12cde4d0-3175-4631-8345-a07fc3446de3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
