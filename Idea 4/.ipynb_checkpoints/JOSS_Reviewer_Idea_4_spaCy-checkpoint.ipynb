{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "29a608a1-b305-4fed-99f0-078e8c530f69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from nltk import word_tokenize\n",
    "from nltk import download\n",
    "from nltk.corpus import stopwords\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.decomposition import PCA\n",
    "%matplotlib inline\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0a1a3312-a56b-45f5-89cc-30b5ab8fd3f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/sunilmcesh/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/sunilmcesh/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/sunilmcesh/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import glob\n",
    "import string\n",
    "import glob\n",
    "from tqdm import tqdm \n",
    "#import pdfminer\n",
    "from pdfminer.high_level import extract_text\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk \n",
    "nltk.download('stopwords')\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk import sent_tokenize\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "from nltk.probability import FreqDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3359f72f-6319-41ba-a5a1-c4701b9a47ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reviewers = pd.read_csv('../Data/JOSS Table Test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4c7ac379-c706-481d-b16f-231cf7f95da7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetReviewer_Vectors(df_reviewers=df_reviewers):\n",
    "    reviewer_vectors = np.zeros(((df_reviewers.shape[0]-1),300))\n",
    "    for i in range(df_reviewers.shape[0]-1):\n",
    "        #if i%10 == 0:\n",
    "        #    print(i)\n",
    "        if pd.isna(df_reviewers['Domains/topic areas you are comfortable reviewing'].iloc[1:].values[i]) == False:\n",
    "            review_text = df_reviewers['Domains/topic areas you are comfortable reviewing'].iloc[1:].values[i].lower()\n",
    "            review_text = review_text.replace('-\\\\n','')\n",
    "            review_text = review_text.replace('\\\\n',' ')\n",
    "            review_text = review_text.replace('\\n', ' ')\n",
    "        \n",
    "            review_arr = []\n",
    "            for token in model(review_text):\n",
    "                if token.is_alpha == True:\n",
    "                    if token.is_stop == False:\n",
    "                        review_arr.append(str(token.lemma_).lower())\n",
    "            review_arr = np.array(review_arr)\n",
    "        \n",
    "            review_str = ''\n",
    "            for j in np.unique(review_arr):\n",
    "                review_str = review_str + j +' '\n",
    "        \n",
    "        #print(model(review_str).vector.shape)\n",
    "        #print(reviewer_vectors.shape)\n",
    "            reviewer_vectors[i] = model(review_str).vector\n",
    "    \n",
    "    return reviewer_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a5c6fd4f-f9de-46b6-953f-7c2dabcfbba3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetReviwerNames(doc_vec, review_vec, df_reviewers=df_reviewers):\n",
    "    reviewer_names = []\n",
    "    diff_arr = doc_vec - review_vec\n",
    "    diff_sum = np.abs(diff_arr.sum(axis=1))\n",
    "    top_10 = np.argsort(diff_sum)[:10]\n",
    "    for i in top_10:\n",
    "        reviewer_names.append(df_reviewers.iloc[i].username)\n",
    "    reviewer_names= np.array(reviewer_names)\n",
    "    \n",
    "    return reviewer_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d4b0bc45-8605-42a4-8111-3b1990c725bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "PAPER_OF_INTEREST_FNAME  = glob.glob('/Volumes/Seagate Backup Plus Drive/JOSS Project/joss-papers-master/*/*/*.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6baa25db-2632-4fb5-af7a-ca75ad581f2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Volumes/Seagate Backup Plus Drive/JOSS Project/joss-papers-master/joss-papers-master/joss.00011/10.21105.joss.00011.pdf\n"
     ]
    }
   ],
   "source": [
    "K = 0\n",
    "print(PAPER_OF_INTEREST_FNAME[K])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f8a547e0-1396-4ce9-9a39-fe55219628a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Paper_interest = PAPER_OF_INTEREST_FNAME[K]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "03545733-fec7-49c0-be2c-f8a5af9d3f2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "POI_PDF = [extract_text(Paper_interest)][0]\n",
    "text = str(POI_PDF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "46b547b3-75d2-4522-a4aa-d5901ee8f22c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'carl: a likelihood-free inference toolbox\\n\\nGilles Louppe1, Kyle Cranmer1, and Juan Pavez2\\n\\nDOI: 10.21105/joss.00011\\n\\n1 New York University 2 Federico Santa María University\\n\\nSummary\\n\\nSoftware\\n\\n• Review\\n• Repository\\n• Archive\\n\\nLicence\\nAuthors of JOSS papers retain\\ncopyright and release the work un-\\nder a Creative Commons Attri-\\nbution 4.0 International License\\n(CC-BY).\\n\\nCarl is a toolbox for likelihood-free inference in Python.\\n\\nThe likelihood function is the central object that summarizes the information from an\\nexperiment needed for inference of model parameters. It is key to many areas of science\\nthat report the results of classical hypothesis tests or confidence intervals using the (gen-\\neralized or profile) likelihood ratio as a test statistic. At the same time, with the advance\\nof computing technology, it has become increasingly common that a simulator (or gener-\\native model) is used to describe complex processes that tie parameters of an underlying\\ntheory and measurement apparatus to high-dimensional observations. However, directly\\nevaluating the likelihood function in these cases is often impossible or is computationally\\nimpractical.\\n\\nIn this context, the goal of this package is to provide tools for the likelihood-free setup,\\nincluding likelihood (or density) ratio estimation algorithms, along with helpers to carry\\nout inference on top of these.\\n\\nApproximating likelihood ratios with calibrated classifiers\\n\\nMethodological details regarding likelihood-free inference with calibrated classifiers can\\nbe found in the companion paper (Cranmer, Pavez, and Louppe 2016).\\n\\nFuture development aims at providing further density ratio estimation algorithms, along\\nwith alternative algorithms for the likelihood-free setup, such as Approximate Bayesian\\nComputation (ABC).\\n\\nFuture works\\n\\nReferences\\n\\nCranmer, Kyle, Juan Pavez, and Gilles Louppe. 2016. “Approximating Likelihood Ratios\\nwith Calibrated Discriminative Classifiers,” March. http://arxiv.org/abs/1506.02169v2.\\n\\nLouppe et al., (2016). carl: a likelihood-free inference toolbox. Journal of Open Source Software, 1(1), 11, doi:10.21105/joss.00011\\n\\n1\\n\\n\\x0c'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "de4a3fc2-f55b-410e-961f-cf33545e05d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "text = text.replace('-\\\\n','')\n",
    "text = text.replace('-\\n','')\n",
    "text = text.replace('\\\\n',' ')\n",
    "text = text.replace('\\n', ' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7c0f0adb-bc85-4e10-9f00-6b7c486bbf2a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'carl: a likelihood-free inference toolbox  Gilles Louppe1, Kyle Cranmer1, and Juan Pavez2  DOI: 10.21105/joss.00011  1 New York University 2 Federico Santa María University  Summary  Software  • Review • Repository • Archive  Licence Authors of JOSS papers retain copyright and release the work under a Creative Commons Attribution 4.0 International License (CC-BY).  Carl is a toolbox for likelihood-free inference in Python.  The likelihood function is the central object that summarizes the information from an experiment needed for inference of model parameters. It is key to many areas of science that report the results of classical hypothesis tests or confidence intervals using the (generalized or profile) likelihood ratio as a test statistic. At the same time, with the advance of computing technology, it has become increasingly common that a simulator (or generative model) is used to describe complex processes that tie parameters of an underlying theory and measurement apparatus to high-dimensional observations. However, directly evaluating the likelihood function in these cases is often impossible or is computationally impractical.  In this context, the goal of this package is to provide tools for the likelihood-free setup, including likelihood (or density) ratio estimation algorithms, along with helpers to carry out inference on top of these.  Approximating likelihood ratios with calibrated classifiers  Methodological details regarding likelihood-free inference with calibrated classifiers can be found in the companion paper (Cranmer, Pavez, and Louppe 2016).  Future development aims at providing further density ratio estimation algorithms, along with alternative algorithms for the likelihood-free setup, such as Approximate Bayesian Computation (ABC).  Future works  References  Cranmer, Kyle, Juan Pavez, and Gilles Louppe. 2016. “Approximating Likelihood Ratios with Calibrated Discriminative Classifiers,” March. http://arxiv.org/abs/1506.02169v2.  Louppe et al., (2016). carl: a likelihood-free inference toolbox. Journal of Open Source Software, 1(1), 11, doi:10.21105/joss.00011  1  \\x0c'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2c20a0cb-37b1-4d08-b4ce-8de7594b2f0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5b6b2a5-e4b2-43ca-81ec-263b5b38dbc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviewer_vectors = GetReviewer_Vectors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5239d98-5a78-4d0c-af36-f5d552f837d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc = model(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c0af5ef-e3e1-4ce7-afbc-87c87c3cb348",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = 112\n",
    "W = 111\n",
    "print(doc[Q])\n",
    "print(doc[W])\n",
    "doc[Q].similarity(doc[W])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0bf0855-c41c-4892-a0d0-6dd3b8fffa3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5dc8d02-4423-4fba-91a6-6dd64c2904f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_arr_Orignal = []\n",
    "for token in doc:\n",
    "    if token.is_alpha == True:\n",
    "            doc_arr_Orignal.append(str(token).lower())\n",
    "doc_arr_Orignal = np.array(doc_arr_Orignal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99bc1b95-bdf2-4db9-85dd-cb5a3c67f5b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "fdist = FreqDist(doc_arr_Orignal)\n",
    "print(fdist.most_common(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2689852-5830-4b62-8e5e-e4333aba6997",
   "metadata": {},
   "outputs": [],
   "source": [
    "fdist.plot(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d8dc58-4e62-4bfc-95fe-a4d022ecdefb",
   "metadata": {},
   "outputs": [],
   "source": [
    "GetReviwerNames(model(doc).vector, reviewer_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a62845ca-fde8-484b-a947-0c25c89f36de",
   "metadata": {},
   "outputs": [],
   "source": [
    "ms = model.vocab.vectors.most_similar(np.asarray([model(doc).vector]), n=25)\n",
    "[model.vocab.strings[w] for w in ms[0][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e316cbcb-0b1c-4e99-9d82-57b03a6da1ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_arr = []\n",
    "for token in doc:\n",
    "    if token.is_alpha == True:\n",
    "        if token.is_stop == False:\n",
    "            doc_arr.append(str(token.lemma_).lower())\n",
    "doc_arr = np.array(doc_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5772de6a-8611-4a77-8d7f-41800922937b",
   "metadata": {},
   "outputs": [],
   "source": [
    "fdist = FreqDist(doc_arr)\n",
    "print(fdist.most_common(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a407510a-b869-4aa8-ac59-34c7238ccafd",
   "metadata": {},
   "outputs": [],
   "source": [
    "fdist.plot(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95c49d39-817f-4ea0-af3f-7fb8e5962eeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_arr_mod = ''\n",
    "for i in doc_arr:\n",
    "    doc_arr_mod = doc_arr_mod + i +' '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae44a7a-33a1-43f0-a8e8-c08e43bf8e8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_arr_mod"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5322346c-7082-4fb5-8111-82f0cce8c245",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_arr_mod = model(doc_arr_mod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a89bb9-c326-47a7-9f58-37b0b89f056c",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = 112\n",
    "W = 111\n",
    "print(doc_arr_mod[Q])\n",
    "print(doc_arr_mod[W])\n",
    "doc_arr_mod[Q].similarity(doc_arr_mod[W])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff351110-e3ae-4ad9-842e-c395986d1867",
   "metadata": {},
   "outputs": [],
   "source": [
    "GetReviwerNames(model(doc_arr_mod).vector, reviewer_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ac5079d-7bc2-4dd2-94a1-4eb013ad4782",
   "metadata": {},
   "outputs": [],
   "source": [
    "ms = model.vocab.vectors.most_similar(np.asarray([model(doc_arr_mod).vector]), n=25)\n",
    "[model.vocab.strings[w] for w in ms[0][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7b407f1-55c9-4d4e-9184-d54a54bcf142",
   "metadata": {},
   "outputs": [],
   "source": [
    "np.unique(doc_arr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5f5e26-986a-4d70-8bd4-3baf48f88b06",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_str = ''\n",
    "for i in np.unique(doc_arr):\n",
    "    doc_str = doc_str + i +' '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca52e87f-5404-4630-89f7-cc47fc96c568",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12fdaf3a-4847-47f1-a347-8c37e266a323",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_unique = model(doc_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae56f9fe-16a4-4bac-bfc2-80242c74f16f",
   "metadata": {},
   "outputs": [],
   "source": [
    "fdist = FreqDist(doc_unique)\n",
    "print(fdist.most_common(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f08b4671-f91d-4ee0-9a59-16e401c4c452",
   "metadata": {},
   "outputs": [],
   "source": [
    "fdist.plot(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64637ef9-2b43-42d0-bf41-954121f23080",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q = 112\n",
    "W = 111\n",
    "print(doc_unique[Q])\n",
    "print(doc_unique[W])\n",
    "doc_unique[Q].similarity(doc_unique[W])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0552aaab-16ba-4e6e-8602-90a8df2b64db",
   "metadata": {},
   "outputs": [],
   "source": [
    "GetReviwerNames(model(doc_unique).vector, reviewer_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "390f3c4d-144e-467a-a1fd-6d25b4875b60",
   "metadata": {},
   "outputs": [],
   "source": [
    "ms = model.vocab.vectors.most_similar(np.asarray([model(doc_unique).vector]), n=25)\n",
    "[model.vocab.strings[w] for w in ms[0][0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "429b07f2-48e8-440e-a340-84a60f92fbed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "526a37d6-1057-466b-8059-df0fccb4ae99",
   "metadata": {},
   "outputs": [],
   "source": [
    "GetReviwerNames(model(doc_unique).vector, reviewer_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c0ae77f-1608-4985-8a10-9180cc99df97",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7a0a30c-e6fe-4897-a59f-7f0b148f3875",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dacd8aa0-bf75-4ab5-b21a-667620bb3002",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90503213-dad7-42d9-9799-b75335c8f2ec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
