{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dc078878-dce4-4812-9bac-8ec973af080d",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/JoshWilde/JOSS_Reviewer_Matcher/blob/main/Idea%201/JOSS_Reviewer_Idea_1.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a1ce9706-d315-4e1c-9cf5-276d973772e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import glob\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e65264d2-adb4-445c-a9e2-8af7090dbb1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/sunilmcesh/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "from tqdm import tqdm \n",
    "#import pdfminer\n",
    "from pdfminer.high_level import extract_text\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk \n",
    "nltk.download('stopwords')\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "400363b2-9d96-4fd2-9e1f-eb115b8321bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/sunilmcesh/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/sunilmcesh/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk import sent_tokenize\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "from nltk.probability import FreqDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a12862cc-4453-44fd-ad38-6ac1b6b92cae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from JOSS_PDF_Cleaner import Clean_PDF\n",
    "import re\n",
    "from termcolor import colored\n",
    "import warnings\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "576b4f68-4f8a-49aa-9c9b-6871a74b5e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "PAPER_OF_INTEREST_FNAME  =  glob.glob('/Volumes/Seagate Backup Plus Drive/JOSS Project/joss-papers-master/*/*/*.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bf0a5e92-3e3f-4d37-a074-b29ad7ad12dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Get_Lemma_Words(POI_PDF):\n",
    "  text = str(POI_PDF)\n",
    "  text2 = text.split()\n",
    "  words_no_punc = []\n",
    "\n",
    "  for w in text2:\n",
    "    if w.isalpha():\n",
    "      words_no_punc.append(w.lower())\n",
    "  from nltk.corpus import stopwords\n",
    "  stopwords = stopwords.words('english')  \n",
    "  clean_words = []\n",
    "  for w in words_no_punc:\n",
    "    if w not in stopwords:\n",
    "      clean_words.append(w)\n",
    "  clean_words_arr = ''\n",
    "  for i in range(len(clean_words)):\n",
    "    clean_words_arr = clean_words_arr + ' ' + str(clean_words[i])\n",
    "\n",
    "  string_for_lemmatizing = clean_words_arr\n",
    "  lemmatizer = WordNetLemmatizer()\n",
    "  words_2 = word_tokenize(string_for_lemmatizing)\n",
    "  lemmatized_words = [lemmatizer.lemmatize(word) for word in words_2]\n",
    "\n",
    "  lemmatized_words_arr = ''\n",
    "  for i in range(len(lemmatized_words)):\n",
    "    lemmatized_words_arr = lemmatized_words_arr + ' ' + str(lemmatized_words[i])\n",
    "  words = word_tokenize(lemmatized_words_arr)\n",
    "\n",
    "  return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0088cba4-82fe-4716-950f-da697b0b0663",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reviewers = pd.read_csv('../Data/JOSS Table Test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "fa637666-2b60-4849-9fc4-bb4f0f6552da",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Get_Top_Words_tf(Paper_interest, df_reviewers=df_reviewers,num_suggestions=5, num_top20=20):\n",
    "    POI_PDF = [extract_text(Paper_interest)]\n",
    "    text = str(POI_PDF)\n",
    "    words =  Get_Lemma_Words(POI_PDF)\n",
    "    #print(len(words))\n",
    "    fdist = FreqDist(words)\n",
    "    X = np.array(fdist.most_common())\n",
    "    top20_tf = X[:num_top20,0]\n",
    "\n",
    "    #df_reviewers.username.iloc[+1]\n",
    "\n",
    "    return top20_tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "844388bc-ccca-4c83-b0c7-270748c7ee7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Compare_topics(top20, df_reviewers):\n",
    "  length = df_reviewers.shape[0] - 1\n",
    "  match_arr = np.zeros(length)\n",
    "  for i in range(length):\n",
    "    if pd.isna(df_reviewers['Domains/topic areas you are comfortable reviewing'].str.lower().values[1+i]) == False:\n",
    "      t = df_reviewers['Domains/topic areas you are comfortable reviewing'].str.lower().values[1+i]\n",
    "      #print(i)\n",
    "      uniarr = Split_columns(t)\n",
    "      for j in range(len(uniarr)):\n",
    "        for k in range(len(top20)):\n",
    "          if uniarr[j] == top20[k]:\n",
    "            match_arr[i] = match_arr[i] + 1\n",
    "  return match_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4ff675ff-8d67-4bcb-80a8-5901d1fa0046",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Split_columns(t):\n",
    "  txt = \" \".join(\"\".join([\" \" if ch in string.punctuation else ch for ch in t]).split())\n",
    "  sol1 = np.char.split(txt, ' ')\n",
    "  txt_arr  = array_of_lists_to_array(sol1)\n",
    "  uniarr = np.unique(txt_arr)\n",
    "  return uniarr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "1abc58eb-39a5-4b6a-8866-1721039f6b57",
   "metadata": {},
   "outputs": [],
   "source": [
    " def array_of_lists_to_array(arr):\n",
    "    return np.apply_along_axis(lambda a: np.array(a[0]), -1, arr[..., None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "50f2525d-c171-4c20-8d95-f7795510a314",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summatation_bot(all_usernames, all_domains, all_num_matched_words, all_matched_words):\n",
    "  length = len(all_usernames)\n",
    "  message = 'Hello. \\nI have found ' + str(length) + ' possible reviewers for this paper.' +'\\n\\n'\n",
    "  for i in range(length):\n",
    "    ps = 'I believe ' + all_usernames[i] + ' will make a good reviewer for this paper because they have matched ' + str(int(all_num_matched_words[i])) +  ' words from their comfortable domain topics with the top 20 most frequent words in the paper. These matched words are ' + str(all_matched_words[i]) +'.\\nFrom their topics domain: ' + str(all_domains[i].replace('\\n', ', ')) +'.\\n'\n",
    "    message = message + ps + '\\n'\n",
    "  print(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "77b98f0c-0a7c-4521-8f99-fcfabcdd1d4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Volumes/Seagate Backup Plus Drive/JOSS Project/joss-papers-master/joss-papers-master/joss.00775/10.21105.joss.00775.pdf\n"
     ]
    }
   ],
   "source": [
    "Q = 340\n",
    "print(PAPER_OF_INTEREST_FNAME[Q])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c4788e6a-9a24-4c6d-b93d-035491fb075b",
   "metadata": {},
   "outputs": [],
   "source": [
    "top20_tf = Get_Top_Words_tf(PAPER_OF_INTEREST_FNAME[Q])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7ef91c24-9830-4607-bcb3-18ec8947139c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['command', 'line', 'software', 'user', 'open', 'helpme', 'helper',\n",
       "       'issue', 'source', 'research', 'tool', 'support', 'way', 'journal',\n",
       "       'common', 'help', 'need', 'needing', 'perform', 'interface'],\n",
       "      dtype='<U21')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top20_tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "099b0d1f-975d-4fa2-ac6a-c2f224e235c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetReviewer_Vectors(df_reviewers=df_reviewers):\n",
    "    reviewer_vectors = np.zeros(((df_reviewers.shape[0]-1),300))\n",
    "    for i in range(df_reviewers.shape[0]-1):\n",
    "        #if i%10 == 0:\n",
    "        #    print(i)\n",
    "        if pd.isna(df_reviewers['Domains/topic areas you are comfortable reviewing'].iloc[1:].values[i]) == False:\n",
    "            review_text = df_reviewers['Domains/topic areas you are comfortable reviewing'].iloc[1:].values[i].lower()\n",
    "            review_text = review_text.replace('-\\\\n','')\n",
    "            review_text = review_text.replace('\\\\n',' ')\n",
    "            review_text = review_text.replace('\\n', ' ')\n",
    "        \n",
    "            review_arr = []\n",
    "            for token in model(review_text):\n",
    "                if token.is_alpha == True:\n",
    "                    if token.is_stop == False:\n",
    "                        review_arr.append(str(token.lemma_).lower())\n",
    "            review_arr = np.array(review_arr)\n",
    "        \n",
    "            review_str = ''\n",
    "            for j in np.unique(review_arr):\n",
    "                review_str = review_str + j +' '\n",
    "        \n",
    "        #print(model(review_str).vector.shape)\n",
    "        #print(reviewer_vectors.shape)\n",
    "            reviewer_vectors[i] = model(review_str).vector\n",
    "    \n",
    "    return reviewer_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c0a9c582-5b2e-4071-9a8a-9fb76b86a5d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetCosineSims(doc_vec, review_vec, df_reviewers=df_reviewers):\n",
    "    all_usernames = []\n",
    "    all_domains = []\n",
    "    all_cosine_sims = []\n",
    "    for j in range(len(review_vec)):\n",
    "        if pd.isna(df_reviewers.iloc[j+1]['Domains/topic areas you are comfortable reviewing']) == False:\n",
    "            all_cosine_sims.append(cosine_similarity(np.array([doc_vec]), np.array([review_vec[j]]))[0,0])\n",
    "            all_domains.append(df_reviewers.iloc[j+1]['Domains/topic areas you are comfortable reviewing'].lower())\n",
    "            all_usernames.append(df_reviewers.iloc[j+1].username)\n",
    "    all_usernames= np.array(all_usernames)\n",
    "    all_domains= np.array(all_domains)\n",
    "    all_cosine_sims= np.array(all_cosine_sims)\n",
    "    \n",
    "    return all_usernames, all_domains, all_cosine_sims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "47135382-c140-4e55-bf01-80c0ddbadbaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "model = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2c7f52a8-a6ae-403d-bfbd-2b5bbc3a1ce4",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviewer_vectors = GetReviewer_Vectors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b88dbe9a-e700-402f-8014-66b15a6e958b",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_top20= ''\n",
    "for i in top20_tf:\n",
    "    doc_top20 = doc_top20 + i +' '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "adaede04-b13d-45be-ad84-1076853a2e06",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_usernames2, all_domains2, all_cosine_sims2 = GetCosineSims(model(doc_top20).vector, reviewer_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d4b32ba1-c9b5-44e3-8fc4-d52781bb39e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def TopReviewers(number=5, all_usernames=all_usernames2, all_domains=all_domains2, all_cosine_sims=all_cosine_sims2):\n",
    "    message = 'Hello.\\n I have found ' +str(number) + ' possible reviewers for this paper.'+ '\\n\\n'\n",
    "    for J in range(number):\n",
    "        index = np.argsort(all_cosine_sims)[-1-J]\n",
    "        #print(index)\n",
    "        ps = 'I believe '+ colored(str(all_usernames[index]), 'green') + ' will be a good reviewer for this paper. Their domain interests and this paper have a cosine similairity score of ' + colored(str(all_cosine_sims[index])[:6], 'blue') + '. This reviewers domain interests are ' + colored(str(all_domains[index].replace('\\n', ',')), 'red')\n",
    "        message = message + ps + '\\n\\n'\n",
    "    print(message)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "944ef2c1-6753-4140-8091-719e36217168",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello.\n",
      " I have found 5 possible reviewers for this paper.\n",
      "\n",
      "I believe \u001b[32malexpghayes\u001b[0m will be a good reviewer for this paper. Their domain interests and this paper have a cosine similairity score of \u001b[34m0.8903\u001b[0m. This reviewers domain interests are \u001b[31mi'm interested in making sure that r packages for modelling have useful and intuitive interfaces and documentation. i'm not interested in double checking theory and correctness, but making sure that a new user can quickly and easily get the results they want.\u001b[0m\n",
      "\n",
      "I believe \u001b[32mjarrah42\u001b[0m will be a good reviewer for this paper. Their domain interests and this paper have a cosine similairity score of \u001b[34m0.8881\u001b[0m. This reviewers domain interests are \u001b[31mhigh performance/scientific computing, operating systems, distributed systems, parallel programming, software engineering, user interfaces, debugging, performance analysis, development tools, security, web development, internet of things\u001b[0m\n",
      "\n",
      "I believe \u001b[32mjcolomb\u001b[0m will be a good reviewer for this paper. Their domain interests and this paper have a cosine similairity score of \u001b[34m0.8589\u001b[0m. This reviewers domain interests are \u001b[31mneurobiology, accessibility (i am not a coder, and can therefore see what info is needed for non-coder to use/build on the software)\u001b[0m\n",
      "\n",
      "I believe \u001b[32mmichaelberks\u001b[0m will be a good reviewer for this paper. Their domain interests and this paper have a cosine similairity score of \u001b[34m0.8481\u001b[0m. This reviewers domain interests are \u001b[31mmedical imaging, image processing, compute vision, machine learning/ai applied to images, general scientific/numerical data processing (unless requiring specialist domain specific knowledge outside of the areas listed above)\u001b[0m\n",
      "\n",
      "I believe \u001b[32mthelinuxmaniac\u001b[0m will be a good reviewer for this paper. Their domain interests and this paper have a cosine similairity score of \u001b[34m0.8420\u001b[0m. This reviewers domain interests are \u001b[31mcomputer vision, machine learning, computer graphics, user experience/user interface, linux, software engineering\u001b[0m\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "TopReviewers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31d967e-cbf1-49d3-ac5c-3ace125b7b51",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d8bdae8-83a7-4143-865a-319bd122f209",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6ef1dd-b676-4c84-88e7-ed2100fb5617",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b66a0be-a9b4-4602-8783-d1b2ac188b83",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c8c9387-690d-4099-b29f-4f604a51b4ea",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
