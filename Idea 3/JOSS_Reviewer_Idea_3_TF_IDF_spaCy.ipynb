{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c8a91d7-296d-440b-a661-79b0b4b22910",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://github.com/JoshWilde/JOSS_Reviewer_Matcher/blob/main/Idea%202/JOSS_Reviewer_Idea_2.ipynb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b8981e8a-4d64-4708-b8ef-b1ef81cd72f7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import glob\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "043d9b97-63fb-4340-8588-1eeb157280d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/sunilmcesh/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "from tqdm import tqdm \n",
    "#import pdfminer\n",
    "from pdfminer.high_level import extract_text\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk \n",
    "nltk.download('stopwords')\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a00b4e9a-5e3f-4e56-aa44-072e5208d69f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/sunilmcesh/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/sunilmcesh/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk import sent_tokenize\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "from nltk.probability import FreqDist\n",
    "import spacy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "203b9e3c-6a6d-4e77-bdb3-981137bee1a6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a9e4ad0-79cf-4507-80a4-20196e606324",
   "metadata": {},
   "outputs": [],
   "source": [
    "PAPER_OF_INTEREST_FNAME  =  glob.glob('/Volumes/Seagate Backup Plus Drive/JOSS Project/joss-papers-master/*/*/*.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6ffd626-ba52-4a2d-a2cd-d27e52de6bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://zenodo.org/record/3631674#.YeglfP7P2Uk\n",
    "df = pd.read_csv('/Volumes/Seagate Backup Plus Drive/JOSS Project/wiki_tfidf_terms.csv')\n",
    "df_reviewers = pd.read_csv('../Data/JOSS Table Test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f77a30c2-38c7-4318-9663-5aafc51b897d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Get_Lemma_Words(POI_PDF):\n",
    "  text = str(POI_PDF)\n",
    "  text2 = text.split()\n",
    "  words_no_punc = []\n",
    "\n",
    "  for w in text2:\n",
    "    if w.isalpha():\n",
    "      words_no_punc.append(w.lower())\n",
    "  from nltk.corpus import stopwords\n",
    "  stopwords = stopwords.words('english')  \n",
    "  clean_words = []\n",
    "  for w in words_no_punc:\n",
    "    if w not in stopwords:\n",
    "      clean_words.append(w)\n",
    "  clean_words_arr = ''\n",
    "  for i in range(len(clean_words)):\n",
    "    clean_words_arr = clean_words_arr + ' ' + str(clean_words[i])\n",
    "\n",
    "  string_for_lemmatizing = clean_words_arr\n",
    "  lemmatizer = WordNetLemmatizer()\n",
    "  words_2 = word_tokenize(string_for_lemmatizing)\n",
    "  lemmatized_words = [lemmatizer.lemmatize(word) for word in words_2]\n",
    "\n",
    "  lemmatized_words_arr = ''\n",
    "  for i in range(len(lemmatized_words)):\n",
    "    lemmatized_words_arr = lemmatized_words_arr + ' ' + str(lemmatized_words[i])\n",
    "  words = word_tokenize(lemmatized_words_arr)\n",
    "\n",
    "  return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c073e9a0-7cab-48ae-a8a4-65d2ce358e9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Get_Top_Words_tf_idf(Paper_interest, df=df, df_reviewers=df_reviewers, num_suggestions=5, num_top20=20):\n",
    "    POI_PDF = [extract_text(Paper_interest)]\n",
    "    text = str(POI_PDF)\n",
    "    words = Get_Lemma_Words(POI_PDF)\n",
    "    #print(len(words))\n",
    "    fdist = FreqDist(words)\n",
    "    X = np.array(fdist.most_common())\n",
    "    tf_idf_arr_names, tf_idf_arr_floats = determine_wiki_td_idf(X, df=df)\n",
    "    #print('determined wiki')\n",
    "    num_arr = np.array(tf_idf_arr_floats)\n",
    "    tf_idf_arr_names_arr = np.array(tf_idf_arr_names)\n",
    "    top20_tf_idf = tf_idf_arr_names_arr[np.argsort(num_arr)[:num_top20]]\n",
    "    return top20_tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6e63928c-2d27-48c7-be14-842a9b9bc34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Compare_topics(top20, df_reviewers):\n",
    "  length = df_reviewers.shape[0] - 1\n",
    "  match_arr = np.zeros(length)\n",
    "  for i in range(length):\n",
    "    if pd.isna(df_reviewers['Domains/topic areas you are comfortable reviewing'].str.lower().values[1+i]) == False:\n",
    "      t = df_reviewers['Domains/topic areas you are comfortable reviewing'].str.lower().values[1+i]\n",
    "      #print(i)\n",
    "      uniarr = Split_columns(t)\n",
    "      for j in range(len(uniarr)):\n",
    "        for k in range(len(top20)):\n",
    "          if uniarr[j] == top20[k]:\n",
    "            match_arr[i] = match_arr[i] + 1\n",
    "  return match_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "37980f8b-7170-463a-b8c1-eeefd9a52571",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Split_columns(t):\n",
    "  txt = \" \".join(\"\".join([\" \" if ch in string.punctuation else ch for ch in t]).split())\n",
    "  sol1 = np.char.split(txt, ' ')\n",
    "  txt_arr  = array_of_lists_to_array(sol1)\n",
    "  uniarr = np.unique(txt_arr)\n",
    "  return uniarr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "189fddc3-c354-49fb-b6cb-d4dc114ba001",
   "metadata": {},
   "outputs": [],
   "source": [
    " def array_of_lists_to_array(arr):\n",
    "    return np.apply_along_axis(lambda a: np.array(a[0]), -1, arr[..., None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c031aab9-6476-4d1e-b805-c1a1acd1ea87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_wiki_td_idf(x, df=df):\n",
    "    tf_idf_arr_names = []\n",
    "    tf_idf_arr_floats = []\n",
    "    for i in range(len(x)):\n",
    "        if df[df['token'] ==x[i][0]].frequency.empty == False:\n",
    "            wiki_tf = df[df['token'] ==x[i][0]].frequency.values[0]\n",
    "            doc_tf = int(x[i][1])\n",
    "            tf_idf = np.log(wiki_tf/doc_tf)\n",
    "            tf_idf_arr_names.append(x[i][0])\n",
    "            tf_idf_arr_floats.append(tf_idf)\n",
    "    return tf_idf_arr_names, tf_idf_arr_floats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c3fb9c0c-9036-45e1-9e5d-e396e969da25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summatation_bot(all_usernames, all_domains, all_num_matched_words, all_matched_words):\n",
    "  length = len(all_usernames)\n",
    "  message = 'Hello. \\nI have found ' + str(length) + ' possible reviewers for this paper.' +'\\n\\n'\n",
    "  for i in range(length):\n",
    "    ps = 'I believe ' + all_usernames[i] + ' will make a good reviewer for this paper because they have matched ' + str(int(all_num_matched_words[i])) +  ' words from their comfortable domain topics with the top 20 most frequent words in the paper. These matched words are ' + str(all_matched_words[i]) +'.\\nFrom their topics domain: ' + str(all_domains[i].replace('\\n', ', ')) +'.\\n'\n",
    "    message = message + ps + '\\n'\n",
    "  print(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cb926465-08d3-40cb-bf76-a31bc4b1ca67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Volumes/Seagate Backup Plus Drive/JOSS Project/joss-papers-master/joss-papers-master/joss.00775/10.21105.joss.00775.pdf\n"
     ]
    }
   ],
   "source": [
    "Q = 340\n",
    "print(PAPER_OF_INTEREST_FNAME[Q])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3eb2cd27-8be4-4d1e-9ce3-fcd20fb5ebae",
   "metadata": {},
   "outputs": [],
   "source": [
    "top20_tf_idf = Get_Top_Words_tf_idf(PAPER_OF_INTEREST_FNAME[Q])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2f1c46f0-fc58-4746-8657-7a99412a0073",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['helpme', 'uservoice', 'helper', 'serf', 'endpoint', 'github',\n",
       "       'headless', 'ticketing', 'interrupt', 'intuitive', 'needing',\n",
       "       'user', 'click', 'repository', 'attribution', 'navigate', 'ample',\n",
       "       'manually', 'software', 'tool'], dtype='<U13')"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top20_tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "b0d93473-4c84-4534-af92-8c15aa0b44e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetReviewer_Vectors(df_reviewers=df_reviewers):\n",
    "    reviewer_vectors = np.zeros(((df_reviewers.shape[0]-1),300))\n",
    "    for i in range(df_reviewers.shape[0]-1):\n",
    "        #if i%10 == 0:\n",
    "        #    print(i)\n",
    "        if pd.isna(df_reviewers['Domains/topic areas you are comfortable reviewing'].iloc[1:].values[i]) == False:\n",
    "            review_text = df_reviewers['Domains/topic areas you are comfortable reviewing'].iloc[1:].values[i].lower()\n",
    "            review_text = review_text.replace('-\\\\n','')\n",
    "            review_text = review_text.replace('\\\\n',' ')\n",
    "            review_text = review_text.replace('\\n', ' ')\n",
    "        \n",
    "            review_arr = []\n",
    "            for token in model(review_text):\n",
    "                if token.is_alpha == True:\n",
    "                    if token.is_stop == False:\n",
    "                        review_arr.append(str(token.lemma_).lower())\n",
    "            review_arr = np.array(review_arr)\n",
    "        \n",
    "            review_str = ''\n",
    "            for j in np.unique(review_arr):\n",
    "                review_str = review_str + j +' '\n",
    "        \n",
    "        #print(model(review_str).vector.shape)\n",
    "        #print(reviewer_vectors.shape)\n",
    "            reviewer_vectors[i] = model(review_str).vector\n",
    "    \n",
    "    return reviewer_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b51d0b97-7295-4d90-abd2-4b7cec002f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "def GetCosineSims(doc_vec, review_vec, df_reviewers=df_reviewers):\n",
    "    all_usernames = []\n",
    "    all_domains = []\n",
    "    all_cosine_sims = []\n",
    "    for j in range(len(review_vec)):\n",
    "        if pd.isna(df_reviewers.iloc[j+1]['Domains/topic areas you are comfortable reviewing']) == False:\n",
    "            all_cosine_sims.append(cosine_similarity(np.array([doc_vec]), np.array([review_vec[j]]))[0,0])\n",
    "            all_domains.append(df_reviewers.iloc[j+1]['Domains/topic areas you are comfortable reviewing'].lower())\n",
    "            all_usernames.append(df_reviewers.iloc[j+1].username)\n",
    "    all_usernames= np.array(all_usernames)\n",
    "    all_domains= np.array(all_domains)\n",
    "    all_cosine_sims= np.array(all_cosine_sims)\n",
    "    \n",
    "    return all_usernames, all_domains, all_cosine_sims"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b86524d4-fa3a-4ab0-9d39-5261e0f47915",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = spacy.load('en_core_web_lg')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "eea8714a-7e4f-4415-a3fc-99475ddc5e29",
   "metadata": {},
   "outputs": [],
   "source": [
    "reviewer_vectors = GetReviewer_Vectors()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "423638d9-7a7b-44fb-b39e-8256f36b893b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['helpme', 'uservoice', 'helper', 'serf', 'endpoint', 'github',\n",
       "       'headless', 'ticketing', 'interrupt', 'intuitive', 'needing',\n",
       "       'user', 'click', 'repository', 'attribution', 'navigate', 'ample',\n",
       "       'manually', 'software', 'tool'], dtype='<U13')"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "top20_tf_idf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b60f0c8b-24ca-4d1a-a91a-5449f90189ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "doc_top20= ''\n",
    "for i in top20_tf_idf:\n",
    "    doc_top20 = doc_top20 + i +' '"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "dd82b2b6-68fa-4238-8d51-665fab406e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ad4239fe-21bd-48ce-be70-36b748fd4e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_usernames2, all_domains2, all_cosine_sims2 = GetCosineSims(model(doc_top20).vector, reviewer_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "455c2d37-fa2d-490f-ac6d-bdb4571ab28f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def TopReviewers(number=5, all_usernames=all_usernames2, all_domains=all_domains2, all_cosine_sims=all_cosine_sims2):\n",
    "    message = 'Hello.\\n I have found ' +str(number) + ' possible reviewers for this paper.'+ '\\n\\n'\n",
    "    for J in range(number):\n",
    "        index = np.argsort(all_cosine_sims)[-1-J]\n",
    "        #print(index)\n",
    "        ps = 'I believe '+ colored(str(all_usernames[index]), 'green') + ' will be a good reviewer for this paper. Their domain interests and this paper have a cosine similairity score of ' + colored(str(all_cosine_sims[index])[:6], 'blue') + '. This reviewers domain interests are ' + colored(str(all_domains[index].replace('\\n', ',')), 'red')\n",
    "        message = message + ps + '\\n\\n'\n",
    "    print(message)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "63c9dcff-0243-4d4e-9787-7c91059b60f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from termcolor import colored\n",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "2e9cb748-f6cf-4a6d-929b-60111a2f4289",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello.\n",
      " I have found 5 possible reviewers for this paper.\n",
      "\n",
      "I believe \u001b[32mGaurangTandon\u001b[0m will be a good reviewer for this paper. Their domain interests and this paper have a cosine similairity score of \u001b[34m0.7602\u001b[0m. This reviewers domain interests are \u001b[31mproducts that build on general web technologies (web servers/scrapers/frontend/backend), , userscripts/chrome extensions, , performance oriented applications (fast python/c++/rust, flamegraphs, perf stat)\u001b[0m\n",
      "\n",
      "I believe \u001b[32mabhishekvp\u001b[0m will be a good reviewer for this paper. Their domain interests and this paper have a cosine similairity score of \u001b[34m0.7566\u001b[0m. This reviewers domain interests are \u001b[31mweb dev tools, web browser, remote sensing tools, geospatial , gis, semantic web, knowledge representation, ontology, iot, hpc, embedded hpc\u001b[0m\n",
      "\n",
      "I believe \u001b[32mjarrah42\u001b[0m will be a good reviewer for this paper. Their domain interests and this paper have a cosine similairity score of \u001b[34m0.7212\u001b[0m. This reviewers domain interests are \u001b[31mhigh performance/scientific computing, operating systems, distributed systems, parallel programming, software engineering, user interfaces, debugging, performance analysis, development tools, security, web development, internet of things\u001b[0m\n",
      "\n",
      "I believe \u001b[32mjcolomb\u001b[0m will be a good reviewer for this paper. Their domain interests and this paper have a cosine similairity score of \u001b[34m0.7153\u001b[0m. This reviewers domain interests are \u001b[31mneurobiology, accessibility (i am not a coder, and can therefore see what info is needed for non-coder to use/build on the software)\u001b[0m\n",
      "\n",
      "I believe \u001b[32msneakers-the-rat\u001b[0m will be a good reviewer for this paper. Their domain interests and this paper have a cosine similairity score of \u001b[34m0.7109\u001b[0m. This reviewers domain interests are \u001b[31mneuroscience, audio, realtime/embedded, computer vision/image processing, browser-based tools/scientific web infrastructure\u001b[0m\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "TopReviewers()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "659f3d2a-9a01-41cd-a54f-aeea0f983cb8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
