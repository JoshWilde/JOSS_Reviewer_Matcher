{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "624ac468-ed98-4cbb-a99c-3f3103292cf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import glob\n",
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "043d9b97-63fb-4340-8588-1eeb157280d8",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/sunilmcesh/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import glob\n",
    "from tqdm import tqdm \n",
    "#import pdfminer\n",
    "from pdfminer.high_level import extract_text\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk \n",
    "nltk.download('stopwords')\n",
    "import numpy as np\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a00b4e9a-5e3f-4e56-aa44-072e5208d69f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/sunilmcesh/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/sunilmcesh/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "from nltk import sent_tokenize\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "nltk.download('wordnet')\n",
    "from nltk.probability import FreqDist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a9e4ad0-79cf-4507-80a4-20196e606324",
   "metadata": {},
   "outputs": [],
   "source": [
    "PAPER_OF_INTEREST_FNAME  =  glob.glob('/Volumes/Seagate Backup Plus Drive/JOSS Project/joss-papers-master/*/*/*.pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f6ffd626-ba52-4a2d-a2cd-d27e52de6bb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://zenodo.org/record/3631674#.YeglfP7P2Uk\n",
    "df = pd.read_csv('/Volumes/Seagate Backup Plus Drive/JOSS Project/wiki_tfidf_terms.csv')\n",
    "df_reviewers = pd.read_csv('../Data/JOSS Table Test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f77a30c2-38c7-4318-9663-5aafc51b897d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Get_Lemma_Words(POI_PDF):\n",
    "  text = str(POI_PDF)\n",
    "  text2 = text.split()\n",
    "  words_no_punc = []\n",
    "\n",
    "  for w in text2:\n",
    "    if w.isalpha():\n",
    "      words_no_punc.append(w.lower())\n",
    "  from nltk.corpus import stopwords\n",
    "  stopwords = stopwords.words('english')  \n",
    "  clean_words = []\n",
    "  for w in words_no_punc:\n",
    "    if w not in stopwords:\n",
    "      clean_words.append(w)\n",
    "  clean_words_arr = ''\n",
    "  for i in range(len(clean_words)):\n",
    "    clean_words_arr = clean_words_arr + ' ' + str(clean_words[i])\n",
    "\n",
    "  string_for_lemmatizing = clean_words_arr\n",
    "  lemmatizer = WordNetLemmatizer()\n",
    "  words_2 = word_tokenize(string_for_lemmatizing)\n",
    "  lemmatized_words = [lemmatizer.lemmatize(word) for word in words_2]\n",
    "\n",
    "  lemmatized_words_arr = ''\n",
    "  for i in range(len(lemmatized_words)):\n",
    "    lemmatized_words_arr = lemmatized_words_arr + ' ' + str(lemmatized_words[i])\n",
    "  words = word_tokenize(lemmatized_words_arr)\n",
    "\n",
    "  return words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f25f172a-5855-4b23-a3d6-b2e05008f6f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Get_reviewer_sample_tf_idf(Paper_interest, df=df, df_reviewers=df_reviewers, num_suggestions=5, num_top20=20):\n",
    "    #POI_PDF = [extract_text(Paper_interest)]\n",
    "    #text = str(POI_PDF)\n",
    "    #words =  Get_Lemma_Words(POI_PDF)\n",
    "    ##print(len(words))\n",
    "    #fdist = FreqDist(words)\n",
    "    #X = np.array(fdist.most_common())\n",
    "    #top20_tf = X[:num_top20,0]\n",
    "    #match_arr = Compare_topics(top20_tf, df_reviewers)\n",
    "    #top5_reviewers = np.argsort(match_arr)[-num_suggestions:]\n",
    "\n",
    "    POI_PDF = [extract_text(Paper_interest)]\n",
    "    text = str(POI_PDF)\n",
    "    words = Get_Lemma_Words(POI_PDF)\n",
    "    #print(len(words))\n",
    "    fdist = FreqDist(words)\n",
    "    X = np.array(fdist.most_common())\n",
    "    tf_idf_arr_names, tf_idf_arr_floats = determine_wiki_td_idf(X, df=df)\n",
    "    #print('determined wiki')\n",
    "    num_arr = np.array(tf_idf_arr_floats)\n",
    "    tf_idf_arr_names_arr = np.array(tf_idf_arr_names)\n",
    "    top20_tf_idf = tf_idf_arr_names_arr[np.argsort(num_arr)[:20]]\n",
    "    match_arr = Compare_topics(top20_tf_idf, df_reviewers)\n",
    "    top5_reviewers = np.argsort(match_arr)[-num_suggestions:]\n",
    "    \n",
    "    \n",
    "    all_usernames = []\n",
    "    all_domains = []\n",
    "    all_num_matched_words = []\n",
    "    all_matched_words = []\n",
    "    for i in range(num_suggestions):\n",
    "      K = -1*(i+1)\n",
    "      index = top5_reviewers[K]\n",
    "      #print(i)\n",
    "      t =df_reviewers.iloc[index+1]['Domains/topic areas you are comfortable reviewing'].lower()\n",
    "      \n",
    "      all_usernames.append(df_reviewers.username.iloc[index+1])\n",
    "      all_domains.append(t)\n",
    "      all_num_matched_words.append(match_arr[index])\n",
    "\n",
    "      uniarr = Split_columns(t)\n",
    "      matched_words = []\n",
    "      #print(uniarr)\n",
    "      for j in range(len(uniarr)):\n",
    "        for k in range(len(top20_tf_idf)):\n",
    "          if uniarr[j] == top20_tf_idf[k]:\n",
    "            matched_words.append(uniarr[j])\n",
    "      all_matched_words.append(matched_words)\n",
    "\n",
    "    #df_reviewers.username.iloc[+1]\n",
    "\n",
    "    return all_usernames, all_domains, all_num_matched_words, all_matched_words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6e63928c-2d27-48c7-be14-842a9b9bc34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Compare_topics(top20, df_reviewers):\n",
    "  length = df_reviewers.shape[0] - 1\n",
    "  match_arr = np.zeros(length)\n",
    "  for i in range(length):\n",
    "    if pd.isna(df_reviewers['Domains/topic areas you are comfortable reviewing'].str.lower().values[1+i]) == False:\n",
    "      t = df_reviewers['Domains/topic areas you are comfortable reviewing'].str.lower().values[1+i]\n",
    "      #print(i)\n",
    "      uniarr = Split_columns(t)\n",
    "      for j in range(len(uniarr)):\n",
    "        for k in range(len(top20)):\n",
    "          if uniarr[j] == top20[k]:\n",
    "            match_arr[i] = match_arr[i] + 1\n",
    "  return match_arr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "37980f8b-7170-463a-b8c1-eeefd9a52571",
   "metadata": {},
   "outputs": [],
   "source": [
    "def Split_columns(t):\n",
    "  txt = \" \".join(\"\".join([\" \" if ch in string.punctuation else ch for ch in t]).split())\n",
    "  sol1 = np.char.split(txt, ' ')\n",
    "  txt_arr  = array_of_lists_to_array(sol1)\n",
    "  uniarr = np.unique(txt_arr)\n",
    "  return uniarr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "189fddc3-c354-49fb-b6cb-d4dc114ba001",
   "metadata": {},
   "outputs": [],
   "source": [
    " def array_of_lists_to_array(arr):\n",
    "    return np.apply_along_axis(lambda a: np.array(a[0]), -1, arr[..., None])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c031aab9-6476-4d1e-b805-c1a1acd1ea87",
   "metadata": {},
   "outputs": [],
   "source": [
    "def determine_wiki_td_idf(x, df=df):\n",
    "    tf_idf_arr_names = []\n",
    "    tf_idf_arr_floats = []\n",
    "    for i in range(len(x)):\n",
    "        if df[df['token'] ==x[i][0]].frequency.empty == False:\n",
    "            wiki_tf = df[df['token'] ==x[i][0]].frequency.values[0]\n",
    "            doc_tf = int(x[i][1])\n",
    "            tf_idf = np.log(wiki_tf/doc_tf)\n",
    "            tf_idf_arr_names.append(x[i][0])\n",
    "            tf_idf_arr_floats.append(tf_idf)\n",
    "    return tf_idf_arr_names, tf_idf_arr_floats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c3fb9c0c-9036-45e1-9e5d-e396e969da25",
   "metadata": {},
   "outputs": [],
   "source": [
    "def summatation_bot(all_usernames, all_domains, all_num_matched_words, all_matched_words):\n",
    "  length = len(all_usernames)\n",
    "  message = 'Hello. \\nI have found ' + str(length) + ' possible reviewers for this paper.' +'\\n\\n'\n",
    "  for i in range(length):\n",
    "    ps = 'I believe ' + all_usernames[i] + ' will make a good reviewer for this paper because they have matched ' + str(int(all_num_matched_words[i])) +  ' words from their comfortable domain topics with the top 20 most frequent words in the paper. These matched words are ' + str(all_matched_words[i]) +'.\\nFrom their topics domain: ' + str(all_domains[i].replace('\\n', ', ')) +'.\\n'\n",
    "    message = message + ps + '\\n'\n",
    "  print(message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "cb926465-08d3-40cb-bf76-a31bc4b1ca67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/Volumes/Seagate Backup Plus Drive/JOSS Project/joss-papers-master/joss-papers-master/joss.00775/10.21105.joss.00775.pdf\n"
     ]
    }
   ],
   "source": [
    "Q = 340\n",
    "print(PAPER_OF_INTEREST_FNAME[Q])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d46a6fc7-518f-4a25-9291-b877aabf4173",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_usernames, all_domains, all_num_matched_words, all_matched_words = Get_reviewer_sample_tf_idf(PAPER_OF_INTEREST_FNAME[Q])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c7fd7706-df2a-49f2-83ae-65b05e90839a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Hello. \n",
      "I have found 5 possible reviewers for this paper.\n",
      "\n",
      "I believe csadorf will make a good reviewer for this paper because they have matched 2 words from their comfortable domain topics with the top 20 most frequent words in the paper. These matched words are ['github', 'software'].\n",
      "From their topics domain: data management,  workflow management,  molecular dynamics,  metropolis monte carlo,  crystallization,  machine learning,  neural networks,  parallelism,  cuda,  mpi,  high-performance clusters,  supercomputers,  github actions,  circle ci,  pandas,  project jupyter,  materials science,  chemical engineering,  scientific software.\n",
      "\n",
      "I believe jarrah42 will make a good reviewer for this paper because they have matched 2 words from their comfortable domain topics with the top 20 most frequent words in the paper. These matched words are ['software', 'user'].\n",
      "From their topics domain: high performance/scientific computing,  operating systems,  distributed systems,  parallel programming,  software engineering,  user interfaces,  debugging,  performance analysis,  development tools,  security,  web development,  internet of things.\n",
      "\n",
      "I believe thelinuxmaniac will make a good reviewer for this paper because they have matched 2 words from their comfortable domain topics with the top 20 most frequent words in the paper. These matched words are ['software', 'user'].\n",
      "From their topics domain: computer vision,  machine learning,  computer graphics,  user experience/user interface,  linux,  software engineering.\n",
      "\n",
      "I believe alexpghayes will make a good reviewer for this paper because they have matched 2 words from their comfortable domain topics with the top 20 most frequent words in the paper. These matched words are ['intuitive', 'user'].\n",
      "From their topics domain: i'm interested in making sure that r packages for modelling have useful and intuitive interfaces and documentation. i'm not interested in double checking theory and correctness, but making sure that a new user can quickly and easily get the results they want..\n",
      "\n",
      "I believe ritwikagarwal will make a good reviewer for this paper because they have matched 1 words from their comfortable domain topics with the top 20 most frequent words in the paper. These matched words are ['software'].\n",
      "From their topics domain: open source ,  open advocacy & publishing,  diversity & inclusion,  new software technologies ,  ethical ai.\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "summatation_bot(all_usernames, all_domains, all_num_matched_words, all_matched_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eebea77f-1512-4e3c-b31e-111ee73f5451",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
